{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nixiesearch: batteries included search engine","text":""},{"location":"#what-is-nixiesearch","title":"What is Nixiesearch?","text":"<p>Nixiesearch is a modern search engine that runs on S3-compatible storage. We built it after dealing with the headaches of running large Elastic/OpenSearch clusters (here's the blog post full of pain), and here\u2019s why it\u2019s awesome:</p> <ul> <li>Powered by Apache Lucene: You get support for 39 languages, facets, advanced filters, autocomplete suggestions, and the familiar sorting features you're used to.</li> <li>Decoupled S3-based storage and compute: There's nothing to break. You get risk-free backups, upgrades, schema changes and auto-scaling, all on a stateless index stored in S3.</li> <li>Flexible deployment options: Run on Kubernetes, Docker, or serverless with AWS Lambda for scale-to-zero cost optimization.</li> <li>Pull indexing: Supports both offline and online incremental indexing using an Apache Spark based ETL process. No more POSTing JSON blobs to prod cluster (and overloading it).</li> <li>No state inside the cluster: All changes (settings, indexes, etc.) are just config updates, which makes blue-green deployments of index changes a breeze.</li> <li>AI batteries included: fully local Embedding and LLM inference, first class RAG API support.</li> </ul> <p></p> <p>Search is never easy, but Nixiesearch has your back. It takes care of the toughest parts\u2014like reindexing, capacity planning, and maintenance\u2014so you can save time (and your sanity).</p> <p>Note</p> <p>Want to learn more? Go straight to the quickstart and check out the live demo.</p>"},{"location":"#what-nixiesearch-is-not","title":"What Nixiesearch is not?","text":"<ul> <li>Nixiesearch is not a database, and was never meant to be. Nixiesearch is a search index for consumer-facing apps to find top-N most relevant documents for a query. For analytical cases consider using good old SQL with Clickhouse or Snowflake.</li> <li>Not a tool to search for logs. Log search is about throughput, and Nixiesearch is about relevance. If you plan to use Nixiesearch as a log storage system, please don't: consider ELK or Quickwit as better alternatives.</li> <li>Not meant for unstructured data. Your unstructured documents do have an implicit internal schema, and knowing it in advance makes all the indexing magic much more efficient. Elastic, Opensearch and SOLR are better choices in this case.</li> </ul>"},{"location":"#the-difference","title":"The difference","text":"<p>Our elasticsearch cluster has been a pain in the ass since day one with the main fix always \"just double the size of the server\" to the point where our ES cluster ended up costing more than our entire AWS bill pre-ES  [HN source] </p> <p>When your search cluster is red again when you accidentally send a wrong JSON to a wrong REST endpoint, you can just write your own S3-based search engine like big guys do:</p> <ul> <li>Uber:  Lucene: Uber\u2019s Search Platform Version Upgrade.</li> <li>Amazon: E-Commerce search at scale on Apache Lucene.</li> <li>Doordash: Introducing DoorDash\u2019s in-house search engine.</li> </ul> <p></p> <p>Nixiesearch was inspired by these search engines, but is fully open-source (with no paid addons and enterprise tier). Decoupling search and storage makes ops simpler. Making your search configuration immutable makes it even more simple. </p> <p></p> <p>How it's different from popular search engines?</p> <ul> <li>vs Elastic: Embedding inference, hybrid search and reranking are free and open-source. For ES these are part of the proprietary cloud.</li> <li>vs OpenSearch: While OpenSearch can use S3-based segment replication, Nixiesearch can also offload cluster state to S3.</li> <li>vs Qdrant and Weaviate: Not a sidecar search engine to handle just vector search. Autocomplete, facets, RAG and embedding inference out of the box.</li> </ul>"},{"location":"#try-it-out","title":"Try it out","text":"<p>Get the sample MSRD: Movie Search Ranking Dataset dataset:</p> <pre><code>curl -Lo movies.jsonl https://nixiesearch.ai/data/movies.jsonl\n</code></pre> <pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   162  100   162    0     0   3636      0 --:--:-- --:--:-- --:--:--  3681\n100 32085  100 32085    0     0   226k      0 --:--:-- --:--:-- --:--:--  226k\n</code></pre> <p>Create an index mapping for <code>movies</code> index in a file <code>config.yml</code>:</p> <pre><code>inference:\n  embedding:\n    e5-small: # (1)\n      model: intfloat/e5-small-v2 # (2)\nschema:\n  movies: # index name\n    fields:\n      title: # field name\n        type: text\n        search: \n          lexical: # build lexical index\n            analyze: english\n          semantic: # and a vector search index also\n            model: e5-small\n        suggest: true\n      overview:\n        type: text\n        search: false\n</code></pre> <ol> <li>We use ONNX Runtime for local embedding inference. But you can also use any API-based SaaS embedding provider.</li> <li>Any SBERT-compatible embedding model can be used, and you can bring your own</li> </ol> <p>Run the Nixiesearch docker container:</p> <pre><code>docker run -itp 8080:8080 -v .:/data nixiesearch/nixiesearch:latest standalone -c /data/config.yml\n</code></pre> <p>If you see a cool ASCII-art logo, then the server is ready to serve requests:</p> <pre><code>- Local index movies opened\n- opening index movies\n- \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\n- \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\n- \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n- \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n- \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2554\u255d \u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n- \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n- version=0.5.0 jdk[build]=21.0.6+7-LTS jdk[runtime]=21+35-2513 arch=arm64 build=CPU\n- JVM args: -Xmx1g -verbose:gc --add-modules=jdk.incubator.vector\n-                                                                                \n- Ember-Server service bound to address: [::]:8080\n</code></pre> <p>Let's submit the document corpus for indexing:</p> <pre><code>curl -XPOST -d @movies.jsonl http://localhost:8080/v1/index/movies\n</code></pre> <pre><code>{\"result\":\"created\",\"took\":8256}\n</code></pre> <p>And send a hybrid search request mixing a lexical match and semantic query with the RRF ranking:</p> <pre><code>curl -XPOST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \n    \"query\": {\n      \"rrf\": {\n        \"retrieve\": [\n          {\"match\": {\"title\": \"batman\"}},\n          {\"semantic\": {\"title\": \"batman nolan\"}}\n        ],\n        \"rank_window_size\": 20\n      } \n    }, \n    \"fields\": [\"title\"], \n    \"size\": 5\n  }'\n</code></pre> <p>And you get a response:</p> <pre><code>{\n  \"took\": 8,\n  \"hits\": [\n    {\n      \"_id\": \"414906\",\n      \"title\": \"The Batman\",\n      \"_score\": 0.033333335\n    },\n    {\n      \"_id\": \"272\",\n      \"title\": \"Batman Begins\",\n      \"_score\": 0.032786883\n    },\n    {\n      \"_id\": \"209112\",\n      \"title\": \"Batman v Superman: Dawn of Justice\",\n      \"_score\": 0.031257633\n    },\n    {\n      \"_id\": \"324849\",\n      \"title\": \"The Lego Batman Movie\",\n      \"_score\": 0.031054404\n    },\n    {\n      \"_id\": \"155\",\n      \"title\": \"The Dark Knight\",\n      \"_score\": 0.016129032\n    }\n  ],\n  \"aggs\": {},\n  \"ts\": 1745590503193\n}\n</code></pre> <p>Nixiesearch can do much more, like filtering, facets, autocomplete and RAG out of the box! For more details and more complex queries, see a complete Quickstart guide.</p>"},{"location":"#license","title":"License","text":"<p>This project is released under the Apache 2.0 license, as specified in the License file.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#rest-api-reference","title":"REST API Reference","text":""},{"location":"key_features/","title":"Key Features","text":"<p>Nixiesearch is built to make search simple, scalable, and safe - especially compared to the complexity of running Elasticsearch or OpenSearch in production. Here are a few of the standout features that make Nixiesearch different:</p>"},{"location":"key_features/#schemaless-is-not-ok","title":"\ud83d\udd12 Schemaless is not OK","text":"<p>Unlike databases, where schemaless design often leads to chaos, many search engines (we're watching at you,  Elasticsearch) still let you get away with it, until it breaks everything.</p> <p>Why it matters:</p> <ul> <li> <p>Mapping changes can wreck your index: In Elasticsearch, altering a mapping in an incompatible way while running indexing can silently break your queries, when half of documents are processed in a new way, and half of them stays in an old format.</p> </li> <li> <p>Nixiesearch enforces document schema on ingestion, and validates correctness of schema migrations (so you cannot flip field type from <code>int</code> to <code>text</code> without explicit reindexing). You always know exactly what\u2019s indexed and how. No surprises, no \u201cuh-oh\u201d moments.</p> </li> </ul> <p>Wildcard support is still there when you really need flexibility, but only in a controlled and explicit way.</p>"},{"location":"key_features/#immutable-s3-based-indexes","title":"\ud83d\udce6 Immutable S3-Based Indexes","text":"<p>Storing indexes on S3 isn't just a cost-saver - it changes the game operationally.</p> <p>What\u2019s different:</p> <ul> <li>Truly immutable index files: No risk of corruption or half-written segments. Snapshots and backups are just\u2026 copies on S3.</li> <li>No cluster state: All config lives in plain text. Upgrades, schema changes, and rollbacks are simple file updates. You can GitOps your whole search setup in a simple ArgoCD manifest.</li> <li>Zero-downtime everything: Want to change schema? Just deploy a new config and do a rolling or blue-green restart of the Kubernetes Deployment. No need to babysit an existing cluster.</li> </ul>"},{"location":"key_features/#pull-based-indexing","title":"\ud83e\uddf2 Pull-Based Indexing","text":"<p>Indexing should never interfere with your search workload.</p> <p>What\u2019s different:</p> <ul> <li>Nixiesearch uses a pull-based architecture: indexing runs in a separate tier, so your search nodes never get overloaded.</li> <li>Supports both offline batch (for reindexing) and real-time streaming (Kafka, S3, etc.), so you can scale indexing independently.</li> <li>You get natural backpressure and safe ingestion by design, without needing to tune queues or buffers.</li> </ul>"},{"location":"key_features/#power-of-apache-lucene","title":"\ud83e\udde0 Power of Apache Lucene","text":"<p>At its core, Nixiesearch runs on Apache Lucene \u2014 the same battle-tested engine behind Elasticsearch, but without the operational baggage.</p> <p>What you get:</p> <ul> <li>Lexical, semantic, and hybrid search with Reciprocal Rank Fusion (RRF), plus cross-encoder reranking for neural relevance scoring</li> <li>Facets, filters, autocomplete, and all the advanced query goodness.</li> <li>Embedding inference and RAG out of the box\u2014no extra services needed.</li> </ul> <p>And yes, it's fully open-source\u2014no feature gating or \u201cyou need Enterprise\u201d surprises.</p> <p>Whether you're migrating from Elasticsearch or starting fresh, Nixiesearch offers a radically simpler way to build production-grade search. It's the search engine that feels like infrastructure should in 2025: immutable, stateless, scalable, and smart.</p>"},{"location":"performance/","title":"Performance","text":"<p>TODO</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will show you how to run Nixiesearch in a standalone mode on your local machine using Docker. We will:</p> <ul> <li>start Nixiesearch in a standalone mode using Docker</li> <li>index a demo set of documents using REST API</li> <li>run a couple of search queries</li> </ul>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes that you already have the following available:</p> <ul> <li>Docker: Docker Desktop for Mac/Windows, or Docker for Linux.</li> <li>Operating system: Linux, macOS, Windows with WSL2.</li> <li>Architecture: x86_64 or AArch64.</li> <li>Memory: 2Gb dedicated to Docker.</li> </ul>"},{"location":"quickstart/#getting-the-dataset","title":"Getting the dataset","text":"<p>For this guide we'll use a MSRD: Movie Search Ranking Dataset, which contains textual, categorical and numerical information for each document. Dataset is hosted on Huggingface at nixiesearch/demo-datasets, each document contains following fields:</p> <pre><code>{\n  \"_id\": \"27205\",\n  \"title\": \"Inception\",\n  \"overview\": \"Cobb, a skilled thief who commits corporate espionage by infiltrating the subconscious of his targets is offered a chance to regain his old life as payment for a task considered to be impossible: inception, the implantation of another person's idea into a target's subconscious.\",\n  \"tags\": [\n    \"alternate reality\",\n    \"thought-provoking\",\n    \"visually appealing\"\n  ],\n  \"genres\": [\n    \"action\",\n    \"crime\",\n    \"drama\"\n  ],\n  \"director\": \"Christopher Nolan\",\n  \"actors\": [\n    \"Tom Hardy\",\n    \"Cillian Murphy\",\n    \"Leonardo DiCaprio\"\n  ],\n  \"characters\": [\n    \"Eames\",\n    \"Robert Fischer\",\n  ],\n  \"year\": 2010,\n  \"votes\": 32606,\n  \"rating\": 8.359,\n  \"popularity\": 91.834,\n  \"budget\": 160000000,\n  \"img_url\": \"https://image.tmdb.org/t/p/w154/8IB2e4r4oVhHnANbnm7O3Tj6tF8.jpg\"\n}\n</code></pre> <p>You can download the dataset and unpack it with the following command:</p> <pre><code>curl -L -o movies.jsonl https://nixiesearch.ai/data/movies.jsonl \n</code></pre>"},{"location":"quickstart/#index-schema","title":"Index schema","text":"<p>Unlike other search engines, Nixiesearch requires a strongly-typed description of all the fields of documents you plan to index. As we know that our movie documents from the demo dataset have <code>title</code>, <code>description</code> and some other fields, let's define a <code>movies</code> index in a file <code>config.yml</code>:</p> <pre><code>inference:\n  embedding:\n    e5-small:\n      model: intfloat/e5-small-v2\nschema:\n  movies: # index name\n    fields:\n      title: # field name\n        type: text\n        search:\n          lexical: # build lexical index\n            analyze: english\n          semantic: # and a vector search index also\n            model: e5-small\n        suggest: true\n      overview:\n        type: text\n        search: false\n</code></pre> <p>This YAML file defines: * an embedding model <code>e5-small</code> with a HuggingFace model <code>intfloat/e5-small-v2</code> * a single index <code>movies</code> with a text field <code>title</code> configured for both lexical and semantic search.</p> <p>External Embedding Computation</p> <p>You don't need to configure local embedding inference if you prefer to compute embeddings outside of Nixiesearch. Instead of the <code>model</code> parameter, you can use the <code>dim</code> parameter and provide pre-computed embeddings with your documents. See text fields documentation for details.</p> <p>Note</p> <p>Each document field definition must have a type. Schemaless dynamic mapping is considered an anti-pattern, as the search engine must know beforehand which structure to use for the index. int, float, long, double, text, text[], bool field types are currently supported.</p> <p>See a full index mapping reference for more details on defining indexes, and ML inference on configuring ML models inside Nixiesearch for CPU and GPU.</p>"},{"location":"quickstart/#starting-the-service","title":"Starting the service","text":"<p>Nixiesearch is distributed as a Docker container, which can be run with the following command:</p> <pre><code>docker run -itp 8080:8080 -v .:/data nixiesearch/nixiesearch:latest standalone -c /data/config.yml\n</code></pre> <pre><code>a.nixiesearch.index.sync.LocalIndex$ - Local index movies opened\nai.nixiesearch.index.Searcher$ - opening index movies\na.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\na.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\na.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\na.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\na.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2554\u255d \u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\na.n.main.subcommands.StandaloneMode$ - \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\na.n.main.subcommands.StandaloneMode$ -                                                                                \no.h.ember.server.EmberServerBuilder - Ember-Server service bound to address: [::]:8080\n</code></pre> <p>Options breakdown:</p> <ul> <li><code>-i</code> and <code>-t</code>: interactive docker mode with allocated TTY. Useful when you want to be able to press Ctrl-C to stop the application.</li> <li><code>-p 8080:8080</code>: expose the port 8080.</li> <li><code>-v .:/data</code>: mount current dir (with a <code>config.yml</code> file!) as a <code>/data</code> inside the container</li> <li><code>standalone</code>: a Nixiesearch running mode, with colocated indexer and searcher processes.</li> <li><code>-c /data/config.yml</code>: use a config file with <code>movies</code> index mapping</li> </ul> <p>Note</p> <p>Standalone mode is designed for small-scale and development deployments: it uses local filesystem for index storage, and runs both indexer and searcher within a single application. For production usage please consider a distributed mode over S3-compatible block storage.</p>"},{"location":"quickstart/#indexing-data","title":"Indexing data","text":"<p>After you start the Nixiesearch service in the <code>standalone</code> mode listening on port <code>8080</code>, let's index some docs with REST API!</p> <p>Nixiesearch uses a similar API semantics as Elasticsearch, so to upload docs for indexing, you need to make a HTTP PUT request to the <code>/&lt;index-name&gt;/_index</code> endpoint:</p> <pre><code>curl -XPOST -d @movies.jsonl http://localhost:8080/v1/index/movies\n</code></pre> <pre><code>{\"result\":\"created\",\"took\":8256}\n</code></pre> <p>As Nixiesearch is running a local embedding model inference inside, indexing large document corpus on CPU may take a while. Optionally you can use API-based embedding providers like OpenAI and Cohere.</p> <p>Note</p> <p>Nixiesearch can also index documents directly from a local file, S3 bucket or Kafka topic in a pull-based scenario. Both in realtime and offline. Check Building index reference for more information about indexing your data.</p>"},{"location":"quickstart/#sending-search-requests","title":"Sending search requests","text":"<p>Query DSL in Nixiesearch is inspired but not compatible with the JSON syntax used in Elasticsearch/OpenSearch Query DSL.</p> <p>To perform a single-field lexical search over our newly-created <code>movies</code> index, run the following cURL command:</p> <pre><code>curl -XPOST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \n    \"query\": { \n      \"match\": { \n        \"title\": \"batman\" \n      } \n    }, \n    \"fields\": [\"title\"], \n    \"size\": 5\n  }'\n</code></pre> <p>You will get the following response:</p> <pre><code>{\n  \"took\": {\n    \"total\": 0.001,\n    \"search\": 0.0005\n  },\n  \"hits\": [\n    {\n      \"_id\": \"414906\",\n      \"title\": \"The Batman\",\n      \"_score\": 3.0470526\n    },\n    {\n      \"_id\": \"272\",\n      \"title\": \"Batman Begins\",\n      \"_score\": 2.4646688\n    },\n    {\n      \"_id\": \"324849\",\n      \"title\": \"The Lego Batman Movie\",\n      \"_score\": 2.0691848\n    },\n    {\n      \"_id\": \"209112\",\n      \"title\": \"Batman v Superman: Dawn of Justice\",\n      \"_score\": 1.5664694\n    }\n  ],\n  \"aggs\": {},\n  \"ts\": 1745590547587\n}\n</code></pre> <p>Let's go deeper and perform hybrid search query, by mixing lexical (using match query) and semantic (using semantic query) with Reciprocal Rank Fusion:</p> <pre><code>curl -XPOST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": {\n      \"rrf\": {\n        \"retrieve\": [\n          { \"match\": { \"title\": \"batman\" } },\n          { \"semantic\": { \"title\": \"batman nolan\" } }\n        ],\n        \"rank_window_size\": 20\n      }\n    },\n    \"fields\": [\"title\"],\n    \"size\": 5\n  }'\n</code></pre> <p>And we also got a \"The Dark Knight\" movie!</p> <pre><code>{\n  \"took\": {\n    \"total\": 0.008,\n    \"search\": 0.004,\n    \"rerank\": 0.002\n  },\n  \"hits\": [\n    {\n      \"_id\": \"414906\",\n      \"title\": \"The Batman\",\n      \"_score\": 0.033333335\n    },\n    {\n      \"_id\": \"272\",\n      \"title\": \"Batman Begins\",\n      \"_score\": 0.032786883\n    },\n    {\n      \"_id\": \"209112\",\n      \"title\": \"Batman v Superman: Dawn of Justice\",\n      \"_score\": 0.031257633\n    },\n    {\n      \"_id\": \"324849\",\n      \"title\": \"The Lego Batman Movie\",\n      \"_score\": 0.031054404\n    },\n    {\n      \"_id\": \"155\",\n      \"title\": \"The Dark Knight\",\n      \"_score\": 0.016129032\n    }\n  ],\n  \"aggs\": {},\n  \"ts\": 1745590503193\n}\n</code></pre> <p>This query performed a hybrid search:</p> <ul> <li>for lexical search, it built and executed a Lucene query of <code>title:batman</code></li> <li>for semantic search, it computed an LLM embedding of the query <code>batman nolan</code> and performed a-kNN search over document embeddings, stored in Lucene HNSW index.</li> <li>combined results of both searches into a single ranking with the Reciprocal Rank Fusion.</li> </ul> <p>Note</p> <p>Learn more about searching in the Search section.</p>"},{"location":"quickstart/#next-steps","title":"Next steps","text":"<p>If you want to continue learning about Nixiesearch, these sections of documentation are great next steps:</p> <ul> <li>An overview of Nixiesearch design to understand how it differs from existing search engines.</li> <li>Using Filters and Facets while searching.</li> <li>How it should be deployed in a production environment.</li> <li>Building semantic autocomplete index for search-as-you-type support.</li> </ul> <p>If you have a question not covered in these docs and want to chat with the team behind Nixiesearch, you're welcome to join our Community Slack</p>"},{"location":"deployment/overview/","title":"Deployment","text":"<p>Nixiesearch is distributed as a single Docker container (with a special flavors for CPU and GPU inference) and can be deployed as a regular dockerized app:</p> <ul> <li>Standalone: simple single-node install with Docker, with search and index API colocated within a single process. Recommended for dev environments and installs with no need for fault tolerance.</li> <li>Distributed: distributed setup with separate indexer and searcher services, backed by S3-compatible block storage. Recommended for large and fault-tolerant installs.</li> </ul>"},{"location":"deployment/overview/#docker-images","title":"Docker Images","text":"<p>Nixiesearch offers different Docker image variants to match your deployment requirements:</p>"},{"location":"deployment/overview/#jdk-based-images","title":"JDK-Based Images","text":"<p>Standard images built with OpenJDK, tagged as <code>&lt;version&gt;</code> (e.g., <code>0.8.0</code>, <code>latest</code>):</p> <ul> <li>Architecture support: x86_64 and arm64</li> <li>Features: Full ONNX embedding inference and llamacpp LLM support</li> <li>Use cases: Production deployments with semantic search and RAG capabilities</li> <li>GPU variant: Available with <code>-gpu</code> suffix (e.g., <code>latest-gpu</code>) for CUDA acceleration</li> </ul>"},{"location":"deployment/overview/#native-images","title":"Native Images","text":"<p>GraalVM Native Image builds for faster startup, tagged as <code>&lt;version&gt;-native</code> (e.g., <code>0.8.0-native</code>, <code>latest-native</code>):</p> <ul> <li>Architecture support: x86_64 only</li> <li>Features: Lexical search and API-based inference only (no ONNX or llamacpp)</li> <li>Use cases: Serverless deployments (Lambda), environments requiring fast cold starts</li> <li>Status: Highly experimental</li> </ul> <p>Choose JDK-based images for production workloads with full feature support, or native images for serverless deployments where fast startup matters more than local inference capabilities.</p>"},{"location":"deployment/overview/#container-registries","title":"Container Registries","text":"<p>Nixiesearch Docker images are available from two registries:</p>"},{"location":"deployment/overview/#docker-hub-primary","title":"Docker Hub (Primary)","text":"<pre><code>nixiesearch/nixiesearch:&lt;tag&gt;\n</code></pre> <p>Docker Hub is the default registry for most deployments. All image variants and tags are available:</p> <ul> <li><code>nixiesearch/nixiesearch:latest</code> - Latest JDK-based multi-arch image (Docker automatically pulls the correct variant for your platform)</li> <li><code>nixiesearch/nixiesearch:0.8.0</code> - Specific version (multi-arch)</li> <li><code>nixiesearch/nixiesearch:0.8.0-amd64</code> - Platform-specific x86_64 variant</li> <li><code>nixiesearch/nixiesearch:0.8.0-arm64</code> - Platform-specific ARM64 variant</li> <li><code>nixiesearch/nixiesearch:latest-gpu</code> - GPU-enabled variant</li> <li><code>nixiesearch/nixiesearch:latest-native</code> - Native GraalVM image</li> <li><code>nixiesearch/nixiesearch:0.8.0-native-amd64</code> - Specific native version</li> </ul> <p>Multi-arch tags (like <code>0.8.0</code> and <code>latest</code>) use Docker manifests to automatically pull the correct platform-specific image for your architecture.</p>"},{"location":"deployment/overview/#aws-ecr-public-registry","title":"AWS ECR Public Registry","text":"<pre><code>public.ecr.aws/nixiesearch/nixiesearch:&lt;tag&gt;\n</code></pre> <p>AWS ECR Public Registry mirrors all Docker Hub images and is required for certain AWS services:</p> <ul> <li>AWS Lambda: Lambda deployments must use ECR images (see Lambda deployment guide)</li> <li>Same tags and variants as Docker Hub</li> <li>Lower latency when deploying to AWS infrastructure</li> </ul> <p>Both registries are kept in sync and contain identical images. Choose Docker Hub for general use, or ECR when deploying to AWS Lambda or when you need optimized performance within AWS.</p>"},{"location":"deployment/overview/#standalone","title":"Standalone","text":"<p>In standalone mode searcher and indexer are colocated on a single Nixiesearch process:</p> <p></p> <p>Standalone mode is easy to start as it enforces a set of simplifications for the setup:</p> <ul> <li>only REST API based document indexing is possible. </li> <li>S3-based index sync is not enabled.</li> </ul> <p>To start nixiesearch in a standalone mode, use the nixiesearch standalone subcommand (where <code>config.yml</code> is an index mapping configuration file):</p> <pre><code>docker run -i -t -v &lt;data dir&gt;:/data/ -p 8080:8080 nixiesearch/nixiesearch:latest\\\n  standalone --config config.yml\n</code></pre>"},{"location":"deployment/overview/#distributed","title":"Distributed","text":"<p>For production installs Nixiesearch can be rolled out in a distributed manner:</p> <ul> <li>separate searchers and indexer. Potentially searchers can be auto-scaled, and even scaled to zero.</li> <li>sync between indexer and searchers happens via S3-compatible block storage.</li> </ul> <p></p> <p>Nixiesearch has separate nixiesearch index and nixiesearch search sub-commands to run these sub-tasks.</p> <p>Helm chart for smooth k8s deployment is planned for v0.3.</p>"},{"location":"deployment/standalone/","title":"Standalone Docker Deployment","text":"<p>Nixiesearch can be run as a single Docker container for local development, testing, and small-scale production workloads. This deployment method combines indexing and search functionality in a single process, making it the simplest way to get started with Nixiesearch. For a complete introduction, see the quickstart guide.</p>"},{"location":"deployment/standalone/#configuration-setup","title":"Configuration Setup","text":"<p>Create a minimal <code>config.yml</code> file for standalone operation. For complete configuration options, see the configuration reference:</p> <pre><code>store:\n  local:\n    path: /data\n\nschema:\n  movies:\n    fields:\n      title:\n        type: text\n        search:\n          lexical:\n            analyze: english\n      overview:\n        type: text\n        search:\n          lexical:\n            analyze: english\n      year:\n        type: int\n        facet: true\n</code></pre> <p>For more details on field types and mapping configuration, see the index mapping documentation.</p>"},{"location":"deployment/standalone/#with-semantic-search-and-llm-features","title":"With Semantic Search and LLM Features","text":"<p>To enable semantic search, embeddings, and LLM capabilities:</p> <pre><code>store:\n  local:\n    path: /data\n\ninference:\n  embedding:\n    e5-small:\n      model: intfloat/e5-small-v2\n  completion:\n    qwen2:\n      provider: llamacpp\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n\nschema:\n  movies:\n    fields:\n      title:\n        type: text\n        search:\n          lexical: \n            analyze: english\n          semantic:\n            model: e5-small\n      overview:\n        type: text\n        search:\n          lexical:\n            analyze: english\n          semantic:\n            model: e5-small\n      year:\n        type: int\n        facet: true\n</code></pre>"},{"location":"deployment/standalone/#local-storage-setup","title":"Local Storage Setup","text":"<p>Nixiesearch standalone mode uses local filesystem storage for index persistence. </p> <pre><code>docker run -p 8080:8080 \\\n  -v $(pwd)/config.yml:/config.yml \\\n  -v $(pwd)/data:/data \\\n  nixiesearch/nixiesearch:latest \\\n  standalone --config /config.yml\n</code></pre> <p>This exposes the API on port 8080, mounts your configuration file, creates a local data directory for index storage, and runs in standalone mode.</p> <p>Note</p> <p>Storage Considerations: Plan for 1.5-2x your document size for index storage. LLM models can be 100MB-2GB each, so ensure adequate space. Use SSD storage for better search performance.</p>"},{"location":"deployment/standalone/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>For easier management, use Docker Compose:</p> <pre><code>version: '3.8'\n\nservices:\n  nixiesearch:\n    image: nixiesearch/nixiesearch:latest\n    command: standalone --config /config.yml\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - ./config.yml:/config.yml\n      - ./data:/data\n      - ./models:/models\n    environment:\n      - JAVA_OPTS=-Xmx2g\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n</code></pre> <p>Start with <code>docker compose up</code>.</p>"},{"location":"deployment/standalone/#usage-examples","title":"Usage Examples","text":""},{"location":"deployment/standalone/#indexing-documents","title":"Indexing Documents","text":"<pre><code>curl -X POST http://localhost:8080/v1/index/movies \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"title\": \"The Matrix\",\n    \"overview\": \"A computer programmer discovers reality is a simulation\",\n    \"year\": 1999\n  }'\n</code></pre>"},{"location":"deployment/standalone/#basic-search","title":"Basic Search","text":"<p>Use match queries for lexical search:</p> <pre><code>curl -X POST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": {\"match\": {\"title\": \"matrix\"}},\n    \"fields\": [\"title\"],\n    \"size\": 10\n  }'\n</code></pre>"},{"location":"deployment/standalone/#semantic-search","title":"Semantic Search","text":"<p>Use semantic queries for vector-based search:</p> <pre><code>curl -X POST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": {\"semantic\": {\"overview\": \"computer simulation reality\"}},\n    \"fields\": [\"title\"],\n    \"size\": 5\n  }'\n</code></pre> <p>For more query examples, see the search overview and query DSL documentation.</p>"},{"location":"deployment/standalone/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<p>Check service health with <code>curl http://localhost:8080/health</code> and monitor index statistics using <code>curl http://localhost:8080/v1/index/movies/_stats</code>. View container logs with <code>docker logs nixiesearch-container</code> or follow them in real-time with <code>docker logs -f nixiesearch-container</code>. For advanced monitoring setup, see the metrics documentation.</p>"},{"location":"deployment/standalone/#next-steps-distributed-deployment","title":"Next Steps: Distributed Deployment","text":"<p>For production workloads requiring higher availability and scale, see the distributed deployment guide.</p>"},{"location":"deployment/distributed/gpu/","title":"GPU support","text":"<p>Nixiesearch supports both CPU and GPU inference for embeddings and generative models:</p> <ul> <li>for embedding inference the ONNXRuntime is used with CPU and CUDA Execution Providers.</li> <li>for GenAI inference the llamacpp backend is used with both CUDA and CPU support built-in.</li> </ul> <p>All official Nixiesearch Docker containers on hub.docker.com/u/nixiesearch starting from a <code>0.3.0</code> version are published in two flavours:</p> <ul> <li>with a <code>-gpu</code> suffix: <code>nixiesearch/nixiesearch:0.3.0-amd64-gpu</code> which includes GPU support. These containers include GPU native libraries and CUDA runtime, so their size is huge: ~6Gb.</li> <li>without the suffix: <code>nixiesearch/nixiesearch:0.3.0</code>. No GPU native libs, no CUDA runtime, slim size of 700Mb.</li> </ul> <p>Note</p> <p>Nixiesearch currently supports CUDA12 on Linux-x86_64 only. If you need AArch64 support, please open a ticket with your use-case.</p> <p>Note</p> <p>Nixiesearch currently supports only single-GPU inference for embedding models. If your host has 2+ GPUs, Nixiesearch will use the first one only. Generative models can use any number of GPUs.</p>"},{"location":"deployment/distributed/gpu/#gpu-pass-through-with-docker","title":"GPU pass-through with Docker","text":"<p>To perform a GPU pass-through from your host machine to the Nixiesearch docker container, you need to have nvidia-container-toolkit installed and configured. AWS NVIDIA GPU-Optimized AMI and GCP Deep Learning VM Image support this out of the box.</p> <p>To validate that the pass-through works correctly, pass the <code>--gpus all</code> flag to docker for a sample workload:</p> <pre><code>docker run --gpus all ubuntu nvidia-smi\n</code></pre> <p>To run Nixiesearch in a standalone mode with GPU support:</p> <pre><code>docker run --gpus all -itv &lt;dir&gt;:/data nixiesearch/nixiesearch:latest-gpu \\\n    standalone -c /data/config.yml\n</code></pre> <p>When GPU gets detected, you'll get the following log:</p> <pre><code>12:42:22.450 INFO  ai.nixiesearch.main.Main$ - ONNX CUDA EP Found: GPU Build\n12:42:22.492 INFO  ai.nixiesearch.main.Main$ - GPU 0: NVIDIA GeForce RTX 4090\n12:42:22.492 INFO  ai.nixiesearch.main.Main$ - GPU 1: NVIDIA GeForce RTX 4090\n...\n14:11:23.629 INFO  a.n.c.n.m.embedding.EmbedModelDict$ - loading model.onnx\n14:11:23.629 INFO  a.n.c.n.m.embedding.EmbedModelDict$ - Fetching hf://nixiesearch/e5-small-v2-onnx from HF: model=model.onnx tokenizer=tokenizer.json\n14:11:23.630 INFO  a.n.core.nn.model.HuggingFaceClient - found cached /home/shutty/cache/models/nixiesearch/e5-small-v2-onnx/model.onnx file for requested nixiesearch/e5-small-v2-onnx/model.onnx\n14:11:23.630 INFO  a.n.core.nn.model.HuggingFaceClient - found cached /home/shutty/cache/models/nixiesearch/e5-small-v2-onnx/tokenizer.json file for requested nixiesearch/e5-small-v2-onnx/tokenizer.json\n14:11:23.631 INFO  a.n.core.nn.model.HuggingFaceClient - found cached /home/shutty/cache/models/nixiesearch/e5-small-v2-onnx/config.json file for requested nixiesearch/e5-small-v2-onnx/config.json\n14:11:23.636 INFO  a.n.c.n.m.e.EmbedModel$OnnxEmbedModel$ - Embedding model scheduled for GPU inference\n...\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\nllm_load_tensors: ggml ctx size =    0.38 MiB\nllm_load_tensors: offloading 24 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 25/25 layers to GPU\nllm_load_tensors:        CPU buffer size =   137.94 MiB\nllm_load_tensors:      CUDA0 buffer size =   104.91 MiB\nllm_load_tensors:      CUDA1 buffer size =   226.06 MiB\n...........................................\nllama_new_context_with_model: n_ctx      = 32768\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 1000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   208.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =   176.00 MiB\nllama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     2.90 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nllama_new_context_with_model:      CUDA0 compute buffer size =  1166.01 MiB\nllama_new_context_with_model:      CUDA1 compute buffer size =  1166.02 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   257.77 MiB\nllama_new_context_with_model: graph nodes  = 846\nllama_new_context_with_model: graph splits = 3\n[INFO] initializing slots n_slots=4\n[INFO] new slot id_slot=0 n_ctx_slot=8192\n[INFO] new slot id_slot=1 n_ctx_slot=8192\n[INFO] new slot id_slot=2 n_ctx_slot=8192\n[INFO] new slot id_slot=3 n_ctx_slot=8192\n[INFO] model loaded\n</code></pre> <p>After the successful startup you can see the Nixiesearch process in the <code>nvidia-smi</code>:</p> <pre><code>+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4090        Off |   00000000:41:00.0 Off |                  Off |\n|  0%   56C    P0             67W /  450W |    2064MiB /  24564MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 4090        Off |   00000000:C1:00.0 Off |                  Off |\n|  0%   56C    P0             72W /  450W |    1984MiB /  24564MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A    324023      C   java                                         2054MiB |\n|    1   N/A  N/A    324023      C   java                                         1974MiB |\n+-----------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"deployment/distributed/kubernetes/","title":"Kubernetes Deployment","text":"<p>This page has moved to overview.md.</p> <p>The comprehensive Kubernetes deployment guide, including both standalone and distributed deployment options, is now available in the distributed deployment overview.</p>"},{"location":"deployment/distributed/lambda/","title":"AWS Lambda Deployment","text":"<p>You can run Nixiesearch searcher as an AWS Lambda function for serverless deployments. This works well for variable traffic patterns where you'd rather pay per request than keep servers running around the clock.</p> <p>Searcher Only</p> <p>Lambda deployment only works for searcher nodes. You'll need to run the indexer separately on Kubernetes, ECS, or EC2.</p>"},{"location":"deployment/distributed/lambda/#image-selection","title":"Image Selection","text":"<p>Nixiesearch offers two Docker image types with different tradeoffs:</p>"},{"location":"deployment/distributed/lambda/#jdk-images-nixiesearchnixiesearchlatest","title":"JDK Images (<code>nixiesearch/nixiesearch:latest</code>)","text":"<p>These are the standard production images with full feature support:</p> <ul> <li>Complete ONNX embedding and llamacpp LLM inference</li> <li>Larger image size (~500MB-1GB) with 1-3 second cold starts</li> <li>Use these for semantic search with local embedding models</li> </ul>"},{"location":"deployment/distributed/lambda/#native-images-nixiesearchnixiesearchlatest-native","title":"Native Images (<code>nixiesearch/nixiesearch:latest-native</code>)","text":"<p>GraalVM native images optimized for serverless:</p> <ul> <li>Zero JVM warmup time, smaller footprint (~50-100MB), sub-500ms cold starts</li> <li>Highly experimental - stability not guaranteed</li> <li>No ONNX embedding or llamacpp support - API-based inference only</li> <li>Good for lexical search or when using API-based embeddings (OpenAI, Cohere)</li> </ul>"},{"location":"deployment/distributed/lambda/#storage-options","title":"Storage Options","text":"<p>Lambda searchers need access to pre-built indexes. You have two choices:</p>"},{"location":"deployment/distributed/lambda/#s3-storage","title":"S3 Storage","text":"<p>Index segments live in S3, synced to Lambda ephemeral storage on initialization:</p> <pre><code>schema:\n  movies:\n    store:\n      distributed:\n        searcher:\n          local:\n            disk:\n              path: /tmp/index\n        indexer:\n          memory:\n        remote:\n          s3:\n            bucket: your-nixiesearch-indexes\n            prefix: movies\n            region: us-west-2\n</code></pre> <p>Cold starts include index sync time (2-10 seconds depending on index size), but search queries run fast since the index can use page cache. Doesn't require VPC configuration.</p> <p>Indexes are continuously updated - Nixiesearch pulls new segments from S3 when the indexer writes new documents, so searchers stay in sync automatically.</p>"},{"location":"deployment/distributed/lambda/#efs-storage","title":"EFS Storage","text":"<p>Lambda mounts an EFS volume where the indexer writes directly:</p> <pre><code>schema:\n  movies:\n    store:\n      local:\n        path: /mnt/efs/indexes/movies\n</code></pre> <p>No sync wait on cold start, but search queries have higher latency (50-100ms) due to network I/O. Requires VPC setup with proper security groups.</p>"},{"location":"deployment/distributed/lambda/#index-size-considerations","title":"Index Size Considerations","text":"<p>Nixiesearch uses memory-mapped I/O (mmap in JDK builds, NIO in native builds) to access index data, so there's no hard limit on index size. For native builds, you can explicitly configure <code>directory: \"nio\"</code> in the index configuration if needed. However, for semantic search with HNSW vector indexes, if your index doesn't fit in available RAM, expect search latency to jump roughly 10x due to random disk reads during graph traversal.</p> <p>Note that Lambda has a practical 3GB memory limit (not 10GB as some docs suggest) unless you submit a quota increase request through AWS Support.</p>"},{"location":"deployment/distributed/lambda/#resource-configuration","title":"Resource Configuration","text":"<p>Lambda allocates CPU proportionally to memory. For search workloads, max out at 3008MB to get the fastest CPU available (~1.8 vCPUs).</p> <pre><code>MemorySize: 3008\nTimeout: 60\nEphemeralStorage:\n  Size: 2048  # Only needed for S3-based indexes\n</code></pre> <p>For S3 storage, allocate ephemeral storage roughly equal to your index size. For EFS storage, ephemeral storage can be minimal.</p> <p>We don't recommend running ONNX embedding inference on Lambda - the CPU is too limited. Use API-based embedding providers (OpenAI, Cohere) instead.</p>"},{"location":"deployment/distributed/lambda/#networking","title":"Networking","text":"<p>By default, Lambda functions have internet access. If you're using API-based embeddings and don't need VPC resources, you're all set.</p> <p>If you put Lambda in a VPC (for EFS or private resources), you'll need to set up proper routing for external API calls:</p> <pre><code>Lambda (private subnet) \u2192 NAT Gateway (public subnet) \u2192 Internet Gateway\n</code></pre>"},{"location":"deployment/distributed/lambda/#iam-permissions","title":"IAM Permissions","text":"<p>For S3-based deployment:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n      \"Resource\": [\n        \"arn:aws:s3:::your-nixiesearch-indexes\",\n        \"arn:aws:s3:::your-nixiesearch-indexes/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\"\n    }\n  ]\n}\n</code></pre> <p>For VPC deployment, add:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"ec2:CreateNetworkInterface\",\n    \"ec2:DescribeNetworkInterfaces\",\n    \"ec2:DeleteNetworkInterface\"\n  ],\n  \"Resource\": \"*\"\n}\n</code></pre> <p>For EFS storage, add:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"elasticfilesystem:ClientMount\",\n    \"elasticfilesystem:ClientWrite\"\n  ],\n  \"Resource\": \"arn:aws:elasticfilesystem:region:account-id:file-system/fs-xxxxx\"\n}\n</code></pre>"},{"location":"deployment/distributed/lambda/#configuration","title":"Configuration","text":"<p>Here's a typical config for Lambda with S3 storage and API-based embeddings:</p> <pre><code>inference:\n  embedding:\n    openai-embed:\n      provider: openai\n      model: text-embedding-3-small\n\nschema:\n  movies:\n    store:\n      distributed:\n        searcher:\n          local:\n            disk:\n              path: /tmp/index\n        indexer:\n          memory:\n        remote:\n          s3:\n            bucket: your-nixiesearch-indexes\n            prefix: movies\n            region: us-west-2\n    fields:\n      title:\n        type: text\n        search:\n          lexical:\n            analyze: english\n          semantic:\n            model: openai-embed\n      overview:\n        type: text\n        search:\n          lexical:\n            analyze: english\n</code></pre> <p>API keys are passed via Lambda environment variables (see deployment example below).</p> <p>See the configuration reference for all available options.</p>"},{"location":"deployment/distributed/lambda/#deployment","title":"Deployment","text":"<p>Private ECR Registry Required</p> <p>AWS Lambda requires container images to be stored in a private ECR registry in the same region as your Lambda function. While Nixiesearch images are hosted in the public ECR registry, Lambda cannot use them directly.</p> <p>To deploy Nixiesearch on Lambda:</p> <ol> <li> <p>Pull the public image:    <pre><code>docker pull public.ecr.aws/nixiesearch/nixiesearch:latest\n</code></pre></p> </li> <li> <p>Tag it for your private ECR registry:    <pre><code>docker tag public.ecr.aws/nixiesearch/nixiesearch:latest \\\n  XXXXXXX.dkr.ecr.us-east-1.amazonaws.com/nixiesearch:latest\n</code></pre></p> </li> <li> <p>Authenticate and push to your private ECR:    <pre><code>aws ecr get-login-password --region us-east-1 | \\\n  docker login --username AWS --password-stdin XXXXXXX.dkr.ecr.us-east-1.amazonaws.com\ndocker push XXXXXXX.dkr.ecr.us-east-1.amazonaws.com/nixiesearch:latest\n</code></pre></p> </li> </ol> <p>Replace <code>XXXXXXX</code> with your AWS account ID and <code>us-east-1</code> with your Lambda's region. See the Container Registries section for more details.</p> <p>Using Terraform:</p> <pre><code>resource \"aws_lambda_function\" \"nixiesearch\" {\n  function_name = \"nixiesearch-searcher\"\n  role          = aws_iam_role.lambda_role.arn\n  package_type  = \"Image\"\n  image_uri     = \"XXXXXXX.dkr.ecr.us-east-1.amazonaws.com/nixiesearch:latest\"\n  memory_size   = 3008\n  timeout       = 60\n\n  image_config {\n    command = [\"search\", \"--config\", \"/config.yml\", \"--api\", \"lambda\"]\n  }\n\n  ephemeral_storage {\n    size = 2048\n  }\n\n  environment {\n    variables = {\n      OPENAI_API_KEY = var.openai_api_key\n    }\n  }\n}\n\nresource \"aws_apigatewayv2_api\" \"nixiesearch\" {\n  name          = \"nixiesearch-api\"\n  protocol_type = \"HTTP\"\n}\n\nresource \"aws_apigatewayv2_integration\" \"nixiesearch\" {\n  api_id           = aws_apigatewayv2_api.nixiesearch.id\n  integration_type = \"AWS_PROXY\"\n  integration_uri  = aws_lambda_function.nixiesearch.invoke_arn\n  payload_format_version = \"2.0\"\n}\n</code></pre> <p>Note: Nixiesearch requires API Gateway V2 (HTTP API), not the older REST API or V1.</p>"},{"location":"deployment/distributed/lambda/#usage","title":"Usage","text":"<p>First, build your indexes using a separate indexer process:</p> <pre><code>docker run -it nixiesearch/nixiesearch:latest index file \\\n  --config /config.yml \\\n  --index movies \\\n  --url /data/movies.jsonl\n</code></pre> <p>Deploy the Lambda function:</p> <pre><code>terraform apply\n</code></pre> <p>Test it:</p> <pre><code>curl -X POST \"https://your-api-id.execute-api.region.amazonaws.com/v1/index/movies/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": {\"match\": {\"title\": \"matrix\"}},\n    \"fields\": [\"title\"],\n    \"size\": 5\n  }'\n</code></pre>"},{"location":"deployment/distributed/lambda/#next-steps","title":"Next Steps","text":"<ul> <li>Set up monitoring with CloudWatch metrics</li> <li>Configure autoscaling with provisioned concurrency</li> <li>Review search features and API usage</li> <li>Implement backup procedures for your S3 indexes</li> </ul> <p>For other deployment options, see Kubernetes deployment and standalone deployment.</p>"},{"location":"deployment/distributed/overview/","title":"Running Nixiesearch in Kubernetes","text":"<p>Nixiesearch offers two main deployment patterns for Kubernetes, each designed for different use cases and operational requirements. This guide covers both approaches and helps you choose the right one for your needs.</p> <p>For a broader overview of deployment options, see the deployment overview. If you're new to Nixiesearch, consider starting with our quickstart guide.</p>"},{"location":"deployment/distributed/overview/#deployment-options-overview","title":"Deployment Options Overview","text":""},{"location":"deployment/distributed/overview/#standalone-deployment","title":"Standalone Deployment","text":"<p>Standalone Deployment is perfect for getting started or running smaller workloads. Everything runs in a single pod with persistent storage for your data. For more details, see the standalone deployment guide.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Kubernetes Pod            \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502         Nixiesearch             \u2502 \u2502\n\u2502 \u2502       (All services)            \u2502 \u2502\n\u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502 \u2502\n\u2502 \u2502  \u2502 Search  \u2502  \u2502   Indexing  \u2502   \u2502 \u2502\n\u2502 \u2502  \u2502 Engine  \u2502  \u2502   Service   \u2502   \u2502 \u2502\n\u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502       Persistent Volume         \u2502\n    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n    \u2502  \u2502   Indexes   \u2502 \u2502   Cache   \u2502  \u2502\n    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/distributed/overview/#distributed-deployment","title":"Distributed Deployment","text":"<p>Distributed Deployment is built for production scale. It separates search and indexing into different components that communicate through S3, giving you better performance and reliability.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Searcher      \u2502    \u2502   Searcher      \u2502\n\u2502   pod #1        \u2502    \u2502   pod #2        \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502Search engine\u2502 \u2502    \u2502 \u2502Search engine\u2502 \u2502\n\u2502 \u2502(inmem index)\u2502 \u2502    \u2502 \u2502(inmem index)\u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                      \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502 Download index segments\n                     \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502     S3      \u2502\n              \u2502   storage   \u2502\n              \u2502  (indexes)  \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u25b2\n                     \u2502 Upload index segments\n                     \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Indexer pod               \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502        Nixiesearch indexer      \u2502 \u2502\n\u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502 \u2502  \u2502   Document  \u2502 \u2502   Index   \u2502  \u2502 \u2502\n\u2502 \u2502  \u2502 processing  \u2502 \u2502 building  \u2502  \u2502 \u2502\n\u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/distributed/overview/#standalone-deployment_1","title":"Standalone Deployment","text":"<p>The standalone deployment is the simplest way to run Nixiesearch on Kubernetes. It's great for development, testing, or smaller production workloads where you want to keep things simple.</p>"},{"location":"deployment/distributed/overview/#what-you-get","title":"What You Get","text":"<p>With standalone deployment, you get a single pod running all Nixiesearch functionality. Your data is stored on a persistent volume to keep it safe across pod restarts. The configuration is minimal with very few moving parts, and you can easily scale by adjusting pod resources.</p>"},{"location":"deployment/distributed/overview/#getting-started","title":"Getting Started","text":"<p>The standalone deployment comes with everything you need in the <code>deploy/kubernetes/standalone</code> directory. You'll find the configuration setup with a quickstart movie schema, a 10GB persistent volume for your data, the main Nixiesearch application manifest, and a service that exposes the app on port 8080. The configuration uses the same schema mapping principles as other deployments.</p> <pre><code># Deploy everything at once\ncd deploy/kubernetes/standalone\nkubectl apply -f .\n</code></pre> <p>Your data gets organized under <code>/data</code> inside the pod, with search indexes stored in <code>/data/indexes</code> and cache files in <code>/data/cache</code>.</p>"},{"location":"deployment/distributed/overview/#manifests-overview","title":"Manifests Overview","text":"<p>The standalone deployment includes these YAML manifests:</p> <ul> <li><code>configmap.yaml</code> - Contains Nixiesearch configuration with quickstart movie schema, persistent storage paths, and embedding model settings</li> <li><code>pvc.yaml</code> - PersistentVolumeClaim requesting 10GB storage for indexes and cache data</li> <li><code>deployment.yaml</code> - Main Nixiesearch application running in standalone mode with volume mounts and health checks</li> <li><code>service.yaml</code> - ClusterIP service exposing the application on port 8080 for internal cluster access</li> </ul>"},{"location":"deployment/distributed/overview/#when-to-use-standalone","title":"When to Use Standalone","text":"<p>Standalone deployment works well for development and testing since it's quick to set up and tear down. It handles small to medium workloads effectively when you don't need separate scaling of components. The operational simplicity means fewer components to manage, making it easier to understand and debug while you're learning Nixiesearch.</p>"},{"location":"deployment/distributed/overview/#storage-considerations","title":"Storage Considerations","text":"<p>The standalone deployment relies on a PersistentVolumeClaim, so your cluster should have either a default storage class configured or you'll need to specify one in the PVC manifest. The beauty of this approach is that your data persists across pod restarts, which is exactly what you want for a search index that takes time to build. For configuration details, see the cache configuration documentation.</p>"},{"location":"deployment/distributed/overview/#distributed-deployment_1","title":"Distributed Deployment","text":"<p>The distributed deployment splits Nixiesearch into specialized components. This approach gives you better performance, scalability, and operational flexibility for production workloads where you need more control over how different parts of the system behave.</p> <p>For serverless deployments, see the AWS Lambda deployment guide which covers running searchers as Lambda functions.</p>"},{"location":"deployment/distributed/overview/#what-you-get_1","title":"What You Get","text":"<p>In a distributed setup, you get dedicated searcher pods (typically 2 replicas) that handle all search queries, plus a single indexer pod that processes documents and builds indexes. Everything stays synchronized through S3 storage, and you can scale each component independently. The multiple searcher replicas provide high availability for your search service.</p>"},{"location":"deployment/distributed/overview/#components-deep-dive","title":"Components Deep Dive","text":"<p>The searcher component focuses entirely on handling search queries with low latency. It runs with 2 replicas by default, though you can scale this up for higher throughput. These pods download indexes from S3 into memory for the fastest possible access, and their resources are optimized for query performance rather than index building.</p> <p>The indexer component handles document processing and index building. It always runs as a single replica to avoid indexing conflicts that could corrupt your data. This component builds indexes in memory and uploads completed indexes to S3. Since index building is resource-intensive, the indexer is allocated more CPU and memory than the searchers.</p> <p>S3 integration provides the backbone for the entire system. All search indexes live in S3, with the indexer uploading new or updated indexes and searchers downloading them as needed. This eliminates the need for any local persistent volumes and works with AWS S3, MinIO, or any S3-compatible storage service.</p>"},{"location":"deployment/distributed/overview/#getting-started_1","title":"Getting Started","text":"<p>The distributed deployment lives in <code>deploy/kubernetes/distributed</code> and includes shared configuration with S3 settings, separate manifests for the searcher Deployment and Service, and dedicated manifests for the indexer StatefulSet and Service.</p> <p>Before deploying, you'll need an S3 bucket for storing indexes and AWS credentials configured (though IAM roles are recommended over explicit credentials).</p> <p>To configure the system, edit the S3 settings in <code>configmap.yaml</code> to specify your bucket name, path prefix, and AWS region. If you're using MinIO or another S3-compatible service, uncomment and set the endpoint URL. For detailed configuration options, see the configuration reference.</p> <pre><code>s3:\n  bucket: your-nixiesearch-indexes\n  prefix: movies\n  region: us-west-2\n  # endpoint: http://minio:9000  # For MinIO\n</code></pre> <p>For deployment, you can optionally set up explicit credentials using Kubernetes Secrets, though IAM roles are preferred:</p> <pre><code># Set up credentials (optional if using IAM roles)\nkubectl create secret generic s3-credentials \\\n  --from-literal=access-key-id=YOUR_KEY \\\n  --from-literal=secret-access-key=YOUR_SECRET\n\n# Deploy all components\ncd deploy/kubernetes/distributed  \nkubectl apply -f .\n</code></pre>"},{"location":"deployment/distributed/overview/#manifests-overview_1","title":"Manifests Overview","text":"<p>The distributed deployment includes these YAML manifests:</p> <ul> <li><code>configmap.yaml</code> - Shared configuration with S3 settings, embedding models, and distributed store configuration for both searcher and indexer components</li> <li><code>searcher-deployment.yaml</code> - Deployment running 2 searcher replicas optimized for query performance with memory-based indexes</li> <li><code>searcher-service.yaml</code> - ClusterIP service providing stable network endpoint for search queries</li> <li><code>indexer-statefulset.yaml</code> - StatefulSet with single indexer pod for document processing and index building with higher resource allocation</li> <li><code>indexer-service.yaml</code> - ClusterIP service for indexing operations and administrative tasks</li> </ul>"},{"location":"deployment/distributed/overview/#when-to-use-distributed","title":"When to Use Distributed","text":"<p>Distributed deployment shines for production workloads where you need better reliability and performance. It handles high query volumes well since you can scale searchers independently of the indexer. Large datasets benefit from having dedicated indexer resources that don't compete with search performance. The architecture also supports multi-environment setups where you can share indexes between development, staging, and production via S3. Perhaps most importantly, it gives you operational flexibility to update the indexer without affecting search traffic, or vice versa. For operational best practices, see the autoscaling guide.</p>"},{"location":"deployment/distributed/overview/#operational-benefits","title":"Operational Benefits","text":"<p>The independent scaling capability means you can add more searcher replicas when you need higher query throughput, or allocate more resources to the indexer during heavy indexing periods, all without affecting the other component. For scaling strategies, see the autoscaling tutorial.</p> <p>Zero-downtime updates become possible since you can update the indexer without impacting search queries, or roll out searcher updates gradually across replicas. See the upgrade guide for best practices.</p> <p>Disaster recovery is simplified because indexes are safely stored in S3. If you lose a pod, it automatically rebuilds from S3 without any manual intervention. For backup strategies, see the backup tutorial.</p> <p>Cost optimization opportunities emerge since you can run indexing workloads on larger instances (potentially even spot instances) while keeping searchers on reliable, right-sized compute resources.</p>"},{"location":"deployment/distributed/overview/#choosing-the-right-deployment","title":"Choosing the Right Deployment","text":"<p>Standalone deployment makes sense when you're just getting started with Nixiesearch, working with relatively small datasets (under 1 million documents), prefer operational simplicity, or don't need independent scaling of search versus indexing components. For a detailed guide, see the standalone deployment documentation.</p> <p>Distributed deployment becomes the better choice for production workloads, high query volumes or large datasets, situations where you want the flexibility to scale components independently, environments where you're already using S3 for other storage needs, or when you need high availability for search queries.</p> <p>Both deployments are production-ready, so the choice really comes down to your specific requirements and operational preferences. Many teams start with standalone deployment to get familiar with Nixiesearch, then migrate to distributed as their needs grow and they want more operational control.</p>"},{"location":"deployment/distributed/overview/#next-steps","title":"Next Steps","text":"<p>Once you have your deployment running, you might want to:</p> <ul> <li>Set up monitoring with Prometheus metrics</li> <li>Configure document indexing for your data sources  </li> <li>Learn about search features and API usage</li> <li>Explore autoscaling strategies for production workloads</li> <li>Set up backup and recovery procedures</li> </ul> <p>For troubleshooting and operational questions, see our support resources.</p>"},{"location":"deployment/distributed/prometheus/","title":"Prometheus metrics","text":"<p>Nixiesearch exposes a Prometheus-compatible metrics exposed on the <code>/metrics</code> endpoint. The same port as for REST API (8080 by default) is used for serving. Raw metrics values can be inspected with the following cURL request:</p> <pre><code>curl http://localhost:8080/metrics\n</code></pre> <pre><code># HELP jvm_threads_state Current count of threads by state\n# TYPE jvm_threads_state gauge\njvm_threads_state{state=\"BLOCKED\"} 0.0\njvm_threads_state{state=\"NEW\"} 0.0\njvm_threads_state{state=\"RUNNABLE\"} 6.0\njvm_threads_state{state=\"TERMINATED\"} 0.0\njvm_threads_state{state=\"TIMED_WAITING\"} 7.0\njvm_threads_state{state=\"UNKNOWN\"} 0.0\njvm_threads_state{state=\"WAITING\"} 14.0\n\n# HELP nixiesearch_fs_data_available_bytes Available space on device\n# TYPE nixiesearch_fs_data_available_bytes gauge\nnixiesearch_fs_data_available_bytes{device=\"/dev/mapper/private\"} 3.1363534848E10\nnixiesearch_fs_data_available_bytes{device=\"/dev/nvme0n1p2\"} 6.05224292352E11\nnixiesearch_fs_data_available_bytes{device=\"/dev/root\"} 4.886433792E9\n</code></pre>"},{"location":"deployment/distributed/prometheus/#collecting-metrics","title":"Collecting metrics","text":"<p>To scrape metrics using a vanilla Prometheus server, you can use a <code>static_configs</code> to define an endpoint:</p> <pre><code>global:\n  scrape_interval: 15s\n\nscrape_configs:\n- job_name: nixiesearch\n  static_configs:\n  - targets: ['nixiesearch_host:8080']\n</code></pre> <p>If nixiesearch cluster is running inside Kubernetes as a <code>Deployment</code> without explicit node identity (e.g. pods are ephemeral, and they have no static IP and hostnames), a k8s service discovery should be used to dynamically scrape all Nixiesearch nodes.</p> <pre><code>scrape_configs:\n  - job_name: 'nixiesearch'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_label_app]\n        action: keep\n        regex: nixiesearch\n      - source_labels: [__meta_kubernetes_pod_ip]\n        action: replace\n        target_label: __address__\n        regex: (.*)\n        replacement: ${1}:8080\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        target_label: pod\n    metrics_path: /metrics\n</code></pre>"},{"location":"deployment/distributed/prometheus/#exported-metrics","title":"Exported metrics","text":""},{"location":"deployment/distributed/prometheus/#search-metrics","title":"Search metrics","text":"Name Type Cardinality Help nixiesearch_search_active_queries gauge n_index The number of currently active queries nixiesearch_index_docs gauge n_index Count of documents in this index nixiesearch_index_flush_total counter n_index Total flushes nixiesearch_index_flush_time_seconds gauge n_index Cumulative flush time in seconds nixiesearch_search_query_total counter n_index Total number of search queries nixiesearch_search_query_time_seconds gauge 2*n_index Total search query time in seconds nixiesearch_suggest_query_total counter n_index Total number of suggest queries nixiesearch_suggest_query_time_seconds gauge 2*n_index Total suggest query time in seconds nixiesearch_rag_query_total counter n_index Total number of RAG queries nixiesearch_rag_query_time_seconds gauge n_index Total RAG query time in seconds nixiesearch_inference_embedding_request_total counter n_models Total number of embedding queries nixiesearch_inference_embedding_request_time_seconds gauge n_models Total embedding time in seconds nixiesearch_inference_embedding_doc_total counter n_models Total number of embedding queries nixiesearch_inference_completion_request_total counter n_models Total number of completion queries nixiesearch_inference_completion_request_time_seconds counter n_models Total completion time in seconds nixiesearch_inference_completion_generated_tokens_total counter n_models Total generated tokens"},{"location":"deployment/distributed/prometheus/#system-metrics","title":"System metrics","text":"Name Type Cardinality Help nixiesearch_os_cpu_percent gauge 1 Percent CPU used by the OS nixiesearch_os_cpu_load1 gauge 1 1-minute system load average nixiesearch_os_cpu_load5 gauge 1 5-minute system load average nixiesearch_os_cpu_load15 gauge 1 15-minute system load average nixiesearch_fs_data_available_bytes gauge n_disks Available space on device nixiesearch_fs_data_free_bytes gauge n_disks Free space on device nixiesearch_fs_data_size_bytes gauge n_disks Size of the device"},{"location":"deployment/distributed/prometheus/#jvm-metrics","title":"JVM metrics","text":"<p>As Nixiesearch is a JVM application, it exposes rich set of JVM-related metrics. We use a standard set of JVM instrumentation metrics from the Prometheus Java Client.</p>"},{"location":"deployment/distributed/prometheus/#jvm-garbage-collector-metrics","title":"JVM Garbage Collector Metrics","text":"<p>The data is coming from GarbageCollectorMXBean.</p> <pre><code># HELP jvm_gc_collection_seconds Time spent in a given JVM garbage collector in seconds.\n# TYPE jvm_gc_collection_seconds summary\njvm_gc_collection_seconds_count{gc=\"PS MarkSweep\"} 0\njvm_gc_collection_seconds_sum{gc=\"PS MarkSweep\"} 0.0\njvm_gc_collection_seconds_count{gc=\"PS Scavenge\"} 0\njvm_gc_collection_seconds_sum{gc=\"PS Scavenge\"} 0.0\n</code></pre>"},{"location":"deployment/distributed/prometheus/#jvm-memory-metrics","title":"JVM Memory Metrics","text":"<p>Source of the data - MemoryPoolMXBean</p> <pre><code># HELP jvm_memory_committed_bytes Committed (bytes) of a given JVM memory area.\n# TYPE jvm_memory_committed_bytes gauge\njvm_memory_committed_bytes{area=\"heap\"} 4.98597888E8\njvm_memory_committed_bytes{area=\"nonheap\"} 1.1993088E7\n# HELP jvm_memory_init_bytes Initial bytes of a given JVM memory area.\n# TYPE jvm_memory_init_bytes gauge\njvm_memory_init_bytes{area=\"heap\"} 5.20093696E8\njvm_memory_init_bytes{area=\"nonheap\"} 2555904.0\n# HELP jvm_memory_max_bytes Max (bytes) of a given JVM memory area.\n# TYPE jvm_memory_max_bytes gauge\njvm_memory_max_bytes{area=\"heap\"} 7.38983936E9\njvm_memory_max_bytes{area=\"nonheap\"} -1.0\n# HELP jvm_memory_objects_pending_finalization The number of objects waiting in the finalizer queue.\n# TYPE jvm_memory_objects_pending_finalization gauge\njvm_memory_objects_pending_finalization 0.0\n# HELP jvm_memory_pool_collection_committed_bytes Committed after last collection bytes of a given JVM memory pool.\n# TYPE jvm_memory_pool_collection_committed_bytes gauge\njvm_memory_pool_collection_committed_bytes{pool=\"PS Eden Space\"} 1.30023424E8\njvm_memory_pool_collection_committed_bytes{pool=\"PS Old Gen\"} 3.47078656E8\njvm_memory_pool_collection_committed_bytes{pool=\"PS Survivor Space\"} 2.1495808E7\n# HELP jvm_memory_pool_collection_init_bytes Initial after last collection bytes of a given JVM memory pool.\n# TYPE jvm_memory_pool_collection_init_bytes gauge\njvm_memory_pool_collection_init_bytes{pool=\"PS Eden Space\"} 1.30023424E8\njvm_memory_pool_collection_init_bytes{pool=\"PS Old Gen\"} 3.47078656E8\njvm_memory_pool_collection_init_bytes{pool=\"PS Survivor Space\"} 2.1495808E7\n# HELP jvm_memory_pool_collection_max_bytes Max bytes after last collection of a given JVM memory pool.\n# TYPE jvm_memory_pool_collection_max_bytes gauge\njvm_memory_pool_collection_max_bytes{pool=\"PS Eden Space\"} 2.727870464E9\njvm_memory_pool_collection_max_bytes{pool=\"PS Old Gen\"} 5.542248448E9\njvm_memory_pool_collection_max_bytes{pool=\"PS Survivor Space\"} 2.1495808E7\n# HELP jvm_memory_pool_collection_used_bytes Used bytes after last collection of a given JVM memory pool.\n# TYPE jvm_memory_pool_collection_used_bytes gauge\njvm_memory_pool_collection_used_bytes{pool=\"PS Eden Space\"} 0.0\njvm_memory_pool_collection_used_bytes{pool=\"PS Old Gen\"} 1249696.0\njvm_memory_pool_collection_used_bytes{pool=\"PS Survivor Space\"} 0.0\n# HELP jvm_memory_pool_committed_bytes Committed bytes of a given JVM memory pool.\n# TYPE jvm_memory_pool_committed_bytes gauge\njvm_memory_pool_committed_bytes{pool=\"Code Cache\"} 4128768.0\njvm_memory_pool_committed_bytes{pool=\"Compressed Class Space\"} 917504.0\njvm_memory_pool_committed_bytes{pool=\"Metaspace\"} 6946816.0\njvm_memory_pool_committed_bytes{pool=\"PS Eden Space\"} 1.30023424E8\njvm_memory_pool_committed_bytes{pool=\"PS Old Gen\"} 3.47078656E8\njvm_memory_pool_committed_bytes{pool=\"PS Survivor Space\"} 2.1495808E7\n# HELP jvm_memory_pool_init_bytes Initial bytes of a given JVM memory pool.\n# TYPE jvm_memory_pool_init_bytes gauge\njvm_memory_pool_init_bytes{pool=\"Code Cache\"} 2555904.0\njvm_memory_pool_init_bytes{pool=\"Compressed Class Space\"} 0.0\njvm_memory_pool_init_bytes{pool=\"Metaspace\"} 0.0\njvm_memory_pool_init_bytes{pool=\"PS Eden Space\"} 1.30023424E8\njvm_memory_pool_init_bytes{pool=\"PS Old Gen\"} 3.47078656E8\njvm_memory_pool_init_bytes{pool=\"PS Survivor Space\"} 2.1495808E7\n# HELP jvm_memory_pool_max_bytes Max bytes of a given JVM memory pool.\n# TYPE jvm_memory_pool_max_bytes gauge\njvm_memory_pool_max_bytes{pool=\"Code Cache\"} 2.5165824E8\njvm_memory_pool_max_bytes{pool=\"Compressed Class Space\"} 1.073741824E9\njvm_memory_pool_max_bytes{pool=\"Metaspace\"} -1.0\njvm_memory_pool_max_bytes{pool=\"PS Eden Space\"} 2.727870464E9\njvm_memory_pool_max_bytes{pool=\"PS Old Gen\"} 5.542248448E9\njvm_memory_pool_max_bytes{pool=\"PS Survivor Space\"} 2.1495808E7\n# HELP jvm_memory_pool_used_bytes Used bytes of a given JVM memory pool.\n# TYPE jvm_memory_pool_used_bytes gauge\njvm_memory_pool_used_bytes{pool=\"Code Cache\"} 4065472.0\njvm_memory_pool_used_bytes{pool=\"Compressed Class Space\"} 766680.0\njvm_memory_pool_used_bytes{pool=\"Metaspace\"} 6659432.0\njvm_memory_pool_used_bytes{pool=\"PS Eden Space\"} 7801536.0\njvm_memory_pool_used_bytes{pool=\"PS Old Gen\"} 1249696.0\njvm_memory_pool_used_bytes{pool=\"PS Survivor Space\"} 0.0\n# HELP jvm_memory_used_bytes Used bytes of a given JVM memory area.\n# TYPE jvm_memory_used_bytes gauge\njvm_memory_used_bytes{area=\"heap\"} 9051232.0\njvm_memory_used_bytes{area=\"nonheap\"} 1.1490688E7\n</code></pre>"},{"location":"deployment/distributed/prometheus/#jvm-buffer-pool-metrics","title":"JVM Buffer Pool Metrics","text":"<p>The data is coming from the BufferPoolMXBean.</p> <pre><code># HELP jvm_buffer_pool_capacity_bytes Bytes capacity of a given JVM buffer pool.\n# TYPE jvm_buffer_pool_capacity_bytes gauge\njvm_buffer_pool_capacity_bytes{pool=\"direct\"} 8192.0\njvm_buffer_pool_capacity_bytes{pool=\"mapped\"} 0.0\n# HELP jvm_buffer_pool_used_buffers Used buffers of a given JVM buffer pool.\n# TYPE jvm_buffer_pool_used_buffers gauge\njvm_buffer_pool_used_buffers{pool=\"direct\"} 1.0\njvm_buffer_pool_used_buffers{pool=\"mapped\"} 0.0\n# HELP jvm_buffer_pool_used_bytes Used bytes of a given JVM buffer pool.\n# TYPE jvm_buffer_pool_used_bytes gauge\njvm_buffer_pool_used_bytes{pool=\"direct\"} 8192.0\njvm_buffer_pool_used_bytes{pool=\"mapped\"} 0.0\n</code></pre>"},{"location":"deployment/distributed/prometheus/#jvm-class-loading-metrics","title":"JVM Class Loading Metrics","text":"<p>The data is coming from ClassLoadingMXBean.</p> <pre><code># HELP jvm_classes_currently_loaded The number of classes that are currently loaded in the JVM\n# TYPE jvm_classes_currently_loaded gauge\njvm_classes_currently_loaded 1109.0\n# HELP jvm_classes_loaded_total The total number of classes that have been loaded since the JVM has started execution\n# TYPE jvm_classes_loaded_total counter\njvm_classes_loaded_total 1109.0\n# HELP jvm_classes_unloaded_total The total number of classes that have been unloaded since the JVM has started execution\n# TYPE jvm_classes_unloaded_total counter\njvm_classes_unloaded_total 0.0\n</code></pre>"},{"location":"deployment/distributed/indexing/file/","title":"File-based indexing","text":"<p>todo</p>"},{"location":"deployment/distributed/indexing/kafka/","title":"Pull-based document ingestion with Apache Kafka","text":"<p>When run in a distributed mode, Nixiesearch can pull documents for indexing from an Apache Kafka topic:</p> <ul> <li>Kafka can be used as a journal for CDC-style events: every time a document changes in your master database, an event is emitted with an updated document.</li> <li>To preserve infinitely-growing topics, you can use Compacted Topics to prune stale document records.</li> </ul> <p></p> <p>As for Nixiesearch v0.1, only Apache Kafka and S3/local files are supported for pull-based indexing. We have Apache Pulsar (see issue nixiesearch#190) and AWS Kinesis (see issue nixiesearch#207) on the roadmap.</p>"},{"location":"deployment/distributed/indexing/kafka/#configuration","title":"Configuration","text":"<p>All Kafka-specific connector options should be passed as command-line flags to the nixiesearch index kafka subcommand:</p> <pre><code>  -b, --brokers  &lt;arg&gt;    Kafka brokers endpoints, comma-separated list\n  -c, --config  &lt;arg&gt;     Path to a config file\n  -g, --group_id  &lt;arg&gt;   groupId identifier of consumer. default=nixiesearch\n  -i, --index  &lt;arg&gt;      to which index to write to\n  -l, --loglevel  &lt;arg&gt;   Logging level: debug/info/warn/error, default=info\n  -o, --offset  &lt;arg&gt;     which topic offset to use for initial connection?\n                          earliest/latest/ts=&lt;unixtime&gt;/last=&lt;offset&gt;\n                          default=none (use committed offsets)\n      --options  &lt;arg&gt;    comma-separated list of kafka client custom options\n  -t, --topic  &lt;arg&gt;      Kafka topic name\n  -h, --help              Show help message\n</code></pre> <p><code>brokers</code>, <code>index</code> and <code>topic</code> options are required.</p> <p>The following options are optional: * group_id: a name of consumer group. * offset: a time window in which events are read:     * <code>earliest</code> - start from the first stored message in the topic     * <code>latest</code> - consume only events that came recently (after Metarank connection)     * <code>ts=&lt;timestamp&gt;</code> - start from a specific absolute timestamp in the past     * <code>last=&lt;duration&gt;</code> - consume only events that happened within a defined relative duration (duration supports the       following patterns: <code>1s</code>, <code>1m</code>, <code>1h</code>, <code>1d</code>) * options: a comma-separated list of custom connector options:   * example: <code>--options key1=value1,key2=value2</code> </p>"},{"location":"deployment/distributed/indexing/overview/","title":"Distributed indexing","text":"<p>todo</p>"},{"location":"deployment/distributed/persistence/inmem/","title":"In-Memory Persistence","text":"<p>In-memory persistence stores search indexes entirely in RAM using Lucene's <code>ByteBuffersDirectory</code>. This approach provides extremely fast search performance but comes with trade-offs around data persistence and memory usage.</p> <p>For an overview of all storage options, see the persistence overview. For production deployments, consider S3 persistence or local disk storage.</p>"},{"location":"deployment/distributed/persistence/inmem/#when-to-use-in-memory-storage","title":"When to Use In-Memory Storage","text":"<p>In-memory storage is perfect for development environments where you need fast iteration cycles and don't care about data persistence across restarts. It's ideal for performance testing when benchmarking search performance, as it eliminates disk I/O bottlenecks and provides pure computational performance metrics.</p> <p>A common pattern is to use in-memory storage for searcher nodes in distributed deployments, where searchers download indexes from S3 into memory for ultra-fast query processing, while indexers persist data to disk. It's also useful for short-lived search tasks or batch processing jobs where indexes don't need to survive beyond the process lifetime.</p>"},{"location":"deployment/distributed/persistence/inmem/#configuration-example","title":"Configuration Example","text":"<p>Basic in-memory storage configuration:</p> <pre><code>core:\n  cache:\n    dir: /tmp/cache\n\ninference:\n  embedding:\n    e5-small:\n      model: intfloat/e5-small-v2\n\nschema:\n  movies:\n    store:\n      local:\n        memory:\n    fields:\n      title:\n        type: text\n        search:\n          lexical:\n            analyze: en\n          semantic:\n            model: e5-small\n      overview:\n        type: text\n        search:\n          lexical:\n            analyze: en\n</code></pre> <p>For more field configuration options, see the schema mapping guide. For embedding model configuration, see the embeddings guide.</p>"},{"location":"deployment/distributed/persistence/inmem/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>When using in-memory storage in Kubernetes, ensure adequate memory allocation:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nixiesearch\nspec:\n  template:\n    spec:\n      containers:\n      - name: nixiesearch\n        image: nixiesearch/nixiesearch:latest\n        resources:\n          requests:\n            memory: \"2Gi\"\n          limits:\n            memory: \"4Gi\"\n</code></pre> <p>For complete Kubernetes setup instructions, see the Kubernetes deployment guide.</p>"},{"location":"deployment/distributed/persistence/inmem/#further-reading","title":"Further Reading","text":"<ul> <li>Persistence overview - Compare all storage options</li> <li>S3 persistence - Distributed storage for production</li> <li>Local disk storage - Persistent local storage</li> <li>Distributed deployment overview - Production deployment patterns</li> <li>Quickstart guide - Complete development setup</li> </ul>"},{"location":"deployment/distributed/persistence/local/","title":"Local Directory Persistence","text":"<p>Local directory persistence stores search indexes on the local filesystem using Lucene's standard directory implementation. This approach provides reliable data persistence with good performance for standalone deployments and development environments.</p> <p>For an overview of all storage options, see the persistence overview. For production distributed deployments, consider S3 persistence, and for development/testing, see in-memory storage.</p>"},{"location":"deployment/distributed/persistence/local/#when-to-use-local-storage","title":"When to Use Local Storage","text":"<p>Local storage is ideal for standalone deployments where you have a single Nixiesearch instance handling both indexing and searching on the same machine. It's perfect for development environments when you need persistent data across restarts but don't want the complexity of distributed storage.</p> <p>Use local storage for small to medium workloads where your entire search index fits comfortably on a single machine's storage, and you don't need the scalability of distributed storage.</p>"},{"location":"deployment/distributed/persistence/local/#configuration-example","title":"Configuration Example","text":"<p>Basic local storage configuration:</p> <pre><code>core:\n  cache:\n    dir: /data/cache\n\nschema:\n  movies:\n    store:\n      local:\n        disk:\n          path: /data/indexes\n    fields:\n      title:\n        type: text\n        search:\n          lexical:\n            analyze: en\n      overview:\n        type: text\n        search:\n          lexical:\n            analyze: en\n</code></pre> <p>For more field configuration options, see the schema mapping guide. For configuration details, see the configuration reference.</p>"},{"location":"deployment/distributed/persistence/local/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>When using local storage in Kubernetes, configure persistent volumes:</p> <pre><code># PersistentVolumeClaim\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nixiesearch-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n\n# Deployment with volume mount\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nixiesearch\nspec:\n  template:\n    spec:\n      containers:\n      - name: nixiesearch\n        volumeMounts:\n        - name: data-volume\n          mountPath: /data\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: nixiesearch-data\n</code></pre> <p>For complete Kubernetes setup instructions, see the Kubernetes deployment guide.</p>"},{"location":"deployment/distributed/persistence/local/#further-reading","title":"Further Reading","text":"<ul> <li>Persistence overview - Compare all storage options</li> <li>S3 persistence - Distributed storage for production</li> <li>In-memory storage - Fast ephemeral storage</li> <li>Backup tutorial - Data protection strategies</li> <li>Standalone deployment - Complete standalone setup guide</li> </ul>"},{"location":"deployment/distributed/persistence/overview/","title":"Index Persistence","text":"<p>Nixiesearch supports multiple persistence modes for storing search indexes, each optimized for different deployment scenarios and performance requirements. You can configure persistence per index in your schema configuration.</p>"},{"location":"deployment/distributed/persistence/overview/#supported-persistence-modes","title":"Supported Persistence Modes","text":""},{"location":"deployment/distributed/persistence/overview/#in-memory-storage","title":"In-Memory Storage","text":"<p>Stores search indexes entirely in RAM using Lucene's <code>ByteBuffersDirectory</code>. Provides ultra-fast search performance but all data is lost when the process stops.</p> <p>Configuration: <code>store.local.memory</code></p> <p>See in-memory persistence for detailed configuration and usage.</p>"},{"location":"deployment/distributed/persistence/overview/#local-directory-storage","title":"Local Directory Storage","text":"<p>Stores search indexes on the local filesystem using Lucene's standard directory implementation. Provides reliable data persistence with good performance for single-node deployments.</p> <p>Configuration: <code>store.local.disk.path</code></p> <p>See local directory persistence for detailed configuration and usage.</p>"},{"location":"deployment/distributed/persistence/overview/#s3-storage","title":"S3 Storage","text":"<p>Stores search index segments in S3-compatible object storage, enabling distributed deployments where multiple components can share index data. Supports any S3-compatible service including AWS S3, MinIO, Google Cloud Storage, and others.</p> <p>Configuration: <code>store.distributed.remote.s3</code></p> <p>See S3 persistence for detailed configuration and usage.</p>"},{"location":"deployment/distributed/persistence/overview/#when-to-use-each-mode","title":"When to Use Each Mode","text":""},{"location":"deployment/distributed/persistence/overview/#use-in-memory-storage-when","title":"Use In-Memory Storage When:","text":"<ul> <li>Development and testing: Fast iteration cycles without persistence requirements</li> <li>Performance testing: Eliminating I/O bottlenecks for pure performance metrics</li> <li>Distributed searchers: Fast query processing in distributed deployments where data is loaded from S3</li> <li>Temporary workloads: Short-lived tasks where indexes don't need to survive process restarts</li> </ul>"},{"location":"deployment/distributed/persistence/overview/#use-local-directory-storage-when","title":"Use Local Directory Storage When:","text":"<ul> <li>Standalone deployments: Single-node deployments handling both indexing and searching</li> <li>Development with persistence: Local development requiring data to survive restarts</li> <li>Small to medium workloads: Workloads that fit on a single machine's storage</li> <li>Simple deployments: When you want reliable persistence without distributed complexity</li> </ul>"},{"location":"deployment/distributed/persistence/overview/#use-s3-storage-when","title":"Use S3 Storage When:","text":"<ul> <li>Production distributed deployments: Scaling searcher and indexer components independently</li> <li>Auto-scaling scenarios: Stateless compute nodes that can be added/removed without data loss</li> <li>Multi-environment sharing: Sharing indexes between development, staging, and production</li> <li>Multi-region deployments: Consistent access to indexes across geographic locations</li> <li>High availability: Decoupling storage from compute for better reliability</li> </ul>"},{"location":"deployment/distributed/persistence/overview/#hybrid-configurations","title":"Hybrid Configurations","text":"<p>Nixiesearch supports mixing storage modes within the same deployment. A common pattern for distributed deployments is:</p> <ul> <li>Searchers: In-memory storage for fast query processing</li> <li>Indexers: In-memory or local disk for index building</li> <li>Remote: S3 storage for persistence and sharing between components</li> </ul> <p>This approach combines the performance benefits of in-memory processing with the durability and scalability of S3 storage.</p>"},{"location":"deployment/distributed/persistence/overview/#next-steps","title":"Next Steps","text":"<ul> <li>In-memory persistence - Fast ephemeral storage</li> <li>Local directory persistence - Persistent local storage  </li> <li>S3 persistence - Distributed storage for production</li> <li>Distributed deployment overview - Complete deployment guide</li> <li>Configuration reference - All configuration options</li> </ul>"},{"location":"deployment/distributed/persistence/s3/","title":"S3 Persistence","text":"<p>S3 persistence stores search index segments in S3-compatible object storage, enabling distributed deployments where multiple searcher and indexer nodes can share the same index data. This is the recommended approach for production distributed deployments.</p> <p>For an overview of all storage options, see the persistence overview. For development environments, consider in-memory storage or local disk storage.</p>"},{"location":"deployment/distributed/persistence/s3/#when-to-use-s3-storage","title":"When to Use S3 Storage","text":"<p>S3 storage is ideal for production distributed deployments where you need to scale searcher and indexer components independently. It enables stateless compute nodes that can be added or removed without data loss, making it perfect for auto-scaling scenarios.</p> <p>Use S3 storage when you need to share indexes between multiple environments (development, staging, production) or when you want to decouple storage from compute for better reliability and operational flexibility. It's also essential for multi-region deployments where you need consistent access to search indexes across different geographic locations.</p>"},{"location":"deployment/distributed/persistence/s3/#configuration-example","title":"Configuration Example","text":"<p>Distributed configuration with in-memory components and S3 persistence:</p> <pre><code>schema:\n  movies:\n    store:\n      distributed:\n        searcher:\n          memory:          # Fast in-memory reads\n        indexer:\n          memory:          # Fast in-memory writes\n        remote:\n          s3:              # Persistent index segments\n            bucket: my-search-indexes\n            prefix: movies\n            region: us-east-1\n    fields:\n      title:\n        type: text\n        search:\n          lexical:\n            analyze: en\n      overview:\n        type: text\n        search:\n          lexical:\n            analyze: en\n</code></pre> <p>For more field configuration options, see the schema mapping guide. For complete distributed setup, see the distributed deployment overview.</p>"},{"location":"deployment/distributed/persistence/s3/#s3-authentication","title":"S3 Authentication","text":""},{"location":"deployment/distributed/persistence/s3/#aws-iam-roles-recommended","title":"AWS IAM Roles (Recommended)","text":"<p>The most secure approach is to use IAM roles for service accounts:</p> <pre><code># No credentials in config - uses IAM role\nschema:\n  movies:\n    store:\n      distributed:\n        remote:\n          s3:\n            bucket: my-search-indexes\n            prefix: movies\n            region: us-east-1\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#environment-variables","title":"Environment Variables","text":"<p>Set AWS credentials via environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-key\nexport AWS_REGION=us-east-1\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>For Kubernetes deployments, use secrets for credentials:</p> <pre><code>kubectl create secret generic s3-credentials \\\n  --from-literal=access-key-id=YOUR_ACCESS_KEY \\\n  --from-literal=secret-access-key=YOUR_SECRET_KEY\n</code></pre> <p>Then reference in your deployment:</p> <pre><code>env:\n- name: AWS_ACCESS_KEY_ID\n  valueFrom:\n    secretKeyRef:\n      name: s3-credentials\n      key: access-key-id\n- name: AWS_SECRET_ACCESS_KEY\n  valueFrom:\n    secretKeyRef:\n      name: s3-credentials\n      key: secret-access-key\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#required-iam-permissions","title":"Required IAM Permissions","text":"<p>Your IAM user or role needs these S3 permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-search-indexes\",\n        \"arn:aws:s3:::my-search-indexes/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#alternative-s3-providers","title":"Alternative S3 Providers","text":"<p>Nixiesearch supports any S3-compatible storage service by specifying a custom endpoint:</p>"},{"location":"deployment/distributed/persistence/s3/#minio","title":"MinIO","text":"<pre><code>schema:\n  movies:\n    store:\n      distributed:\n        remote:\n          s3:\n            bucket: nixiesearch-indexes\n            prefix: movies\n            region: us-east-1\n            endpoint: http://minio:9000\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code>schema:\n  movies:\n    store:\n      distributed:\n        remote:\n          s3:\n            bucket: my-gcs-bucket\n            prefix: movies\n            region: us-central1\n            endpoint: https://storage.googleapis.com\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#digitalocean-spaces","title":"DigitalOcean Spaces","text":"<pre><code>schema:\n  movies:\n    store:\n      distributed:\n        remote:\n          s3:\n            bucket: my-spaces-bucket\n            prefix: movies\n            region: nyc3\n            endpoint: https://nyc3.digitaloceanspaces.com\n</code></pre>"},{"location":"deployment/distributed/persistence/s3/#cloudflare-r2","title":"Cloudflare R2","text":"<pre><code>schema:\n  movies:\n    store:\n      distributed:\n        remote:\n          s3:\n            bucket: my-r2-bucket\n            prefix: movies\n            region: auto\n            endpoint: https://your-account-id.r2.cloudflarestorage.com\n</code></pre> <p>For authentication with alternative providers, use their respective access keys with the same environment variables or Kubernetes secrets approach.</p>"},{"location":"deployment/distributed/persistence/s3/#further-reading","title":"Further Reading","text":"<ul> <li>Persistence overview - Compare all storage options</li> <li>In-memory storage - Fast ephemeral storage</li> <li>Local disk storage - Persistent local storage</li> <li>Distributed deployment overview - Complete distributed setup</li> <li>Configuration reference - All configuration options</li> </ul>"},{"location":"features/autocomplete/","title":"Autocomplete suggestions API","text":""},{"location":"features/autocomplete/#suggestion-index-configuration","title":"Suggestion index configuration","text":"<p>You need to explicitly define a suggest flag for a field in index mapping to be able to query for suggestions. Nixie supports two flavors of syntax definitions for suggestions:</p> <ul> <li>short one: only <code>suggest: true</code> using all default settings,</li> <li>long syntax: more verbose, but all settings are exposed.</li> </ul> <p>Example of a short syntax:</p> <pre><code>my-index:\n  fields:\n    title:\n      type: text\n      search: \n        lexical:\n          analyze: english\n      suggest: true\n</code></pre> <p>With longer suggestion syntax you have access to all the internal options:</p> <pre><code>my-index:\n  fields:\n    title:\n      type: text\n      search:\n        lexical:\n          analyze: english\n      suggest:\n        lowercase: false # should we down-case all suggestions?\n        expand:\n          min-terms: 1\n          max-terms: 3\n</code></pre> <p>Suggest parameters are defined as follows:</p> <ul> <li><code>lowercase</code>: optional, should we downcase all strings?, default <code>false</code></li> <li><code>expand</code>: optional, list of suggestion candidate expansion settings</li> <li><code>expand.min-terms</code>: optional, lower length of rolling window expansions, default <code>1</code></li> <li><code>expand.max-terms</code>: optional, higher length of rolling window expansions, default <code>3</code></li> </ul>"},{"location":"features/autocomplete/#sending-suggestion-requests","title":"Sending suggestion requests","text":"<p>Suggest indices have a special <code>_suggest</code> endpoint you can use for autocomplete suggestion generation:</p> <pre><code>curl -XPOST -d '{\"query\": \"hel\", \"fields\":[\"title\"]}' http://localhost:8080/v1/index/&lt;index-name&gt;/suggest\n</code></pre> <p>A full suggest request JSON format:</p> <pre><code>{\n  \"query\": \"hu\",\n  \"fields\": [\"title\"],\n  \"count\": 10\n}\n</code></pre> <p>Where request fields are defined in the following way:</p> <ul> <li><code>query</code>: required, string. A suggestion search query.</li> <li><code>fields</code>: required, string[]. Fields to use for suggestion generation.</li> <li><code>count</code>: optional, int, default=10. The number of top-level suggestions to generate.</li> <li><code>rerank</code>: optional, obj, default=\"rrf\". Suggestion ranking method, see section on RRF below for more details.</li> </ul> <p>The request above emits the following response:</p> <pre><code>{\n  \"suggestions\": [\n    {\"text\": \"hugo\", \"score\": 2.0},\n    {\"text\": \"hugo boss\", \"score\": 1.0},\n    {\"text\": \"hugo boss red\", \"score\": 1.0}\n  ],\n  \"took\": 11\n}\n</code></pre>"},{"location":"features/autocomplete/#reranking-configuration","title":"Reranking configuration","text":"<p>Suggestion request can include a single reranking processor step:</p> <ul> <li>Reciprocal Rank Fusion (RRF) reranking.</li> <li>(Planned in v0.2) Learn-to-Rank suggestions.</li> </ul> <p>Reciprocal Rank Fusion configuration block has the following options:</p> <ul> <li><code>rerank.rrf.depth</code>: How many suggestion candidates should be generated before reranking. Having more might affect latency, but increase precision. Optional, int, default=50.</li> <li><code>rerank.rrf.scale</code>: An RRF constant scaling factor, importance of top vs bottom candidate position on the final ranking. Optional, int, default=60.</li> </ul> <p>Suggestions are generated separately per each field, and then reduced together with Reciprocal Rank Fusion (RRF). A pseudocode example from Elasticsearch docs about RRF describes the algorithm well:</p> <pre><code>score = 0.0\nfor q in queries:\n    if d in result(q):\n        score += 1.0 / ( k + rank( result(q), d ) )\nreturn score\n\n# where\n# k is a ranking constant\n# q is a query in the set of queries\n# d is a document in the result set of q\n# result(q) is the result set of q\n# rank( result(q), d ) is d's rank within the result(q) starting from 1\n</code></pre> <p>Some important considerations regarding RRF ranking:</p> <ul> <li>Suggestions are fused into a single ranking in a case-insensitive way, so <code>hello</code> and <code>Hello</code> are considered equal.</li> <li>Suggestion candidates are not lowercased, Nixie preserves a case of the most highly ranked suggestion, and other variations are linked to it.</li> <li>By default we use scale constant for RRF as 60 to match Elasticsearch implementation.</li> </ul>"},{"location":"features/indexing/format/","title":"Nixiesearch JSON document format","text":"<p>Nixiesearch does not have a strict incoming JSON document schema: any format is probably OK while it can be processed using the existing index mapping.</p>"},{"location":"features/indexing/format/#document-identifier","title":"Document identifier","text":"<p>Each Nixiesearch-indexed document is expected (but not required) to have a global identifier. This identifier is useful for later document manipulation like update and removal:</p> <ul> <li>if JSON document has a special <code>_id</code> field defined, then it is used as an identifier.</li> <li>if the <code>_id</code> field is missing in the document payload, then a UUID-based random identifier is generated automatically.</li> </ul> <p>Internally the document id is a sequence of bytes, and any real JSON type of the <code>_id</code> field (like string and number) will be automatically mapped to the internal id representation.</p> <p>The <code>_id</code> field is automatically added to every index with <code>type: id</code>, which is a special field type optimized for document identifiers. This field type has the following characteristics:</p> <ul> <li><code>store: true</code> - document ID is always stored</li> <li><code>filter: true</code> - can be used in filter queries</li> <li><code>facet: false</code> - cannot be used for faceting</li> <li><code>sort: false</code> - not sortable by default</li> </ul> <p>You don't need to define the <code>_id</code> field in your schema - it's added automatically.</p> <p>This is an exampe of a good <code>_id</code> field:</p> <pre><code>{\"_id\": 10, \"title\": \"hello\"}\n</code></pre> <p>This is also a good <code>_id</code>: <pre><code>{\"_id\": \"levis-jeans-1937481\", \"title\": \"jeans\" }\n</code></pre></p>"},{"location":"features/indexing/format/#flat-documents","title":"Flat documents","text":"<p>Flat documents without any nesting are mapped 1-to-1 to underlying index fields. For example:</p> <pre><code>{\n  \"_id\": 1,\n  \"title\": \"socks\",\n  \"price\": 10.5\n}\n</code></pre> <p>will match the following index mapping:</p> <pre><code>schema:\n  my-index:\n    fields:\n      - title:\n          type: text\n      - price:\n          type: float\n</code></pre>"},{"location":"features/indexing/format/#repeated-fields","title":"Repeated fields","text":"<p>Compared to Elasticsearch/Solr, Nixiesearch has a distinction between repeated and singular fields:</p> <ul> <li>based on a field type, more specific and optimized data structures can be used.</li> <li>when returning a stored field, a JSON response field types can follow the index mapping. In other words, not all returned fields are arrays (like in Elasticsearch), and only the ones being defined as repeated.</li> </ul> <p>As for version <code>0.6.0</code> Nixiesearch only supports repeated textual fields.</p> <p>For example, the following document:</p> <pre><code>{\"_id\": 1, \"tags\": [\"a\", \"b\", \"c\"]}\n</code></pre> <p>should match the following index mapping:</p> <pre><code>schema:\n  my-index:\n    fields:\n      - tags:\n          type: text[]\n</code></pre>"},{"location":"features/indexing/format/#nested-documents","title":"Nested documents","text":"<p>Nixiesearch maps your documents into an underlying Lucene document representation, which is internally just a flat list of fields. To handle nesting, all the rich document structure is flattened.</p> <p>A non-repeated nested document like this:</p> <pre><code>{\n  \"_id\": 1,\n  \"meta\": {\n    \"asin\": \"AAA123\"\n  }\n}\n</code></pre> <p>will be flattened into a dot-separated field in the mapping:</p> <pre><code>schema:\n  my-index:\n    fields:\n      - meta.asin:\n          type: text\n</code></pre> <p>When indexing documents, nested JSON objects are automatically flattened using dot notation. Both flat and nested formats work with the same mapping:</p> <pre><code>// Flat format\n{\"_id\": 1, \"meta.asin\": \"AAA123\"}\n\n// Nested format (automatically flattened)\n{\"_id\": 1, \"meta\": {\"asin\": \"AAA123\"}}\n</code></pre> <p>Repeated documents are also flattened in a similar way, but with a notable exception of transforming all internal singular fields into repeated ones:</p> <pre><code>{\n  \"_id\": 1,\n  \"tracks\": [\n    {\"name\": \"yellow submarine\"},\n    {\"name\": \"smells like teen spirit\"}\n  ]\n}\n</code></pre> <p>will flatten itself into a collection of repeated fields:</p> <pre><code>schema:\n  my-index:\n    fields:\n      - tracks.name:\n          type: text[]\n</code></pre>"},{"location":"features/indexing/format/#pre-embedded-text-fields","title":"Pre-embedded text fields","text":"<p>By default, for text fields Nixiesearch expects a <code>JSON string</code> (or array of strings for <code>text[]</code>) as a document field type, and handles the embedding process internally. Consider the following index schema:</p> <pre><code>inference:\n  embedding:\n    my-model:\n      model: intfloat/e5-small-v2\nschema:\n  my-index:\n    fields:\n      title:\n        type: text\n        search:\n          semantic:\n            model: my-model\n      tags:\n        type: text[]\n        search:\n          semantic:\n            model: my-model\n</code></pre> <p>For a JSON document with server-side inference:</p> <pre><code>{\n  \"title\": \"cookies\",\n  \"tags\": [\"scala\", \"functional\", \"programming\"]\n}\n</code></pre> <p>Nixiesearch will run embedding inference for the model <code>intfloat/e5-small-v2</code>. When you already have text embeddings for documents, you can skip the inference process by providing field text and embedding at the same time:</p>"},{"location":"features/indexing/format/#text-fields","title":"text fields","text":"<pre><code>{\"title\": {\"text\": \"cookies\", \"embedding\": [0.1, 0.2, 0.3, ...]}}\n</code></pre>"},{"location":"features/indexing/format/#text-fields_1","title":"text[] fields","text":"<p>For <code>text[]</code> fields, you can provide embeddings in multiple formats:</p> <p>1:1 text-to-embedding mapping: <pre><code>{\n  \"tags\": {\n    \"text\": [\"scala\", \"functional\", \"programming\"],\n    \"embedding\": [\n      [0.1, 0.2, 0.3, ...],  // embedding for \"scala\"\n      [0.4, 0.5, 0.6, ...],  // embedding for \"functional\"\n      [0.7, 0.8, 0.9, ...]   // embedding for \"programming\"\n    ]\n  }\n}\n</code></pre></p> <p>Multiple embeddings for a single text:</p> <p>A single text value can have multiple embeddings. This is useful for multi-perspective embeddings or chunk-based approaches:</p> <pre><code>{\n  \"description\": {\n    \"text\": [\"product overview\"],\n    \"embedding\": [\n      [0.1, 0.2, 0.3, ...],  // embedding perspective 1\n      [0.4, 0.5, 0.6, ...],  // embedding perspective 2\n      [0.7, 0.8, 0.9, ...]   // embedding perspective 3\n    ]\n  }\n}\n</code></pre> <p>Summary of ingestion formats:</p> <ul> <li><code>JSON string</code> (for <code>text</code>) or <code>JSON array</code> (for <code>text[]</code>): when embedding inference is handled by the server</li> <li><code>JSON obj</code> with <code>text</code> and <code>embedding</code> fields: when embedding inference is skipped</li> <li>For <code>text</code>: <code>embedding</code> is a single array of floats</li> <li>For <code>text[]</code>: <code>embedding</code> is an array of arrays with the following constraints:<ul> <li>1:1 mapping: number of embeddings equals number of text values</li> <li>1:N mapping: a single text value with N embeddings (useful for multi-perspective or chunk-based representations)</li> </ul> </li> </ul> <p>Note: During search with <code>text[]</code> fields, all embeddings are considered, and the highest-scoring embedding determines the document's relevance score. See multi-vector search for text[] fields for details.</p>"},{"location":"features/indexing/mapping/","title":"Index mapping","text":"<p>Index mapping is used to describe how you plan to search for documents in Nixiesearch. Each time a document arrives to the indexer, it iterates over all fields in the payload, and maps them to internal index structures.</p> <p>To create an index mapping, create a block in the <code>schema</code> section of the config file:</p> <pre><code>schema:\n  # a first index\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          lexical:\n            analyze: english\n  # a second index\n  songs:\n    fields:\n      author:\n        type: text\n        search: \n          lexical:\n            analyze: english\n</code></pre> <p>In the index mapping above we defined two indexes <code>movies</code> and <code>songs</code> with a single text field indexed for a traditional lexical search.</p> <p>Note</p> <p>As Nixiesearch avoids having state within the searcher, you can only create indexes statically by defining them in the config file. If you plan to dynamically alter configuration of Nixiesearch cluster when running on Kubernetes, consider using tools like ArgoCD to gradually change configuration.</p>"},{"location":"features/indexing/mapping/#mapping-fields","title":"Mapping fields","text":"<p>A <code>fields</code> block in the index mapping explicitly lists all document fields Nixiesearch uses and stores in the index. For example, when you index the following JSON document:</p> <pre><code>{\n  \"title\": \"The Matrix\",\n  \"year\": 1999,\n  \"genre\": \"sci-fi\"\n}\n</code></pre> <p>You should define field mappings for fields <code>title</code>, <code>year</code> and <code>genre</code> the way you plan to query them. Fields you don't use (but that still present in the JSON document) you can omit, and Nixiesearch will ignore them. </p> <p>Each field definition has a set of common fields, and an extra list of per-type specific ones:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:          # field name\n        type: text    # required, field type\n        store: true   # optional, default true\n        sort: false   # optional, default false\n        facet: false  # optional, default false\n        filter: false # optional, default false \n        required: false # optional, default false\n</code></pre> <p>So by default all fields in Nixiesearch are:</p> <ul> <li><code>store: true</code>: you can retrieve raw field value back from Nixiesearch.</li> <li><code>sort: false</code>: you can't sort over field without explicitly marking a field as sortable. </li> <li><code>facet: false</code>: you can't perform facet aggregations by default.</li> <li><code>filter: false</code>: you cannot filter over fields by default.</li> <li><code>required: false</code>: fields are optional by default, matching Elasticsearch behavior. Set to <code>true</code> to enforce a specific document schema and reject documents with missing fields.</li> </ul> <p>Warning</p> <p>Sorting, faceting and filtering requires multiple type-specific index data structures to be maintained in the background, so only mark fields as sortable/facetable/filterable if you plan to use them so.</p> <p>Multiple field types are supported, so the <code>type</code> parameter can be one of the following:</p> <ul> <li>Document identifier: <code>id</code>. A special field type for document identifiers, automatically used for the <code>_id</code> field.</li> <li>Text fields: <code>text</code> and <code>text[]</code>. Unlike other Lucene-based search engines where all fields are implicitly repeatable, we distinguish between single and multi-value fields.</li> <li>Numerical fields: <code>int</code>, <code>long</code>, <code>double</code>, <code>float</code>, <code>bool</code>, <code>geopoint</code>. You cannot search over numerical fields (unless you treat them as strings), but you can filter, facet and sort!</li> <li>Media fields: <code>image</code>. A special field type for multi-modal search.</li> <li>Date fields: <code>date</code> and <code>datetime</code>. </li> </ul>"},{"location":"features/indexing/mapping/#wildcard-fields","title":"Wildcard fields","text":"<p>To allow more dynamism in index schema, you can use <code>*</code> wildcard placeholder in field names:</p> <pre><code>schema:\n  movies:\n    extra_*:\n      type: text\n      search:\n        type: lexical\n        language: en\n</code></pre> <p>So all fields matching the wildcard pattern are going to be treated according to the schema. Wildcard fields have minor limitations:</p> <ul> <li>only a single <code>*</code> placeholder is allowed.</li> <li>you cannot have a non-wildcard field defined matching a wildcard pattern (e.g. having both a regular <code>title_string</code> field and a wildcard <code>*_string</code> in the same index).</li> </ul> <p>You can also combine nested fields with wildcards using dot notation:</p> <pre><code>schema:\n  products:\n    fields:\n      meta.field_*_str:\n        type: text\n        search:\n          lexical:\n            analyze: english\n</code></pre> <p>This pattern matches fields like <code>meta.field_title_str</code>, <code>meta.field_desc_str</code>, etc. Limitations for nested wildcards:</p> <ul> <li>only one <code>.</code> (dot) and one <code>*</code> (wildcard) allowed per field name</li> <li>the wildcard must appear after the dot (e.g., <code>parent.child_*</code> is valid, <code>*_parent.child</code> is not)</li> </ul> <p>Wildcard fields can be used in the <code>fields</code> block of search request to get multiple fields from the document at once:</p> <pre><code>{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"extra_*\"]\n}\n</code></pre> <p>The <code>multi_match</code> search operator also supports matching over wildcard fields during search.</p>"},{"location":"features/indexing/mapping/#text-field-mapping","title":"Text field mapping","text":"<p>Apart from common <code>type</code>, <code>store</code>, <code>filter</code>, <code>sort</code>, <code>facet</code> and <code>required</code> parameters, text fields have a set of other search-related options:</p> <ul> <li>A field can be <code>search</code>able, with lexical, semantic and hybrid retrieval.</li> <li>You can use field contents to generate search autocomplete suggestions</li> </ul> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          lexical: # make only lexical index\n            analyze: english\n        suggest: false\n      overview:\n        type: text\n        search: # now semantic search!\n          semantic:\n            model: e5-small # a name of the model in the inference section\ninference:\n  embedding:\n    # the model used to embed documents\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n</code></pre>"},{"location":"features/indexing/mapping/#lexical-search","title":"Lexical search","text":"<p>To define a lexical-only search over a field, you mark it as <code>search.lexical</code> and optionally define a target language:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          lexical:\n            analyze: english # optional, default: generic\n      overview:\n        type: text\n        search:\n          lexical: {} # use default generic analyzer\n</code></pre> <p>For a better search quality, it's advised for lexical search to define the <code>analyze</code> of the field: this way Nixiesearch will use a Lucene language-specific analyzer. By default, the <code>StandardAnalyzer</code> is used.</p> <p>See all supported languages in the supported languages section.</p>"},{"location":"features/indexing/mapping/#semantic-search","title":"Semantic search","text":"<p>To use an embedding-based semantic search, mark a text field as <code>search.semantic</code> and define an embedding model to use:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          semantic:          \n            model: e5-small # a model name from the inference section\n\n# each model you plan to use for embedding\n# should be explicitly defined\ninference:\n  embedding:\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n</code></pre> <p>Nixiesearch supports both self-hosted ONNX embedding models and external embedding APIs.</p>"},{"location":"features/indexing/mapping/#hybrid-search","title":"Hybrid search","text":"<p>A hybrid search is a combination of lexical and semantic retrieval for a single field. Hybrid field mapping is a union of both  <code>semantic</code> and <code>lexical</code> mappings:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        language: en              # optional, default: generic\n        search: \n          semantic:      \n            model: e5-small         # a ref to the inference model name\n          lexical:\n            analyze: english\n\ninference:\n  embedding:\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n</code></pre> <p>For hybrid retrieval, Nixiesearch performs two search queries in parallel for both methods, and then mixes search results with Reciprocal Rank Fusion.</p>"},{"location":"features/indexing/mapping/#numerical-fields","title":"Numerical fields","text":"<p>Numerical fields of types <code>int</code>, <code>float</code>, <code>double</code>, <code>long</code> and <code>bool</code> cannot be searchable, so only have a common set of options:</p> <pre><code>schema:\n  movies:\n    fields:\n      year: \n        type: int\n        sort: true\n        filter: true\n        facet: true\n      in-theaters:\n        type: bool\n        sort: true\n        filter: true\n        facet: true\n</code></pre> <p>In the index mapping above we marked <code>year</code> and <code>in-theaters</code> fields as sortable, filterable and facetable.</p>"},{"location":"features/indexing/mapping/#media-fields","title":"Media fields","text":"<p>Warning</p> <p>Image field support is planned for the <code>v0.7</code> release.</p>"},{"location":"features/indexing/mapping/#mapping-options","title":"Mapping options","text":"<p>An index mapping can optionally also include many other index-specific settings like:</p> <ul> <li>index alias: secondary name for an index</li> <li>RAG settings: prompt template and GenAI model settings.</li> <li>store: how index is stored and synchronized.</li> </ul>"},{"location":"features/indexing/mapping/#index-aliases","title":"Index aliases","text":"<p>Nixiesearch indexes cannot be easily renamed (due to cluster state being immutable), but you can give them extra names as an alias:</p> <pre><code>schema:\n  movies:\n    alias: \n    - films\n    - series\n    fields:\n      title:\n        type: text\n</code></pre> <p>When an index has an alias, then it can be queried with multiple names:</p> <pre><code>curl -XPOST -d '{}' http://localhost/films/_search\n</code></pre> <p>Main use-case for aliases is ability to promote indexes:</p> <ol> <li>You reindex your document corpus to a timestamped index <code>index_20240801</code></li> <li>You perform your QA tasks validating that nothing is broken,</li> <li>You promote the <code>index_20240801</code> to <code>prod</code> by giving it alias <code>prod</code></li> </ol> <p>So your end customers won't need to change their REST endpoint address and always target the same index.</p> <p>Warning</p> <p>Aliases should be unique, so a single alias cannot belong to multiple indexes.</p>"},{"location":"features/indexing/mapping/#rag-settings","title":"RAG settings","text":"<p>To use RAG queries, you need to explicitly define in the config file which LLMs you plan to use query-time:</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n  completion:\n    # Used for summarization\n    qwen2:\n      provider: llamacpp\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n\nschema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          semantic:\n            model: e5-small\n        suggest: true\n      overview:\n        type: text\n        search: \n          semantic:\n            model: e5-small\n        suggest: true\n</code></pre> <p>Where:</p> <ul> <li><code>model</code>: a Huggingface model handle in a format of <code>namespace</code>/<code>model-name</code>.</li> <li><code>file</code>: an optional model file name if there are multiple. By default Nixiesearch will pick the lexicographically first GGUF/ONNX file. </li> <li><code>name</code>: name of this model you will reference in RAG search requests</li> </ul> <p>See RAG reference and ML model inference sections for more details.</p>"},{"location":"features/indexing/mapping/#store-settings","title":"Store settings","text":"<p>When ran in distributed mode, you can configure the way index is stored:</p> <pre><code>schema:\n  # a regular disk-based index storage\n  # OK for standalone and single-node deployments\n  # NOT OK for distributed mode\n  movies_local_disk:\n    fields:\n      title:\n        type: text\n    store:\n      local:\n        disk: \n          path: file:///path/to/index\n  # ephemeral in-memory index\n  # all data will be lost upon server restart\n  # OK for development, NOT OK for everything else\n  movies_inmem:\n    fields:\n      title:\n        type: text\n    store:\n      local:\n        memory:\n  # S3-backed index\n  # OK for distributed mode, NOT OK for standalone\n  movies_s3:\n    store:\n      distributed:\n        searcher:\n          memory:\n        indexer:\n          memory:\n        remote:\n          s3:\n            bucket: bucket\n            prefix: e2e\n            region: us-east-1\n            endpoint: http://localhost:4566/\n\n    fields:\n      title:\n        type: text\n</code></pre> <p>See distributed persistence reference for more details.</p>"},{"location":"features/indexing/mapping/#next-steps","title":"Next steps","text":"<p>To continue your journey with setting up indexing, follow to the next sections:</p> <ul> <li>Document format of JSON documents you index.</li> <li>Supported field types.</li> <li>Indexing REST API and nixiesearch index CLI app reference for offline indexing.</li> </ul>"},{"location":"features/indexing/overview/","title":"Building an index","text":"<p>Nixiesearch index is just a regular index like in Elastic/OpenSearch/SOLR, but with the following differences:</p> <ul> <li>Indexes are created by defining their schemas in a config file. It is deliberately not possible to create an index at runtime using REST API, as Nixiesearch instances have an immutable configuration.</li> <li>Index always has a strict schema defined. Schemaless approach is user-friendly, but you will eventually have 10 different ways to store a boolean field, like it usually happens in MongoDB \ud83e\udee0.</li> </ul> <p>To add a set of documents to an index, you need to perform these two steps:</p> <ul> <li>define an index mapping in a config file. Nixiesearch is a strongly-typed document storage system, so dynamic mapping is not supported.</li> <li>write documents to the index, either with push-based REST API or with pull-based stream ingestion.</li> </ul> <p>Note</p> <p>Dynamic mapping in most search engines is considered an anti-pattern: the engine cannot correctly guess how are you going to query documents, so by default all fields are marked as searchable, facetable, filterable and suggestable. This results in slow ingestion throughput and huge index size.</p>"},{"location":"features/indexing/overview/#index-mapping","title":"Index mapping","text":"<p>To define an index mapping, you need to add an index-specific block to the <code>schema</code> section of the configuration file. You can also configure indexing performance settings like merge policies in the configuration:</p> <pre><code>schema:\n  my-first-index:\n    fields:\n      title:\n        type: text\n        search: \n          lexical:\n            analyze: english\n      price:\n        type: float\n        filter: true\n</code></pre> <p>In the example above we defined an index <code>my-first-index</code> with two fields title and price. Index is stored on disk by default.</p> <p>Each field definition in a static mapping has two groups of settings:</p> <ul> <li>Field type specific parameters - like how it's going to be searched for text fields. In the example above we only allowed <code>lexical</code> search over the <code>title</code> field. See the mapping reference for text fields to see how to enable semantic and hybrid search.</li> <li>Global parameters - is this field can be stored, filtered, faceted and sorted.</li> </ul> <p>Go to the mapping reference section for more details on all field parameters.</p>"},{"location":"features/indexing/overview/#writing-documents-to-an-index","title":"Writing documents to an index","text":"<p>Internally Nixiesearch implements a pull-based indexing - the service itself asks for a next chunk of documents from an upstream source. See File-based indexing and Stream indexing from Apache Kafka for more examples on this approach.</p> <p></p> <p>For convenience, Nixiesearch can emulate a push-based approach via REST API you probably got used with other search engines - your app should submit a JSON payload with documents and wait for an acknowledgement.</p>"},{"location":"features/indexing/overview/#starting-nixiesearch","title":"Starting Nixiesearch","text":"<p>Nixiesearch has multiple ways of running indexing:</p> <ul> <li>Offline indexing. Useful when performing full reindexing from static document source, like from a set of files, or from Kafka topic.</li> <li>Online indexing. For folks who got used to Elasticsearch with REST API.</li> </ul> <p>For the sake of simplicity we can start Nixiesearch in a standalone mode, which bundles both searcher and indexer in a single process with a shared REST API.</p> <pre><code>docker run -it -p 8080:8080 -v .:/data nixiesearch/nixiesearch:latest standalone --config /path/to/conf.yml\n</code></pre> <p>Note</p> <p>Standalone mode is intended for small-scale local deployments and developer environments, not for a production use. If you plan to use Nixiesearch with real customer traffic, consider using a distributed deployment with S3-based storage.</p>"},{"location":"features/indexing/overview/#indexing-rest-api","title":"Indexing REST API","text":"<p>Each Nixiesearch index has an <code>/v1/index/&lt;index-name&gt;</code> REST endpoint where you can HTTP POST your documents to.</p> <p>This endpoint expects a JSON payload in one of the following formats:</p> <ul> <li>JSON object: just a single document.</li> <li>JSON array of objects: a batch of documents.</li> <li>JSON-Line array of objects: also a batch of documents, but simpler wire format.</li> </ul> <p>For example, writing a single document to an <code>dev</code> index can be done with a cURL command:</p> <pre><code>curl -XPOST -d '{\"title\": \"hello\", \"color\": [\"red\"], \"meta\": {\"sku\":\"a123\"}}'\\\n  http://localhost:8080//v1/index/my_dev_index\n</code></pre> <p>Note</p> <p>To have proper back-pressure mechanism, prefer using a pull-based indexing with Apache Kafka or with offline file-based ingestion.</p>"},{"location":"features/indexing/overview/#streaming-document-indexing","title":"Streaming document indexing","text":"<p>With pull-based streaming indexing supported natively, it becomes trivial to implement these typical scenarios:</p> <ol> <li>Batch full re-indexing: take all documents from a datasource and periodically re-build index from scratch.</li> <li>Distributed journal as a single source of truth: use Kafka compacted topics as a view over last versions of documents, with real-time updates.</li> <li>Large dataset import: import a complete set of documents from local/S3 files, maintaining optimal throughput and batching.</li> </ol> <p></p> <p>Nixiesearch supports Apache Kafka, AWS S3 (and also compatible object stores) and local files as a source of documents for indexing.</p> <p>If you have your dataset in a JSON file, instead of making HTTP PUT with very large payload using REST API, you can invoke a <code>nixiesearch index</code> sub-command to perform streaming indexing in a separate process:</p> <pre><code>docker run -itv .:/data nixiesearch/nixiesearch:latest index file \\\n  --config /data/conf.yml --index &lt;index name&gt; \\\n  --url file:///data/docs.json\n</code></pre> <p>Where <code>&lt;your-local-dir&gt;</code> is a directory containing the <code>conf.yml</code> config file and a <code>docs.json</code> with documents for indexing. </p> <p>See index CLI reference and Supported URL formats for more details.</p>"},{"location":"features/indexing/overview/#next-steps","title":"Next steps","text":"<p>In the next section, learn how you can create an index mapping.</p>"},{"location":"features/indexing/types/date/","title":"Date fields","text":"<p><code>Date</code> fields are strings in an ISO 8601 format:</p> <pre><code>{\n  \"_id\": 1,\n  \"name\": \"KFC\",\n  \"date\": \"2024-01-01\"\n}\n</code></pre> <p>Internally dates are stored as integer fields with a day offset from the start of the epoch (<code>1970-01-01</code>).</p> <p>Like <code>date</code> fields, <code>datetime</code> fields are also strings in an ISO 8601:</p> <pre><code>{\n  \"_id\": 1,\n  \"name\": \"KFC\",\n  \"date\": \"2024-01-01T00:00:01Z\"\n}\n</code></pre> <p><code>datetime</code> fields are stored as milliseconds till the UNIX epoch in the UTC timezone:</p> <ul> <li>Nixiesearch implicitly converts non-UTC <code>datetime</code> fields into the UTC zone</li> <li>Only the UTC-zoned <code>datetime</code> fields are returned. You need to perform the timezone conversion on the application side.</li> </ul> <p><code>date</code> and <code>datetime</code> fields can be filtered and aggregated.</p> <p>Example field schema for a datetime field <code>updated_at</code>:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: false\n      updated_at:\n        type: datetime  \n        filter: true    # field is filterable\n        facet: true     # field is facetable\n</code></pre>"},{"location":"features/indexing/types/geo/","title":"Geolocation","text":""},{"location":"features/indexing/types/geo/#geo-fields","title":"Geo fields","text":"<p><code>geopoint</code> fields are defined by their lat/lon coordinates:</p> <pre><code>{\n  \"_id\": 1,\n  \"name\": \"KFC\",\n  \"location\": {\"lat\": 1.0, \"lon\": 2.0}\n}\n</code></pre> <p>You can perform <code>geo_distance</code> and <code>geo_box</code> filters over <code>geopoint</code> fields.</p> <p>It is also possible to do a distance sort for this field type.</p> <p>Example field schema for a <code>geopoint</code> field <code>location</code>:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text   \n        search: false\n      location:\n        type: geopoint  \n        filter: true    # field is filterable\n                        # note that geopoint fields are not facetable\n</code></pre>"},{"location":"features/indexing/types/images/","title":"Indexing images","text":"<p>TODO: Planned for <code>v0.7.0</code></p>"},{"location":"features/indexing/types/numeric/","title":"Numeric fields","text":"<p>Numeric fields <code>int</code>, <code>float</code>, <code>long</code>, <code>double</code>, <code>bool</code> are supported. The <code>bool</code> field is more an API syntax sugar and is built on top of an internal <code>int</code> field.</p> <p>Numeric fields can be filtered, sorted by and aggregated.</p>"},{"location":"features/indexing/types/numeric/#single-value-numeric-fields","title":"Single-value numeric fields","text":"<p>Example field schema for a numeric field <code>year</code>:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: false\n      year:\n        type: int       # there can be single year\n        filter: true    # field is filterable, default false\n        facet: true     # field is facetable, default false\n        store: true     # field is stored, default true\n        sort: true     # field is sortable, default false\n</code></pre>"},{"location":"features/indexing/types/numeric/#numeric-list-fields","title":"Numeric list fields","text":"<p>Nixiesearch also supports array/list variants of numeric fields: <code>int[]</code>, <code>long[]</code>, <code>float[]</code>, <code>double[]</code>. These allow storing multiple numeric values for a single field.</p> <pre><code>schema:\n  products:\n    fields:\n      title:\n        type: text\n      ratings:\n        type: int[]     # array of user ratings\n        filter: true    # enable range filtering on ratings\n      sizes:\n        type: float[]   # available product sizes\n        filter: true\n        store: true\n      dimensions:\n        type: double[]  # product dimensions [length, width, height]\n        store: true\n</code></pre>"},{"location":"features/indexing/types/numeric/#json-document-format","title":"JSON document format","text":"<p>When indexing documents with numeric list fields, use standard JSON arrays:</p> <pre><code>{\n  \"title\": \"Wireless Headphones\",\n  \"ratings\": [5, 4, 5, 3, 4, 5],\n  \"sizes\": [6.5, 7.0, 7.5, 8.0, 8.5],\n  \"dimensions\": [15.2, 18.5, 7.3]\n}\n</code></pre>"},{"location":"features/indexing/types/numeric/#use-cases","title":"Use cases","text":"<p>Numeric list fields are ideal for:</p> <ul> <li>Product attributes: Multiple sizes, available ratings, variant prices</li> <li>Multi-dimensional data: Product dimensions, feature vectors</li> <li>Category scores: Relevance scores across different categories</li> <li>Multiple values: Any case where a document has multiple numeric values for the same attribute</li> </ul>"},{"location":"features/indexing/types/numeric/#configuration-options","title":"Configuration options","text":"Field Type Single Array Description <code>int</code> <code>int</code> <code>int[]</code> 32-bit integers <code>long</code> <code>long</code> <code>long[]</code> 64-bit integers <code>float</code> <code>float</code> <code>float[]</code> 32-bit floating point <code>double</code> <code>double</code> <code>double[]</code> 64-bit floating point <p>All numeric list fields support: - <code>store: true/false</code> - whether to store the original values (default: true) - <code>filter: true/false</code> - whether the field can be filtered (default: false) - <code>required: true/false</code> - whether the field is required (default: false)</p> <p>Note: Numeric array fields (<code>int[]</code>, <code>long[]</code>, <code>float[]</code>, <code>double[]</code>) do not support sorting operations. Only single-value numeric fields can be sorted.</p>"},{"location":"features/indexing/types/numeric/#search-behavior","title":"Search behavior","text":"<p>Numeric fields cannot be searched in the sense of full text search as text fields do, but you can filter over them.</p> <p>For numeric list fields, filtering operations check if any element in the array matches the criteria. For example, filtering <code>ratings &gt;= 4</code> on <code>[5, 4, 5, 3, 4, 5]</code> would match because several elements are &gt;= 4.</p> <p>By default all numeric fields are only stored, for <code>filter</code>, <code>sort</code> and <code>facet</code> support you need to explicitly toggle the corresponding flag.</p>"},{"location":"features/indexing/types/overview/","title":"Field types","text":"<p>Nixiesearch supports following field types:</p> <ul> <li>document identifier: <code>id</code> - special field type for the <code>_id</code> field (automatically added to every index).</li> <li>text fields: <code>text</code>, <code>text[]</code>.</li> <li>numeric fields: <code>int</code>, <code>float</code>, <code>long</code>, <code>double</code>, <code>bool</code>, <code>int[]</code>, <code>long[]</code>, <code>float[]</code>, <code>double[]</code>.</li> <li>geolocation fields: <code>geopoint</code></li> <li>date fields: <code>date</code>, <code>datetime</code></li> </ul>"},{"location":"features/indexing/types/overview/#setting-field-type","title":"Setting field type","text":"<p>In the field definition in the index mapping the required field <code>type</code> defines the underlying data type used to store and process the field.</p> <p>For example, defining a <code>text</code> field looks like this:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text # &lt;-- the field type is here\n        search: \n          semantic:\n            model: e5-small\n</code></pre> <p>Field type should be defined in advance before you start indexing documents, as it affects:</p> <ul> <li>the way field is stored on disk (or not stored at all, if you set <code>stored: false</code>).</li> <li>underlying index structures if you plan to <code>search</code>, <code>filter</code>, <code>facet</code> and <code>sort</code> over this field.</li> <li>document schema check during ingestion. </li> </ul> <p>For further reading, see text, numeric, date and geo fields sections.</p>"},{"location":"features/indexing/types/text/","title":"Text fields","text":"<p>Unlike other Lucene-based search engines, Nixiesearch has a distinction between singular and repeated fields on a schema level - so choose your field type wisely:</p> <ul> <li>for singular fields, use the <code>text</code> type.</li> <li>for repeated fields, choose the <code>text[]</code> type.</li> </ul> <p>Example field schema for a text fields <code>title</code> and <code>genre</code>:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text      # only a single title is allowed\n        search:\n          semantic:\n            model: e5-small\n      genre:\n        type: text[]    # there can be multiple genres per document\n        search: \n          lexical:\n            analyze: english\n        filter: true    # field is filterable\n        facet: true     # field is facetable\n        store: true     # can retrieve the field back from index\n        suggest: true   # build autocomplete suggestions based on that field\n</code></pre>"},{"location":"features/indexing/types/text/#semantic-index-parameters","title":"Semantic index parameters","text":"<p>When a text field has a semantic search enabled, there are a couple of parameters you can further configure:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text      # only a single title is allowed\n        search:\n          semantic:\n            model: e5-small  # optional: for server-side inference\n            dim: 384         # optional: for pre-embedded documents\n            ef: 32\n            m: 16\n            quantize: float32\n            workers: 4\n            distance: dot\n</code></pre> <p>Fields:</p> <ul> <li><code>model</code> (optional): embedding model for server-side inference. Required when documents don't have pre-computed embeddings.</li> <li><code>dim</code> (optional): embedding vector dimensions for pre-embedded documents. Required when <code>model</code> is not specified. Maximum supported dimensions: 8192.</li> <li><code>ef</code> and <code>m</code>: HNSW index parameters. The higher these values, the better the search recall at the cost of performance.</li> <li><code>quantize</code> (optional, <code>float32</code>/<code>int8</code>/<code>int4</code>/<code>int1</code>, default <code>float32</code>): index quantization level. <code>int8</code> saves 4x RAM and disk but at the cost of worse recall.</li> <li><code>workers</code> (optional, int, default is same as number of CPUs in the system): how many background workers to use for HNSW indexing operations.</li> <li><code>distance</code> (optional, <code>dot</code>/<code>cosine</code>, default <code>dot</code>): which embedding distance function to use. <code>dot</code> is faster (and mathematically equals to <code>cosine</code>) if your embeddings are normalized (see embedding inference section for details)</li> </ul>"},{"location":"features/indexing/types/text/#server-side-vs-pre-embedded-documents","title":"Server-side vs Pre-embedded documents","text":"<p>Nixiesearch supports two modes for semantic search:</p> <ol> <li>Server-side inference: Use the <code>model</code> parameter to compute embeddings on the server</li> <li>Pre-embedded documents: Use the <code>dim</code> parameter when documents already contain embedding vectors</li> </ol> <p>You must specify either <code>model</code> or <code>dim</code>, but not both. </p>"},{"location":"features/indexing/types/text/#operations-on-text-fields","title":"Operations on text fields","text":""},{"location":"features/indexing/types/text/#document-ingestion-format","title":"Document ingestion format","text":"<p>When a document with a <code>text</code> field is ingested, Nixiesearch expects the document JSON payload for the field to be in either format:</p> <ul> <li><code>JSON string</code>: like <code>{\"title\":\"cookies\"}</code>, when text embedding is computed by the server</li> <li><code>JSON obj</code>: like <code>{\"title\": {\"text\":\"cookies\", \"embedding\": [1,2,3]}}</code> for pre-embedded documents. </li> </ul> <p>See pre-embedded text fields in the Document format section for more details.</p>"},{"location":"features/indexing/types/text/#search","title":"Search","text":"<p>The main reason of text fields existence is to be used in search. Nixiesearch has two types of indexes can be used for search, lexical and semantic:</p> <ul> <li>lexical: an industry traditional BM25 keyword search, like in Elastic/SOLR before 2022. Nowadays called as <code>sparse retrieval</code>.</li> <li>semantic: an a-kNN vector-based search over embeddings of documents. A.k.a <code>dense retrieval</code>.</li> </ul> <p>By default all text fields are not searchable, and you need to explicitly enable either lexical, or semantic retrieval, or both at the same time:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search:\n          semantic: # build an embedding HNSW index \n            model: e5-small\n          lexical:  # build a lexical BM25 index\n            analyze: english\n</code></pre> <p>After that you can search over text fields with all Query DSL operators Nixiesearch supports, for example <code>match</code>, <code>semantic</code> and <code>rrf</code>:</p> <pre><code>curl -XPOST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": {\n      \"rrf\": {\n        \"queries\": [\n          {\"match\": {\"title\": \"batman\"}},\n          {\"semantic\": {\"title\": \"batman nolan\"}}\n        ],\n        \"rank_window_size\": 20\n      }\n    },\n    \"fields\": [\"title\"],\n    \"size\": 5\n  }'\n</code></pre>"},{"location":"features/indexing/types/text/#multi-vector-search-for-text-fields","title":"Multi-vector search for text[] fields","text":"<p>For <code>text[]</code> fields with semantic search enabled, Nixiesearch uses a multi-vector approach where each embedding is indexed separately. During indexing, child documents are created using Lucene's block-join structure, with one child per embedding.</p> <p>Note: For pre-embedded documents, the number of embeddings can differ from the number of text values. Specifically, a single text value can have multiple embeddings (useful for multi-perspective or chunk-based representations), but each text value must have at least one embedding in a 1:1 or 1:N relationship. See pre-embedded text fields for ingestion format details.</p> <p>At search time, the <code>knn</code> and <code>semantic</code> queries use Lucene's <code>DiversifyingChildrenFloatKnnVectorQuery</code> to: 1. Find the k-nearest child documents (embeddings) across all items in the field 2. Aggregate children back to their parent documents 3. Use the maximum similarity score among all children as the document score</p> <p>This ensures the most relevant embedding drives the document's relevance, useful for fields like product descriptions, document chunks, or review collections.</p>"},{"location":"features/indexing/types/text/#facets-filters-and-sorting","title":"Facets, filters and sorting","text":"<p>See facets, filters and sorting sections for more details.</p>"},{"location":"features/indexing/types/text/#suggestions","title":"Suggestions","text":"<p>Text fields can also be used for creating autocomplete suggestions:</p> <pre><code>curl -XPOST -d '{\"query\": \"h\", \"fields\":[\"title\"]}' http://localhost:8080/v1/index/&lt;index-name&gt;/suggest\n</code></pre> <p>The request above emits the following response:</p> <pre><code>{\n  \"suggestions\": [\n    {\"text\": \"hugo\", \"score\": 2.0},\n    {\"text\": \"hugo boss\", \"score\": 1.0},\n    {\"text\": \"hugo boss red\", \"score\": 1.0}\n  ],\n  \"took\": 11\n}\n</code></pre> <p>See Autocomplete suggestions section for more details.</p> <p>For further reading, check out how to define numeric fields in the index mapping.</p>"},{"location":"features/inference/completions/","title":"LLM chat completions inference","text":"<p>Competion models take an input prompt and generate a response. Internally these are used inside the RAG search endpoint to summarise search results in a single answer, but you can use them directly through the <code>/inference/completion/</code> REST Endpoint.</p>"},{"location":"features/inference/completions/#configuring-a-model","title":"Configuring a model","text":"<p>To use an LLM model for inference or for RAG search, you need to explicitly define it in the <code>inference.completion</code> config file section:</p> <pre><code>inference:\n  completion:\n    your-model-name:\n      provider: llamacpp\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n</code></pre> <p>Nixiesearch uses an embedded llamacpp server to handle models, so any GGUF model should work. Also note that you should prefer <code>instruct</code> models which are tuned for instruction-following, and not the raw models.</p> <p>Fields:</p> <ul> <li><code>provider</code>: required, string. As for <code>v0.3.0</code>, only <code>llamacpp</code> is supported. Other SaaS providers like OpenAI, Cohere, mxb and Google are on the roadmap.</li> <li><code>model</code>: required, string. A Huggingface handle, or an HTTP/Local/S3 URL for the model. See model URL reference for more details on how to load your model.</li> <li><code>file</code>: optional, string. A file name for the model, if the target model has multiple. A typical case for quantized models.</li> <li><code>options</code>: optional, obj. A dict of llamacpp-specific options.</li> </ul> <p>See the <code>inference.completion</code> section in config file reference for more details on other advanced options of providers.</p>"},{"location":"features/inference/completions/#sending-requests","title":"Sending requests","text":"<p>After you configured your completion LLM model, it becomes available for inference on the <code>/inference/completion/&lt;your-model-name&gt;</code> REST endpoint:</p> <pre><code>curl -d '{\"prompt\": \"what is 2+2? answer as haiku\", \"max_tokens\": 32}' http://localhost:8080/inference/completion/your-model-name\n</code></pre> <p>A full request payload looks like this:</p> <pre><code>{\n  \"prompt\": \"what is 2+2? answer as haiku\",\n  \"max_tokens\": 32,\n  \"stream\": false\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>prompt</code>: required, string. A prompt to process. Before doing the actual inference, the prompt text will be pre-processed using the prompt template for a particular model.</li> <li><code>max_tokens</code>: required, string. Number of tokens to generate. Consider that as a safety net if model cannot stop generating.</li> <li><code>stream</code>: optional, boolean, default <code>false</code>. Should the response be in a streaming format? If yes, the server will respond with a sequence of Server Side Events. See Streaming responses section for more details.</li> </ul>"},{"location":"features/inference/completions/#non-streaming-responses","title":"Non-streaming responses","text":"<p>A non-streaming regular HTTP response looks like this:</p> <pre><code>{\n  \"output\": \"Two\\nOne, one\\nSumming up\\nEqual to\\n2\",\n  \"took\": 191\n}\n</code></pre> <p>Response fields:</p> <ul> <li><code>output</code>: required, string. Generated answer for the prompt.</li> <li><code>took</code>: required, int. Number of milliseconds spent processing the request.</li> </ul>"},{"location":"features/inference/completions/#streaming-responses","title":"Streaming responses","text":"<p>When a completion request has a <code>\"streaming\": true</code> flag, then Nixiesearch will generate a sequence of Server Side Events for each generated token. This can be used to create a nice ChatGPT interfaces when you see the response being generated in real-time.</p> <p>So for a search request:</p> <pre><code>{\n  \"prompt\":\"what is 2+2? answer short\", \n  \"max_tokens\": 32, \n  \"stream\": true\n}\n</code></pre> <p>The server will generate a SSE payload, having a special <code>Content-Type: text/event-stream</code> header:</p> <pre><code>curl -v -d '{\"prompt\":\"what is 2+2? answer short\", \"max_tokens\": 32, \"stream\": true}'\\\n   http://localhost:8080/inference/completion/qwen2\n\n&lt; HTTP/1.1 200 OK\n&lt; Date: Fri, 13 Sep 2024 16:29:11 GMT\n&lt; Connection: keep-alive\n&lt; Content-Type: text/event-stream\n&lt; Transfer-Encoding: chunked\n&lt; \nevent: generate\ndata: {\"token\":\"2\",\"took\":34,\"last\":false}\n\nevent: generate\ndata: {\"token\":\"+\",\"took\":11,\"last\":false}\n\nevent: generate\ndata: {\"token\":\"2\",\"took\":11,\"last\":false}\n\nevent: generate\ndata: {\"token\":\" =\",\"took\":14,\"last\":false}\n\nevent: generate\ndata: {\"token\":\" \",\"took\":16,\"last\":false}\n\nevent: generate\ndata: {\"token\":\"4\",\"took\":14,\"last\":false}\n\nevent: generate\ndata: {\"token\":\"\",\"took\":13,\"last\":false}\n\nevent: generate\ndata: {\"token\":\"\",\"took\":1,\"last\":true}\n</code></pre> <p>The SSE frame has the following syntax:</p> <pre><code>{\n  \"token\": \"wow\",\n  \"took\": 10,\n  \"last\": false\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>token</code>: required, string. A next generated tokens. To get a full string, you need to concatenate all tokens together.</li> <li><code>took</code>: required, int. How many milliseconds spent generating this token. A first generated token also accounts for prompt processing time, so expect it to always be bigger.</li> <li><code>last</code>: required, boolean. Is this the last token? SSE has no notation of stream end, so you can use this field to assume that the stream is finished.</li> </ul>"},{"location":"features/inference/embeddings/","title":"Text embedding inference","text":"<p>Text embeddings map your text inputs into a numerical representation in such a way, so a query and relevant document embeddings should be close in the cosine similarity space. Multiple embeddings providers are supported:</p> <ul> <li>Local inference with Sentence-Transformers and ONNX - Any SBERT-compatible embedding model with local inference using ONNX runtime.</li> <li>OpenAI - Using external OpenAI-compatible APIs to compute embeddings.</li> <li>Cohere - Using Cohere v2/embed APIs for embeddings. </li> </ul>"},{"location":"features/inference/embeddings/#configuration-file","title":"Configuration file","text":"<p>To use a text embedding model for search or only for inference, you need to configure model in the <code>inference.embedding</code> section of a config file:</p> <pre><code>inference:\n  embedding:\n    your-model-name:\n      model: intfloat/e5-small-v2\n</code></pre> <p>You can also use OpenAI or any other API-based embedding provider: <pre><code>inference:\n  embedding:\n    your-model-name:\n      provider: openai\n      model: text-embedding-3-small\n</code></pre></p> <p>A full configuration looks like this:</p> <pre><code>inference:\n  embedding:\n    your-model-name:\n      provider: onnx\n      model: intfloat/e5-small-v2\n      cache:\n        memory:\n          max_size: 32768\n      prompt: # (1)\n        query: \"query: \"\n        doc: \"passage: \"\n</code></pre> <ol> <li>Nixiesearch can correctly guess the prompt format for all the supported models</li> </ol> <p>Fields:</p> <ul> <li><code>provider</code>: optional, string. As for <code>v0.5.0</code>, only the <code>onnx</code>, <code>openai</code> and <code>cohere</code> providers are supported. Default - auto-detected.</li> <li><code>model</code>: required, string. A Huggingface handle, or an HTTP/Local/S3 URL for the model. See model URL reference for more details on how to load your model.</li> <li><code>prompt</code>: optional. A document and query prefixes for asymmetrical models. Default - auto-detected.</li> <li><code>cache</code>: optional. As computing embeddings is latency heavy operation, caching can be used for frequent strings. See Embedding caching for more details.</li> </ul> <p>See inference.embedding config file reference for all advanced options of the ONNX provider.</p> <p>Nixiesearch supports the following set of models:</p> <ul> <li>any sentence-transformers compatible embedding model in the ONNX format. See the list of supported pre-converted models Nixiesearch already has, or check out the guide on how to convert your own model.</li> <li>As for version <code>0.3.0</code>, Nixiesearch only supports the <code>ONNX</code> provider for embedding inference. We have OpenAI, Cohere, mxb and Google providers on the roadmap.</li> </ul> <p>Note</p> <p>Many embedding models (like E5, BGE and GTE) for an optimal predictive performance require a specific prompt prefix for documents and queries. Please consult the model documentation for the expected format.</p>"},{"location":"features/inference/embeddings/#sending-requests","title":"Sending requests","text":"<p>After your model is configured in the <code>inference.embedding</code> section of the config file, you can send requests to the Nixiesearch endpoint <code>/inference/embedding/your-model-name</code>:</p> <pre><code>curl -v -d '{\"input\": [{\"text\": \"hello\"}]}' http://localhost:8080/inference/embedding/your-model-name\n</code></pre> <p>A full request payload looks like this:</p> <pre><code>{\n  \"input\": [\n    {\"text\": \"what is love?\", \"type\": \"query\"},\n    {\"text\": \"baby don't hurt me no more\", \"type\": \"document\"}\n  ]\n}\n</code></pre> <ul> <li><code>input</code>: list of objects, required. One or more texts to compute embeddings.</li> <li><code>input.text</code>: string, required. A text to compute embedding.</li> <li><code>input.type</code>: string, optional, default <code>raw</code>. A type of the input: <code>query</code>/<code>document</code>/<code>raw</code>. Some asymmetrical embedding models like E5/GTE/BGE produce different ones for queries and documents. </li> </ul> <p>Note</p> <p>When you embed many documents at once, Nixiesearch internally batches them together according to the inference model configuration. It is OK to sent large chunks of documents via inference API, they will be properly split to internal batches for better performance.</p>"},{"location":"features/inference/embeddings/#embedding-responses","title":"Embedding responses","text":"<p>A typical embedding response looks like this:</p> <pre><code>{\n  \"output\": [\n    {\n      \"embedding\": [\n        -0.43652993,\n        0.21856548,\n        0.011309982\n      ]\n    }\n  ],\n  \"took\": 4\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>output</code>: required, list of objects. Contains document embeddings in the same ordering as in request.</li> <li><code>output.embedding</code>: required, list of numbers. A vector of document embedding values. The dimensionality matches the embedding model configured.</li> <li><code>took</code>: required, int. Number of milliseconds spend processing the response.</li> </ul>"},{"location":"features/inference/overview/","title":"ML model inference","text":"<p>Nixiesearch also exposes embeddings and LLM chat completions APIs. These APIs used internally for semantic search and RAG tasks, but you can have a raw access to them. Typical use cases:</p> <ul> <li>adding an additional safety filter on top of RAG responses by prompting an LLM for <code>does {answer} answers the question {question}?</code>.</li> <li>preventing LLM hallucinations by embedding both question and RAG answer and computing cosine similarity between them. Question and a proper answer should be close to each other.</li> </ul>"},{"location":"features/inference/overview/#adding-models-for-inference","title":"Adding models for inference","text":"<p>To use an embedding or chat model for inference, you need to explicitly define it in the config file <code>inference</code> section:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: onnx\n      model: nixiesearch/e5-small-v2-onnx\n  completion:\n    &lt;model-name&gt;:\n      provider: llamacpp\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n</code></pre> <p>The inference section (and embedding/completion sub-sections also) are optional and not required if you do only lexical search. See a full config file reference for all the configuration options and a list of supported LLM models and embeddings.</p> <p>Note</p> <p>Inference for both embedding and completion models can also be done on GPUs.</p>"},{"location":"features/inference/overview/#supported-endpoints","title":"Supported endpoints","text":"<p>Nixiesearch supports the following inference endpoints:</p> <ul> <li><code>/inference/embeddings/&lt;model-name&gt;</code> for text embeddings to translate text inputs to numerical representations.</li> <li><code>/inference/completions/&lt;model-name&gt;</code> for LLM chat completions to prompt the underlying model with natural language questions.</li> </ul>"},{"location":"features/inference/embeddings/cache/","title":"Embedding cache","text":"<p>Computing local SBERT-compatible and API-based embeddings is latency-heavy task. Nixiesearch can cache hot queries in in-memory and remote Redis-based cache. This approach:</p> <ul> <li>minimises end-to-end search latency, as query embedding can take up to 300ms for API-based embedding providers.</li> <li>reduces costs, as most frequent queries are never re-embedded.</li> </ul> <p></p>"},{"location":"features/inference/embeddings/cache/#in-memory-cache","title":"In-memory cache","text":"<p>In-memory caching is a default option. It is defined per-model in the <code>inference</code> section of config file.</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      cache:\n        memory:\n          max_size: 32768\n</code></pre> <p>Cache can be disabled altogether:</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      cache: false\n</code></pre> <p>Cache is local to the node and is ephemeral (so is not persisted between restarts). If Nixiesearch runs in a standalone mode (e.g. when indexing and search tiers are co-located in a single process), the cache is shared between indexing and search threads. </p>"},{"location":"features/inference/embeddings/cache/#redis-cache","title":"Redis cache","text":"<p>Planned in Nixiesearch 0.6. </p>"},{"location":"features/inference/embeddings/cohere/","title":"Cohere API embedding inference","text":""},{"location":"features/inference/embeddings/cohere/#authentication","title":"Authentication","text":"<p>To provide an API key to authenticate to Cohere API, use the <code>COHERE_API_KEY</code> environment variable when starting Nixiesearch:</p> <pre><code>docker run -it -e COHERE_API_KEY=&lt;thekey&gt; nixiesearch/nixiesearch &lt;opts&gt;\n</code></pre>"},{"location":"features/inference/embeddings/cohere/#usage","title":"Usage","text":"<p>To define a Cohere embedding model in the config file, use the following snippet:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: cohere\n      model: embed-english-v3.0\n</code></pre> <p>The full configuration with all default options:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: cohere\n      model: embed-english-v3.0\n      timeout: 2000ms\n      endpoint: \"https://api.cohere.com/\"\n      batch_size: 32\n      retry: 1\n</code></pre> <p>Parameters:</p> <ul> <li>timeout: optional, duration, default 2s. External APIs might be slow sometimes.</li> <li>batch_size: optional, int, default 32. Batch size for calls with many documents.</li> <li>retry: optional, int, default 1. Number of retries to perform when API fails.</li> </ul> <p>See Config file reference for more details on creating a config file. </p>"},{"location":"features/inference/embeddings/openai/","title":"OpenAI API embedding inference","text":""},{"location":"features/inference/embeddings/openai/#authentication","title":"Authentication","text":"<p>To provide an API key to authenticate to OpenAI API, use the <code>OPENAI_API_KEY</code> environment variable when starting Nixiesearch:</p> <pre><code>docker run -it -e OPENAI_API_KEY=&lt;thekey&gt; nixiesearch/nixiesearch &lt;opts&gt;\n</code></pre>"},{"location":"features/inference/embeddings/openai/#usage","title":"Usage","text":"<p>Nixiesearch supports any OpenAI-compatible embedding endpoints (e.g llamacpp). To define an OpenAI embedding model in the config file, use the following snippet:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: openai\n      model: text-embedding-3-small\n</code></pre> <p>The full configuration with all default options:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: openai\n      model: text-embedding-3-small\n      timeout: 2000ms\n      endpoint: \"https://api.openai.com/\"\n      dimensions: null\n      batch_size: 32\n      retry: 1\n</code></pre> <p>Parameters:</p> <ul> <li>timeout: optional, duration, default 2s. External APIs might be slow sometimes.  </li> <li>endpoint: optional, string, default \"https://api.openai.com/\". You can use alternative API or EU-specific endpoint.</li> <li>dimensions: optional, int, default empty. For matryoshka models, how many dimensions to return.</li> <li>batch_size: optional, int, default 32. Batch size for calls with many documents.</li> <li>retry: optional, int, default 1. Number of retries to perform when API fails.</li> </ul> <p>See Config file reference for more details on creating a config file. </p>"},{"location":"features/inference/embeddings/sbert/","title":"Sentence Transformers","text":""},{"location":"features/inference/embeddings/sbert/#supported-embedding-models","title":"Supported embedding models","text":"<p>Nixiesearch supports any sentence-transformers-compatible model in the ONNX format.</p> <p>The following list of models is tested to work well with Nixiesearch: </p> <ul> <li>there is an ONNX model provided in the repo (e.g. a <code>model.onnx</code> file),</li> <li>input tensor shapes are supported,</li> <li>Nixiesearch can correctly guess query and document prompt format (like E5-family of models requiring <code>query:</code> and <code>passage:</code> prefixes),</li> <li>embedding pooling method is supported - <code>CLS</code>, <code>mean</code>, or <code>last</code> (for decoder-style models like Qwen3-Embedding).</li> </ul> <p>Note</p> <p>Nixiesearch can automatically guess the proper prompt format and pooling method for all the models in the supported list table below. You can override this behavior in the model configuration section with <code>pooling</code> and <code>prompt</code> parameters.</p>"},{"location":"features/inference/embeddings/sbert/#list-of-supported-models","title":"List of supported models","text":"Name Size Seqlen Dimensions Prompt Pooling sentence-transformers/all-MiniLM-L6-v2 22M 512 384 not needed mean sentence-transformers/all-MiniLM-L12-v2 33M 512 384 not needed mean sentence-transformers/all-mpnet-base-v2 109M 384 384 not needed mean intfloat/e5-small 33M 512 384 query+doc mean intfloat/e5-base 109M 512 768 query+doc mean intfloat/e5-large 335M 512 1024 query+doc mean intfloat/e5-small-v2 33M 512 384 query+doc mean intfloat/e5-base-v2 109M 512 768 query+doc mean intfloat/e5-large-v2 335M 512 1024 query+doc mean intfloat/multilingual-e5-small 118M 512 384 auto mean intfloat/multilingual-e5-small 278M 512 768 auto mean intfloat/multilingual-e5-small 560M 512 1024 auto mean Alibaba-NLP/gte-base-en-v1.5 137M 8192 768 not needed CLS Alibaba-NLP/gte-large-en-v1.5 434M 8192 1024 not needed CLS Alibaba-NLP/gte-modernbert-base 149M 8192 768 not needed CLS Snowflake/snowflake-arctic-embed-s 33M 512 384 query CLS Snowflake/snowflake-arctic-embed-xs 22M 512 384 query CLS Snowflake/snowflake-arctic-embed-m 109M 512 768 query CLS Snowflake/snowflake-arctic-embed-m-v1.5 109M 512 768 query CLS Snowflake/snowflake-arctic-embed-m-v2.0 109M 512 768 query CLS Snowflake/snowflake-arctic-embed-l 109M 512 1024 query CLS Snowflake/snowflake-arctic-embed-l-v2.0 109M 512 1024 query CLS BAAI/bge-small-en-v1.5 33M 512 384 query mean BAAI/bge-base-en-v1.5 109M 512 768 query mean BAAI/bge-large-en-v1.5 33M 512 1024 query mean BAAI/bge-small-zh-v1.5 33M 512 384 query mean BAAI/bge-base-zh-v1.5 109M 512 768 query mean BAAI/bge-large-zh-v1.5 33M 512 1024 query mean BAAI/bge-m3 560M 8192 1024 not needed mean WhereIsAI/UAE-Large-V1 335M 512 1024 query mean mixedbread-ai/mxbai-embed-large-v1 335M 512 1024 query CLS jinaai/jina-embeddings-v3 572M 8192 1024 query+doc mean NovaSearch/stella_en_400M_v5 435B 4096 8192 query mean Qwen/Qwen3-Embedding-0.6B 600M 8192 1024 query last Qwen/Qwen3-Embedding-4B 4B 8192 1024 query last Qwen/Qwen3-Embedding-8B 8B 8192 1024 query last <p>If the model is not listed in this table, but has an ONNX file available, then most probably it should work well. But you might set <code>prompt</code> and <code>pooling</code> parameters based on model documentation. See embedding model configuration section for more details.</p>"},{"location":"features/inference/embeddings/sbert/#model-handles","title":"Model handles","text":"<p>Nixiesearch supports loading models directly from Huggingface by its handle (e.g. <code>sentence-transformers/all-MiniLM-L6-v2</code>) and from local file directory.</p> <p>You can reference any HF model handle in the inference block, for example:</p> <p><pre><code>inference:\n  embedding:\n    e5-small:\n      model: sentence-transformers/all-MiniLM-L6-v2\n</code></pre> It also works with local paths:</p> <pre><code>inference:\n  embedding:\n    your-model:\n      model: /path/to/model/dir\n</code></pre> <p>Optionally you can define which particular ONNX file to load, for example the QInt8 quantized one:</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      file: model_opt2_QInt8.onnx\n</code></pre> <p>To enable caching for frequent strings, use the <code>cache</code> option. See Embedding caching for more details.</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      cache:\n        memory:\n          max_size: 1024 # cache top-N most popular embeddings\n</code></pre>"},{"location":"features/inference/embeddings/sbert/#device-allocation","title":"Device allocation","text":"<p>By default, ONNX models run on CPU using all available CPU cores. You can configure which device to use for model inference by adding a <code>device</code> section to your model configuration:</p> <pre><code>inference:\n  embedding:\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      device: cpu        # Use CPU with default thread count (number of CPU cores)\n</code></pre> <p>For GPU acceleration, specify a CUDA device (requires CUDA-enabled Docker image - see GPU deployment guide):</p> <pre><code>inference:\n  embedding:\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      device: cuda:0     # Use GPU device 0\n</code></pre> <p>You can also control CPU thread allocation:</p> <pre><code>inference:\n  embedding:\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      device: cpu:8      # Use CPU with 8 threads\n</code></pre> <p>Available device options: - <code>cpu</code> - CPU execution with default thread count (number of available CPU cores) - <code>cpu:N</code> - CPU execution with N threads (where N &gt; 0) - <code>cuda</code> - GPU execution on device 0 (default GPU) - <code>cuda:N</code> - GPU execution on specific GPU device N (where N &gt;= 0)</p>"},{"location":"features/inference/embeddings/sbert/#converting-your-own-model","title":"Converting your own model","text":"<p>You can use the nixiesearch/onnx-convert to convert your own model:</p> <pre><code>python convert.py --model_id intfloat/multilingual-e5-large --optimize 2 --quantize QInt8\n</code></pre> <p><pre><code>Conversion config: ConversionArguments(model_id='intfloat/multilingual-e5-base', quantize='QInt8', output_parent_dir='./models/', task='sentence-similarity', opset=None, device='cpu', skip_validation=False, per_channel=True, reduce_range=True, optimize=2)\nExporting model to ONNX\nFramework not specified. Using pt to export to ONNX.\nUsing the export variant default. Available variants are:\n    - default: The default ONNX variant.\nUsing framework PyTorch: 2.1.0+cu121\nOverriding 1 configuration item(s)\n        - use_cache -&gt; False\nPost-processing the exported models...\nDeduplicating shared (tied) weights...\nValidating ONNX model models/intfloat/multilingual-e5-base/model.onnx...\n        -[\u2713] ONNX model output names match reference model (last_hidden_state)\n        - Validating ONNX Model output \"last_hidden_state\":\n                -[\u2713] (2, 16, 768) matches (2, 16, 768)\n                -[\u2713] all values close (atol: 0.0001)\nThe ONNX export succeeded and the exported model was saved at: models/intfloat/multilingual-e5-base\nExport done\nProcessing model file ./models/intfloat/multilingual-e5-base/model.onnx\nONNX model loaded\nOptimizing model with level=2\nOptimization done, quantizing to QInt8\n</code></pre> See the nixiesearch/onnx-convert repo for more details and options.</p>"},{"location":"features/search/facet/","title":"Facet aggregations","text":"<p>Facet aggregation counters can help in building a filter-based faceted search, when for a single search query you get not only search results, but also all possible filter values, sorted by the number of matching documents:</p> <pre><code>{\n  \"query\": {\"match\": {\"title\": \"socks\"}},\n  \"aggs\": {\n    \"colors\": {\"term\": {\"field\": \"color\"}}\n  }\n}\n</code></pre> <p>A sample query with an aggregation over a <code>color</code> field above, will result in the following response with all the possible colors in matching documents:</p> <pre><code>{\n  \"hits\": [\"&lt;doc1&gt;\", \"&lt;doc2&gt;\", \"...\"],\n  \"aggs\": {\n    \"colors\": {\n      \"buckets\": {\n        \"red\": 10,\n        \"green\": 5,\n        \"blue\": 1\n      }\n    }\n  }\n}\n</code></pre> <p>The JSON schema for the aggregation field is:</p> <pre><code>{\n  \"aggs\": {\n    \"&lt;aggregation_name_1&gt;\": {\n      \"&lt;aggregation_type\": { \"...\": \"...\" }\n    },\n    \"&lt;aggregation_name_2&gt;\": {\n      \"&lt;aggregation_type\": { \"...\": \"...\" }\n    }\n  }\n}\n</code></pre> <p>Single request can contain multiple aggregations as long as they have unique names.</p> <p>Nixiesearch currently supports the following types of facet aggregations:</p> <ul> <li>Term facet counters with a number of documents matching each distinct filter value.</li> <li>Range counters to number the amount of documents within each defined range.</li> </ul>"},{"location":"features/search/facet/#term-aggregations","title":"Term aggregations","text":"<p>A term facet aggregation scans over all values of a specific text, long, int, float, double, date, datetime, int[], long[], float[], double[] field of matching documents, and builds a list of top-N values:</p> <pre><code>{\n  \"aggs\": {\n    \"count_colors\": {\n      \"term\": {\n        \"field\": \"color\",\n        \"size\": 10\n      }\n    }\n  }\n}\n</code></pre> <p>Term aggregation has the following parameters:</p> <ul> <li><code>field</code>: required, string, over which field to aggregate over. The field must be marked as <code>facet: true</code> in the index mapping.</li> <li><code>size</code>: optional, integer or <code>\"all\"</code>, how many top values to collect, default: 10. A special <code>\"all\"</code> value is a substitute for <code>Integer.MAX_VALUE</code> - useful when you need to receive all values.</li> </ul> <p>Term aggregation response has a list of N buckets and counters, sorted from most to least popular:</p> <pre><code>{\n  \"hits\": [\"&lt;doc1&gt;\", \"&lt;doc2&gt;\", \"...\"],\n  \"aggs\": {\n    \"count_colors\": {\n      \"buckets\": {\n        \"red\": 10,\n        \"green\": 5,\n        \"blue\": 1\n      }\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>Computing term facet aggregations requires creating an internal Lucene DocValues field, which has to be kept in RAM for the best performance. Try to minimize the amount of faceted fields to keep RAM usage low.</p>"},{"location":"features/search/facet/#range-aggregations","title":"Range aggregations","text":"<p>Range aggregation scans over all values of a specific numerical field for matching documents, and builds a list of top-N ranges:</p> <pre><code>{\n  \"aggs\": {\n    \"count_prices\": {\n      \"range\": {\n        \"field\": \"price\",\n        \"ranges\": [\n          {\"lt\": 10},\n          {\"gte\": 10, \"lt\": 100},\n          {\"gte\": 100}\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>Range facet aggregation has the following parameters:</p> <ul> <li><code>field</code>: required, string. A field to compute range aggregation. Should be marked as <code>facet: true</code> in index mapping and had the type of <code>int</code>/<code>float</code>/<code>double</code>/<code>long</code>/<code>date</code>/<code>datetime</code>/<code>int[]</code>/<code>float[]</code>/<code>double[]</code>/<code>long[]</code></li> <li><code>ranges</code>, required, non-empty list.</li> <li><code>ranges.lt</code>, optional, number. Less Than. An end of the range, not inclusive.</li> <li><code>ranges.lte</code>, optional, number. Less Than or Equals. An end of the range, inclusive.</li> <li><code>ranges.gt</code>, optional, number. Greater Than. A start of the range, not inclusive.</li> <li><code>ranges.gte</code>, optional, number. Greater Than or Equals. A start of the range, inclusive.</li> </ul> <p>A single range must have at least one <code>gt</code>/<code>gte</code>/<code>lt</code>/<code>lte</code> field.</p> <p>Range facet aggregation response keeps the same ranges as in request, but adds a <code>count</code> field to each of them:</p> <pre><code>{\n  \"hits\": [\"&lt;doc1&gt;\", \"&lt;doc2&gt;\", \"...\"],\n  \"aggs\": {\n    \"count_prices\": {\n      \"buckets\": [\n        {\"lt\": 10, \"count\": 10},\n        {\"gte\": 10, \"lt\": 100, \"count\": 4},\n        {\"gte\": 5, \"count\": 2}\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/facet/#aggregating-over-array-fields","title":"Aggregating over array fields","text":"<p>When aggregating over numeric array fields (<code>int[]</code>, <code>long[]</code>, <code>float[]</code>, <code>double[]</code>), the aggregation considers all elements in each array. A document will be counted in a range bucket if any element in its array falls within that range.</p> <p>For example, with a document containing <code>\"ratings\": [2, 5, 8]</code> and range aggregation with buckets <code>[0-5)</code>, <code>[5-10)</code>:</p> <ul> <li>The document will be counted in the <code>[0-5)</code> bucket (due to values 2 and 5)</li> <li>The document will also be counted in the <code>[5-10)</code> bucket (due to values 5 and 8)</li> </ul> <pre><code>{\n  \"aggs\": {\n    \"rating_ranges\": {\n      \"range\": {\n        \"field\": \"ratings\",\n        \"ranges\": [\n          {\"gte\": 1, \"lt\": 3},\n          {\"gte\": 3, \"lt\": 5},\n          {\"gte\": 5}\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>This behavior makes array field aggregations useful for analyzing multi-valued attributes like product categories, user ratings, or feature scores.</p>"},{"location":"features/search/filter/","title":"Filters","text":"<p>Filters allow you to search over a subset of documents based on a set of predicates. Compared to traditional Lucene-based search engines, Nixiesearch filters are defined separately from the text query and do not affect ranking:</p> <pre><code>{\n  \"query\": { \"match\": { \"title\": \"socks\"}},\n  \"filters\": {\n    \"include\": {\n      \"term\": { \"size\": \"XXL\" }\n    },\n    \"exclude\": {\n      \"range\": { \"price\": {\"gte\": 100}}\n    }\n  }\n}\n</code></pre> <p><code>filters</code> can <code>include</code> and <code>exclude</code> documents based on multiple types of filters:</p> <ul> <li><code>term</code> for text predicates: match only documents where <code>color=red</code></li> <li><code>range</code> for numerical ranges: match documents with <code>price&gt;100</code></li> <li><code>and</code>/<code>or</code>/<code>not</code> for combining multiple filters into a single boolean expression.</li> </ul> <p>To perform filtered queries over a field, you should define the field as <code>filter: true</code> in index mapping. Nixiesearch will emit a warning if you relentlessly try to filter over a non-filterable field.</p>"},{"location":"features/search/filter/#term-filters","title":"Term filters","text":"<p>Term predicate can be defined as a simple JSON key-value pair, where key is a field name, and value is a predicate:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"term\": {\n        \"&lt;field_name&gt;\": \"&lt;field_value&gt;\"\n      }\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>A simple term filter works only with a single field and a single value. If you want to filter over multiple fields and multiple values, use a boolean filter to combine them together in a single expression.</p> <p>Term filters currently support the following field types: <code>int</code>, <code>long</code>, <code>date</code>, <code>datetime</code>, <code>string</code>, <code>boolean</code>. For example, filtering over a boolean field called <code>active</code> can be done with the following query:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"term\": {\n        \"active\": true\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/filter/#range-filters","title":"Range filters","text":"<p>Range filters allow defining open and closed ranges for numeric fields of types <code>[int, long, double, float, date, datetime]</code> and their array variants <code>[int[], long[], double[], float[]]</code> to pre-select documents for search:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"range\": {\n        \"&lt;field_name&gt;\": { \"gte\": 100.0, \"lte\": 1000.0 }\n      }\n    }\n  }\n}\n</code></pre> <p>Range filter takes following arguments:</p> <ul> <li><code>&lt;field_name&gt;</code> a numeric field marked as <code>filter: true</code> in the index mapping</li> <li><code>gt</code>/<code>gte</code>: Greater Than (or Equals), optional field</li> <li><code>lt</code>/<code>lte</code>: Less Than (or Equals), optional field. </li> </ul> <p>There must be at least one <code>gt</code>/<code>gte</code>/<code>lt</code>/<code>lte</code> field present in the filter.</p>"},{"location":"features/search/filter/#filtering-array-fields","title":"Filtering array fields","text":"<p>When filtering array/list fields (<code>int[]</code>, <code>long[]</code>, <code>float[]</code>, <code>double[]</code>), the range filter matches if any element in the array satisfies the condition:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"range\": {\n        \"ratings\": { \"gte\": 4 }\n      }\n    }\n  }\n}\n</code></pre> <p>This query matches documents where any rating in the <code>ratings</code> array is &gt;= 4. For example, a document with <code>\"ratings\": [2, 3, 5, 1]</code> would match because one element (5) satisfies the condition.</p> <p>You can also combine multiple range conditions on array fields:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"and\": [\n        { \"range\": { \"sizes\": { \"gte\": 7.0 }}},\n        { \"range\": { \"sizes\": { \"lte\": 10.0 }}}\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/filter/#boolean-filters","title":"Boolean filters","text":"<p>You can combine multiple basic range and term filters together into a more complicated boolean expression using <code>and</code>, <code>or</code> and <code>not</code> filter types from the boolean family. Each of these filter types takes a list of other filters as an argument:</p> <pre><code>{\n  \"query\": {\"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"and\": [ \"&lt;filter 1&gt;\", \"&lt;filter 2&gt;\", \"...\" ]\n    }\n  }\n}\n</code></pre> <p>For example, to match documents with multiple field values at once, you can define the following query:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"or\": [\n        { \"term\": { \"color\": \"red\" }},\n        { \"term\": { \"color\": \"green\"}}\n      ]\n    }\n  }\n}\n</code></pre> <p>Nesting of boolean filters is also possible:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"and\": [\n        {\"range\": { \"price\": {\"gte\": 100}}},\n        {\n          \"or\": [\n            {\"term\": {\"color\": \"red\"}},\n            {\"term\": {\"color\": \"green\"}}\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/filter/#geolocation-filters","title":"Geolocation filters","text":"<p>With a <code>geopoint</code> datatype, you can filter by a <code>geo_distance</code> and <code>geo_box</code> predicates.</p>"},{"location":"features/search/filter/#distance-filters","title":"Distance filters","text":"<p>With a distance query, you can include/exclude documents being within a distance from a point. Query example:</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"geo_distance\": {\n        \"field\": \"&lt;field_name&gt;\",\n        \"lat\": 1.0,\n        \"lon\": 2.0,\n        \"distance\": \"1 km\"\n      }\n    }\n  }\n}\n</code></pre> <p>Following distance units are supported: </p> <ul> <li>Millimeters: <code>mm</code>, <code>millimeters</code>, <code>millimeter</code></li> <li>Centimeters: <code>cm</code>, <code>centimeters</code>, <code>centimeter</code></li> <li>Meters: <code>m</code>, <code>meter</code>, <code>meters</code></li> <li>Kilometers: <code>km</code>, <code>kilometer</code>, <code>kilometers</code></li> <li>Inches: <code>in</code>, <code>inch</code>, <code>inches</code></li> <li>Feet: <code>ft</code>, <code>foot</code>, <code>feet</code></li> <li>Yards: <code>yd</code>, <code>yard</code>, <code>yards</code></li> <li>Miles: <code>mi</code>, <code>mile</code>, <code>miles</code></li> </ul>"},{"location":"features/search/filter/#bounding-box-filters","title":"Bounding box filters","text":"<p>With a bounding box query, you can include documents laying within a specific rectangle.</p> <pre><code>{\n  \"query\": { \"match_all\": {}},\n  \"filters\": {\n    \"include\": {\n      \"geo_box\": {\n        \"field\": \"&lt;field_name&gt;\",\n        \"top_left\": {\"lat\": 1.0, \"lon\": 2.0},\n        \"bottom_right\": {\"lat\": 3.0, \"lon\": 4.0}\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/filter/#filters-and-lexicalsemantic-search","title":"Filters and lexical/semantic search","text":"<p>Nixiesearch relies on Lucene logic to handle filter execution:</p> <ul> <li>for lexical search include/exclude filters are fused together into a single Lucene query, doing filtering and ranking in a single pass.</li> <li>for semantic search filter behavior is selected at run-time based on filter coverage estimation. </li> </ul> <p>Narrow filters (e.g. selecting only small amount of documents) are defined as pre-filters and executed before the query. Wide filters (e.g. selecting a lot of documents) are executed as post-filters after the main search query. This adaptive behavior is made for performance reasons.</p>"},{"location":"features/search/overview/","title":"Search","text":"<p>To search for documents indexed in Nixiesearch, you can use the following request JSON format:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"&lt;search-field-name&gt;\", \"&lt;search-field-name&gt;\"],\n      \"query\": \"&lt;query-string&gt;\"\n    }\n  }\n}\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;search-field-name&gt;</code>: a text field marked as searchable in the index mapping</li> <li><code>&lt;query-string&gt;</code>: a string to search for.</li> <li><code>multi_match</code>: one of the matching DSL rules. Check more examples of Query DSL in the reference.</li> </ul> <p>For such a search request, Nixiesearch will reply with a JSON response with top-N matching documents:</p> <pre><code>{\n  \"took\": {\n    \"total\": 0.100,\n    \"search\": 0.050\n  },\n  \"hits\": [\n    {\"_id\": \"1\", \"title\": \"hello\", \"_score\": 2},\n    {\"_id\": \"2\", \"title\": \"world\", \"_score\": 1}\n  ]\n}\n</code></pre> <p><code>_id</code> and <code>_score</code> are built-in fields always present in the document payload.</p> <p>Note</p> <p>Compared to Elasticsearch/Opensearch, Nixiesearch has no built-in <code>_source</code> field as it is frequently mis-used. You need to explicitly mark fields you want to be present in response payload as <code>store: true</code> in the index mapping.</p>"},{"location":"features/search/overview/#rag-retrieval-augmented-generation","title":"RAG: Retrieval Augmented Generation","text":"<p>Instead of just getting search results for your query, you can use a RAG approach to get a natural language answer to your query, built with locally-running LLM.</p> <p></p> <p>Nixiesearch supports any GGUF-compatible LLM llamacpp supports. To use RAG, you need to list Huggingface handles of models you'd like to use in the <code>inference</code> section of the config file:</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: intfloat/e5-small-v2\n  completion:\n    # Used for summarization\n    qwen2:\n      provider: llamacpp\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n\nschema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          semantic:\n            model: e5-small\n      overview:\n        type: text\n        search: \n          semantic:\n            model: e5-small\n</code></pre> <p>Here we use a Qwen/Qwen2-0.5B-Instruct-GGUF model with explicitly defined filename (as there may be multiple GGUF model files in the repo).</p> <p>LLM inference on a CPU is a tough task, expect much higher latencies for RAG requests, compared to regular ones.</p> <p>After that we can send RAG search requests to our index:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"title\", \"description\"],\n      \"query\": \"what is pizza\"\n    }\n  },\n  \"rag\": {\n    \"prompt\": \"Summarize search results for a query 'what is pizza'\",\n    \"model\": \"qwen2\",\n    \"fields\": [\"title\", \"description\"]\n  }\n}\n</code></pre> <p>For this query, Nixiesearch will perform following actions: * Make a search for the query <code>what is pizza</code> over <code>title</code> and <code>description</code> fields * pick top-N matching documents from results, and build an LLM prompt: <pre><code>Summarize search results for a query 'what is pizza':\n\n[1]: Pizza is a traditional Italian dish typically consisting of ...\n\n[2]: One of the simplest and most traditional pizzas is the Margherita ...\n\n[3]: The meaning of PIZZA is a dish made typically of flattened bread dough ...\n</code></pre> * stream generated response among search results:</p> <pre><code>{\n  \"took\": 10,\n  \"hits\": [\n    {\"_id\": 1, \"title\": \"...\", \"description\":  \"...\"},\n    {\"_id\": 1, \"title\": \"...\", \"description\":  \"...\"},\n    {\"_id\": 1, \"title\": \"...\", \"description\":  \"...\"}\n  ],\n  \"response\": \"Pizza is a dish of Italian origin ...\"\n}\n</code></pre> <p>As LLM inference is a costly operation, Nixiesearch supports a WebSocket response streaming: you immediately get search result documents in a first frame, and LLM-generated tokens are streamed while being generated. See RAG reference for more details.</p>"},{"location":"features/search/overview/#hybrid-search-with-reciprocal-rank-fusion","title":"Hybrid search with Reciprocal Rank Fusion","text":"<p>When you search over text fields marked as both <code>lexical</code> and <code>semantic</code> search capable in the index mapping, you can use the <code>rrf</code> ranking operator to fuse two search results into a single result list:</p> <pre><code>curl -XPOST http://localhost:8080/v1/index/movies/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \n    \"query\": {\n      \"rrf\": {\n        \"queries\": [\n          {\"match\": {\"title\": \"batman\"}},\n          {\"semantic\": {\"title\": \"batman nolan\"}}\n        ],\n        \"rank_window_size\": 20\n      } \n    }, \n    \"fields\": [\"title\"], \n    \"size\": 5\n  }'\n</code></pre> <p>While performing hybrid search, Nixiesearch does the following:</p> <ol> <li>Collects a separate per-field search result list for semantic and lexical retrieval methods.</li> <li>Merges N search results with RRF - Reciprocal Rank Fusion.</li> </ol> <p></p> <p>RRF merging approach:</p> <ul> <li>Does not use a document score directly (so BM25 or cosine-distance), but a document position in a result list when sorted by the score.</li> </ul> <p>For even better relevance, you can combine RRF results with cross-encoder reranking to get neural-powered relevance scoring on top of hybrid retrieval. * Scores of documents from multiple lists are combined together. * Final ranking is made by sorting merged document list by the combined score.</p> <p>Compared to traditional methods of combining multiple BM25 and cosine scores together, RRF does not depend on the scale and statistical distribution of the underlying scores - and can generate more stable results.</p> <p>See the RRF ranker section for more details.</p>"},{"location":"features/search/overview/#filters","title":"Filters","text":"<p>To select a sub-set of documents for search, add <code>filters</code> directive to the request JSON payload:</p> <p><pre><code>{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"filters\": {\n    \"include\": {\n      \"term\": {\n        \"field\": \"color\",\n        \"value\": \"red\"\n      }\n    }\n  }\n}\n</code></pre> Nixiesearch supports the following set of filter types:</p> <ul> <li>Term filters - to match over text fields.</li> <li>Range filters - to select over numeric <code>int</code>/<code>long</code>/<code>float</code>/<code>double</code>/<code>bool</code> fields.</li> <li>Compound boolean filters - to combine multiple filter types within a single filter predicate.</li> </ul> <p>See Filters DSL reference for more examples and details.</p> <p>Note</p> <p>A field should be defined as <code>filter: true</code> to be used in filter expressions.</p>"},{"location":"features/search/overview/#facets","title":"Facets","text":"<p>Facet count aggregation is useful for building a faceted search: for a search query apart from documents, response contains also a set of possible filter values (sorted by a number of documents this filter value will match).</p> <p>A JSON search request payload can be extended with the <code>aggs</code> parameter:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {}\n  },\n  \"aggs\": {\n    \"count_colors\": {\n      \"term\": {\n        \"field\": \"color\",\n        \"count\": 10\n      }\n    }\n  }\n}\n</code></pre> <p>Where <code>count_colors</code> is an aggregation name, this is a <code>term</code> aggregation over a field <code>color</code>, returning top-<code>10</code> most frequent values for this field.</p> <p>Each facet aggregation adds an extra named section in the search response payload:</p> <pre><code>{\n  \"hits\": [\n    {\"_id\": \"1\", \"_score\": 10},\n    {\"_id\": \"1\", \"_score\": 5},\n  ],\n  \"aggs\": {\n    \"count_colors\": {\n      \"buckets\": [\n        {\"term\": \"red\", \"count\": 10},\n        {\"term\": \"green\", \"count\": 5},\n        {\"term\": \"blue\", \"count\": 2},\n      ]\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>A field should be marked as <code>facet: true</code> to be used in facet aggregations.</p> <p>See a Facet Aggregation DSL section in reference for more details.</p>"},{"location":"features/search/rag/","title":"RAG: Retrieval Augmented Generation","text":"<p>Nixiesearch supports RAG-style question answering over fully local LLMs. For improved document retrieval quality in RAG pipelines, consider using cross-encoder reranking to ensure the most relevant documents are passed to the LLM.</p> <p></p> <p>To use RAG queries, you need to explicitly define in the <code>inference</code> section of the config file which LLMs you plan to use query-time:</p> <pre><code>inference:\n  embedding:\n    # Used for semantic retrieval\n    e5-small:\n      model: intfloat/e5-small-v2\n  completion:\n    # Used for summarization\n    qwen2:\n      provider: llamacpp\n      # Warning: this is a very small and dummy model\n      # for production uses consider using something bigger.\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n\nschema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: \n          semantic:\n            model: e5-small\n      overview:\n        type: text\n        search:\n          semantic:\n            model: e5-small\n</code></pre> <p>Where:</p> <ul> <li><code>model</code>: a Huggingface model handle in a format of <code>namespace</code>/<code>model-name</code>. </li> <li><code>name</code>: name of this model you will reference in RAG search requests</li> </ul> <p>Nixiesearch uses a default prompt format from the GGUF model.</p>"},{"location":"features/search/rag/#sending-requests","title":"Sending requests","text":"<p>For RAG requests, Nixiesearch supports REST and Server Side Events for streaming responses:</p> <ul> <li>REST: much simpler to implement, but blocks till full RAG response is generated.</li> <li>SSE: can stream each generated response token, but is more complex to set up.</li> </ul> <p>Request format is the same for both protocols:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"title\", \"description\"],\n      \"query\": \"what is pizza\"\n    }\n  },\n  \"fields\": [\"title\", \"description\"],\n  \"rag\": {\n    \"prompt\": \"Summarize search results for a query 'what is pizza'\",\n    \"model\": \"qwen2\",\n    \"stream\": false\n  }\n}\n</code></pre> <p>The <code>rag</code> field has the following options:</p> <ul> <li><code>stream</code> (boolean, optional, default <code>false</code>): Should we stream response with SSE, or just block until the complete response is generated.</li> <li><code>prompt</code> (string, required): A main instruction for the LLM.</li> <li><code>model</code> (string, required): Model name from the <code>rag.models</code> index mapping section.</li> <li><code>fields</code> (string[], optional): A list of fields from the search results documents to embed to the LLM prompt. By default, use all stored fields from the response.</li> <li><code>topDocs</code> (int, optional): How many top-N documents to embed to the prompt. By default pick top-10, more documents - longer the context - higher the latency.</li> <li><code>maxDocLength</code> (int, optional): Limit each document in prompt by first N tokens. By default, use first 128 tokens.</li> <li><code>maxResponseLength</code> (int, optional): Maximum number of tokens LLM can generate. Default 64.</li> </ul>"},{"location":"features/search/rag/#rest-responses","title":"REST responses","text":"<p>A complete text of the LLM response you can find in a <code>response</code> field:</p> <pre><code>$&gt; cat rag.json\n\n{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"title\"],\n      \"query\": \"matrix\"\n    }\n  },\n  \"fields\": [\"title\"],\n  \"rag\": {\n    \"prompt\": \"Summarize search results for a query 'matrix'\",\n    \"model\": \"qwen2\"\n  }\n}\n\n$&gt; curl -v -XPOST -d @rag.json http://localhost:8080/movies/_search\n\n{\n  \"took\": 3,\n  \"hits\": [\n    {\n      \"_id\": \"604\",\n      \"title\": \"The Matrix Reloaded\",\n      \"_score\": 0.016666668\n    },\n    {\n      \"_id\": \"605\",\n      \"title\": \"The Matrix Revolutions\",\n      \"_score\": 0.016393442\n    },\n  ],\n  \"aggs\": {},\n  \"response\": \"The following is a list of search results for the query 'matrix'. It includes the following:\\n\\n- The matrix is the first film in the \\\"Matrix\\\" franchise.\"\n}\n</code></pre>"},{"location":"features/search/rag/#streaming-responses","title":"Streaming responses","text":"<p>The main REST search endpoint <code>/&lt;index_name&gt;/_search</code> can also function as an SSE endpoint. </p> <pre><code>$&gt; cat rag.json\n\n{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"title\"],\n      \"query\": \"matrix\"\n    }\n  },\n  \"fields\": [\"title\"],\n  \"rag\": {\n    \"prompt\": \"Summarize search results for a query 'matrix'\",\n    \"model\": \"qwen2\",\n    \"stream\" true\n  }\n}\n\n$&gt; curl -v -XPOST -d @rag.json http://localhost:8080/movies/_search\n\n&lt; HTTP/1.1 200 OK\n&lt; Date: Fri, 13 Sep 2024 16:29:11 GMT\n&lt; Connection: keep-alive\n&lt; Content-Type: text/event-stream\n&lt; Transfer-Encoding: chunked\n&lt; \nevent: results\ndata: {\"took\":3,\"hits\":[\"... skipped ...\"],\"aggs\":{},\"ts\":1726246416275}\n\nevent: rag\ndata: {\"token\":\"Summary\",\"ts\":1726246417457,\"took\":1178,\"last\":false}\n\nevent: rag\ndata: {\"token\":\":\",\"ts\":1726246417469,\"took\":12,\"last\":false}\n\nevent: rag\ndata: {\"token\":\" Searches\",\"ts\":1726246417494,\"took\":24,\"last\":false}\n\nevent: rag\ndata: {\"token\":\" for\",\"ts\":1726246417511,\"took\":18,\"last\":false}\n\nevent: rag\ndata: {\"token\":\" '\",\"ts\":1726246417526,\"took\":15,\"last\":false}\n\nevent: rag\ndata: {\"token\":\"matrix\",\"ts\":1726246417543,\"took\":17,\"last\":true}\n</code></pre> <p>SSE response consists of two frame types:</p> <ul> <li><code>results</code>: a regular search response as for non-streaming requests</li> <li><code>rag</code>: a sequence of live generated per-token events.</li> </ul>"},{"location":"features/search/rag/#results-frame","title":"<code>results</code> frame","text":"<p>A <code>results</code> frame has the following structure:</p> <pre><code>{\n  \"took\": 112,\n  \"hits\": [\n    {\n      \"_id\": \"604\",\n      \"title\": \"The Matrix Reloaded\",\n      \"_score\": 0.016666668\n    },\n    {\n      \"_id\": \"605\",\n      \"title\": \"The Matrix Revolutions\",\n      \"_score\": 0.016393442\n    }\n  ],\n  \"ts\":1722354191905\n}\n</code></pre> <p>Note</p> <p>Note that unlike in the REST response, the <code>results.response</code> field is missing from the response payload: it is going to be streamed per token with the <code>rag</code> frames!</p>"},{"location":"features/search/rag/#rag-frame","title":"<code>rag</code> frame","text":"<p>A <code>rag</code> frame is a tiny frame always following the <code>results</code> frame:</p> <pre><code>{\n  \"token\": \" Matrix\",\n  \"ts\": 1722354192184,\n  \"took\": 20,\n  \"last\": false\n}\n</code></pre> <ul> <li><code>token</code> (required, string): next generated LLM token</li> <li><code>ts</code> (required, long): generation timestamp</li> <li><code>took</code> (required, long): how many millis underlying LLM spend generating this token</li> <li><code>last</code> (required, bool): is this the last token in the response stream?</li> </ul>"},{"location":"features/search/rag/#assembling-frames-together","title":"Assembling frames together","text":"<ul> <li>The <code>results</code> frame with search results is always the first one</li> <li>If there was a <code>request.rag</code> field present in the search request, server will start streaming RAG response tokens</li> <li>When server finishes generating RAG response, it will set <code>last: true</code> flag to communicate that.</li> </ul>"},{"location":"features/search/sort/","title":"Sorting search results","text":"<p>You can sort search results based on one or more sort predicates on fields marked as sortable (e.g. having a <code>sort: true</code> option in the index mapping). You can also reverse the sorting order, sort by document <code>_score</code> and also by document indexing order using field <code>_doc</code>.</p> <p>As sorting by a specific field requires holding an in-memory DocValue data structure, by default all fields are not sortable. To make field sortable, add the <code>sort: true</code> option in the field configuration:</p> <pre><code>schema:\n  &lt;index-name&gt;:\n    fields:\n      price:\n        type: double\n        sort: true # by default all fields are non-sortable\n      created_at:\n        type: datetime\n        sort: true\n      category:\n        type: text\n        search: false\n        sort: true\n</code></pre> <p>For the index mapping above you can use the following search request to sort by field values:</p> <pre><code>curl -XPOST /&lt;index-name&gt;/_search \\\n    -d '{\n          \"query\": {\"match_all\": {}},\n          \"sort\": [\n            \"price\",\n            {\"created_at\": {\"order\": \"asc\"}},\n            {\"category\": {\"missing\": \"last\"}}\n          ]\n        }'\n</code></pre> <p>A sorting predicate can be defined in two forms:</p> <ul> <li>short: just a field name, with all the default options.   Example: <pre><code>{\n  \"sort\": [\"price\", \"color\"]\n}\n</code></pre></li> <li>full: a JSON object with non-default options: <pre><code>{\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"asc\",\n        \"missing\": \"last\"\n      }\n    }\n  ]\n}\n</code></pre></li> </ul> <p>Sorting can be done over numeric, text and geopoint fields.</p>"},{"location":"features/search/sort/#sort-order","title":"Sort order","text":"<p>For any non distance sorting predicate, the <code>order</code> option can have the following values:</p> <ul> <li><code>desc</code>: sort in descending order. Default value for <code>_score</code> field.</li> <li><code>asc</code>: sort in ascending order. Default value for other fields.</li> </ul> <p>Distance sorting can only be done in the ascending order.</p>"},{"location":"features/search/sort/#missing-values","title":"Missing values","text":"<p>In a case when a document field is not marked as <code>required: true</code> (Work-in-progress feature, see PR#482 for details), the position of such documents can be controlled with the <code>missing</code> option having two possible values:</p> <ul> <li><code>{\"missing\": \"last\"}</code> - the default behavior, all documents with missing field value are on the lowest position.</li> <li><code>{\"missing\": \"first\"}</code> - so all the documents without field are on the top.</li> </ul>"},{"location":"features/search/sort/#sorting-numeric-fields","title":"Sorting numeric fields","text":"<p>Nixiesearch supports sorting over all numerical field types like <code>int</code>, <code>float</code>, <code>long</code> and <code>double</code>. Other semi-numeric field types can also be sorted:</p> <ul> <li><code>date</code> field is internally stored as a number of days from Unix epoch (<code>1970-01-01</code>), so it behaves as a regular <code>int</code> field.</li> <li><code>datetime</code> field is stored as millis from the start of epoch, so is semantically equals to a <code>long</code> field.</li> <li><code>boolean</code> field is mapped to <code>1</code> and <code>0</code> and also behaves as <code>int</code> field.</li> </ul>"},{"location":"features/search/sort/#sorting-text-fields","title":"Sorting text fields","text":"<p>When a text field is marked as sorted, it is sorted lexicographically without any internal analysis and processing. The same field can be searchable and sortable, as internally these are two separate Lucene fields with different analysis pipelines:</p> <ul> <li><code>search: true</code> marked field uses a regular language specific analysis (e.g. ICU tokenization, stopwords removal, etc.)</li> <li><code>sort: true</code> field also creates an internal <code>&lt;field-name&gt;$sort</code> field with no analysis, so these two fields do not interfere.</li> </ul>"},{"location":"features/search/sort/#distance-sorting","title":"Distance sorting","text":"<p>For <code>geopoint</code> fields it is possible to sort by distance from a specific point. With the following schema:</p> <pre><code>schema:\n  &lt;index-name&gt;:\n    fields:\n      location:\n        type: geopoint\n        sort: true # by default all fields are non-sortable\n</code></pre> <p>To sort documents by distance the following request can be used:</p> <pre><code>curl -XPOST /&lt;index-name&gt;/_search \\\n    -d '{\n          \"query\": {\"match_all\": {}},\n          \"sort\": [\n            {\n              \"location\": { \n                \"lat\": 12.34,\n                \"lon\": 56.78 \n              }\n            }\n          ]\n        }'\n</code></pre> <p>Distance sorting can only be done in the <code>asc</code> order (so from starting from the closest document), and missing values are always <code>last</code>.</p>"},{"location":"features/search/query/overview/","title":"Query DSL","text":"<p>Nixiesearch has a Lucene-inspired query DSL with multiple search operators.</p> <p>Note</p> <p>To search over a field, make sure that this field is marked as searchable in index mapping.</p> <p>Unlike Elastic/OpenSearch query DSL, Nixiesearch has a distinction between search operators and filters:</p> <ul> <li>Search operators affect document relevance scores (like semantic and match)</li> <li>Filters only control how we include/exclude documents. See Filters for more details. </li> </ul>"},{"location":"features/search/query/overview/#search-request-format","title":"Search request format","text":"<p>Search request format is similar to existing Lucene-based search engines:</p> <pre><code>{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"title\", \"desc\"],\n  \"size\": 10,\n  \"aggs\": {\n    \"color_counts\": {\"term\": {\"field\": \"color\"}}\n  },\n  \"filters\": {\n    \"include\": {\"term\": {\"category\": \"pants\"}}\n  }\n}\n</code></pre> <p>Where fields are:</p> <ul> <li><code>query</code>: required, a search query operator. </li> <li><code>fields</code>: optional (default: all stored fields), which document fields to return in the response payload. Note that these fields should be marked as <code>store: true</code> in index mapping.</li> <li><code>size</code>: optional (default: 10), number of documents to return</li> <li><code>aggs</code>: optional, facet aggregations, see Facets for more examples.</li> <li><code>filters</code>: optional, include/exclude filters to select a sub-set of documents for searching.</li> </ul>"},{"location":"features/search/query/overview/#search-operators","title":"Search operators","text":"<p>Search operators allow you to actually perform the full-text search over your documents. They're designed to be fast and quickly get top-N most relevant document for your search query.</p> <p>Nixiesearch supports the following search operators:</p> <ul> <li>match: search over a single field</li> <li>multi_match: search over multiple fields at once.</li> <li>match_all: match all documents.</li> <li>semantic: embed a query and perform a-kNN vector search over document embeddings.</li> <li>knn: perform a-kNN vector search over document embeddings (without embedding the query).</li> </ul> <p>Operators can be combined into a single query:</p> <ul> <li>dis_max: search over multiple fields, but sort by the most matching field score.</li> <li>bool: combine multiple queries in a boolean expression.</li> </ul> <p>Note</p> <p>All search operators can be combined with filters to search over a subset of documents.</p>"},{"location":"features/search/query/overview/#ranking-operators","title":"Ranking operators","text":"<p>Rank operators accept one or more search operators but only operate on top-N of them.</p> <p>Nixiesearch supports the following list of rank operators:</p> <ul> <li>RRF: Reciprocal Rank Fusion, merge two search results lists based on document position.</li> <li>Cross-Encoder: Neural reranking model that jointly processes query-document pairs for more accurate relevance scoring.</li> </ul>"},{"location":"features/search/query/rank/ce/","title":"Cross-Encoder Reranking","text":"<p>Cross-encoder models are powerful neural ranking models that can significantly improve search relevance by reranking initial search results. Unlike bi-encoder models that encode queries and documents separately, cross-encoders process query-document pairs jointly, enabling more sophisticated relevance scoring. </p> <p>Cross-encoders complement other ranking methods like RRF (Reciprocal Rank Fusion) and work as part of Nixiesearch's comprehensive search query system. See sentence-transformers Cross-Encoder docs for more details on the underlying technology.</p>"},{"location":"features/search/query/rank/ce/#how-cross-encoders-work","title":"How Cross-Encoders Work","text":"<p>Cross-encoders take a query and document as input, concatenate them, and output a relevance score. This joint processing allows the model to understand complex relationships between query terms and document content, leading to more accurate relevance judgments.</p> <p>The typical workflow is:</p> <ol> <li>Initial Retrieval: Use a fast retrieval method (semantic, lexical, or hybrid) to get candidate documents</li> <li>Reranking: Apply the cross-encoder to score and rerank the top candidates</li> <li>Final Results: Return the reranked documents with improved relevance ordering</li> </ol>"},{"location":"features/search/query/rank/ce/#configuration","title":"Configuration","text":""},{"location":"features/search/query/rank/ce/#model-setup","title":"Model Setup","text":"<p>Configure cross-encoder models in your configuration file under the <code>inference.ranker</code> section:</p> <pre><code>inference:\n  ranker:\n    ce_model:\n      provider: onnx\n      model: cross-encoder/ms-marco-MiniLM-L6-v2\n      max_tokens: 512\n      batch_size: 32\n      device: cpu\n</code></pre> <p>Configuration Options:</p> <ul> <li><code>provider</code>: Model provider (currently only <code>onnx</code> supported)</li> <li><code>model</code>: HuggingFace model identifier or local path</li> <li><code>max_tokens</code>: Maximum sequence length (default: 512)</li> <li><code>batch_size</code>: Inference batch size (default: 32)</li> <li><code>device</code>: Processing device (<code>cpu</code> or <code>gpu</code>) - see inference overview for hardware requirements</li> <li><code>file</code>: Optional path to custom ONNX model file</li> </ul> <p>Advanced Configuration:</p> <p>Some reranker models like Qwen3-Reranker use template-based prompting with special tokens instead of simple query-document concatenation. Nixiesearch automatically detects model-specific configurations:</p> <ul> <li><code>padding_side</code>: Tokenizer padding direction (<code>left</code> or <code>right</code>). Auto-detected for known models.</li> <li><code>prompt</code>: Custom prompt template with placeholders for <code>{query}</code> and <code>{document}</code>. Auto-detected for Qwen3-based models.</li> <li><code>logits_processor</code>: Output transformation (<code>sigmoid</code> or <code>noop</code>). Auto-detected as <code>sigmoid</code> for Qwen3 models.</li> </ul> <p>Users can override auto-detected settings if needed by explicitly setting these values in the configuration.</p> <p>Popular cross-encoder models to consider:</p> <ul> <li><code>cross-encoder/ms-marco-MiniLM-L6-v2</code>: Fast, general-purpose ranking model, English only.</li> <li><code>jinaai/jina-reranker-v2-base-multilingual</code>: Slower, but much more precise multilingual ranker.</li> <li><code>zhiqing/Qwen3-Reranker-0.6B-seq-cls-ONNX</code>: Fast, efficient decoder-based reranker with automatic prompt template detection.</li> </ul> <p>Nixiesearch supports any sentence-transformer cross-encoder models in ONNX format. See the Speeding up Inference &gt; ONNX section of SBERT docs for more details on how to convert your own model.</p>"},{"location":"features/search/query/rank/ce/#query-syntax","title":"Query Syntax","text":"<p>Basic Cross-Encoder query:</p> <pre><code>{\n  \"query\": {\n    \"cross_encoder\": {\n      \"model\": \"ce_model\",\n      \"query\": \"artificial intelligence applications\",\n      \"doc_template\": \"{{ title }} {{ description }}\",\n      \"retrieve\": {\n        \"semantic\": {\n          \"title\": \"AI machine learning\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code>: Reference to configured cross-encoder model</li> <li><code>query</code>: Query text to compare against documents</li> <li><code>doc_template</code>: Jinja template for rendering document content</li> <li><code>retrieve</code>: Initial retrieval query (can be any query type - semantic, match, bool, etc.)</li> <li><code>rank_window_size</code>: Number of documents to retrieve before reranking (optional)</li> </ul> <p>Note</p> <p>Important: The <code>query</code> parameter requires explicit text that represents the user's search intent. While the <code>retrieve</code> query can be any query type (including wildcards, filters, or complex boolean queries), the cross-encoder needs a clear text representation of what the user is looking for. This text query is usually a copy of the main search terms from your retrieval query, but cannot always be extracted automatically - especially when using non-textual queries like category filters or wildcards.</p>"},{"location":"features/search/query/rank/ce/#document-templates","title":"Document Templates","text":"<p>Document templates use Jinja syntax to combine multiple document fields into the text passed to the cross-encoder:</p> <p>Simple template:</p> <pre><code>{\n  \"doc_template\": \"{{ title }}\"\n}\n</code></pre> <p>Multi-field template:</p> <pre><code>{\n  \"doc_template\": \"Title: {{ title }}\\nDescription: {{ description }}\\nCategories: {{ categories }}\"\n}\n</code></pre> <p>Conditional template:</p> <pre><code>{\n  \"doc_template\": \"{{ title }}{% if description %} - {{ description }}{% endif %}\"\n}\n</code></pre> <p>The system automatically extracts required fields from your template based on your index mapping, so you only need to specify the template without listing fields separately.</p>"},{"location":"features/search/query/rank/ce/#examples","title":"Examples","text":""},{"location":"features/search/query/rank/ce/#e-commerce-product-search","title":"E-commerce Product Search","text":"<p>This example uses multi-match queries for initial retrieval:</p> <pre><code>{\n  \"query\": {\n    \"cross_encoder\": {\n      \"model\": \"ce_model\",\n      \"query\": \"wireless bluetooth headphones\",\n      \"doc_template\": \"{{ title }} {{ brand }} {{ description }}\",\n      \"rank_window_size\": 50,\n      \"retrieve\": {\n        \"multi_match\": {\n          \"query\": \"wireless bluetooth headphones\",\n          \"fields\": [\"title^2\", \"description\", \"brand\"]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/query/rank/ce/#knowledge-base-search","title":"Knowledge Base Search","text":"<p>This example uses semantic search for initial retrieval:</p> <pre><code>{\n  \"query\": {\n    \"cross_encoder\": {\n      \"model\": \"ce_model\",\n      \"query\": \"how to configure SSL certificates\",\n      \"doc_template\": \"{{ title }}\\n{{ content }}\",\n      \"retrieve\": {\n        \"semantic\": {\n          \"content\": \"SSL certificate configuration setup\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/query/rank/ce/#hybrid-retrieval-with-cross-encoder-reranking","title":"Hybrid Retrieval with Cross-Encoder Reranking","text":"<p>This example combines semantic and lexical search using RRF (Reciprocal Rank Fusion):</p> <pre><code>{\n  \"query\": {\n    \"cross_encoder\": {\n      \"model\": \"ce_model\",\n      \"query\": \"machine learning model deployment\",\n      \"doc_template\": \"{{ title }} {{ abstract }}\",\n      \"retrieve\": {\n        \"rrf\": {\n          \"queries\": [\n            {\"semantic\": {\"abstract\": \"ML model deployment\"}},\n            {\"match\": {\"title\": \"machine learning deployment\"}}\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>Cross-encoders expect a single, unified set of documents for reranking. For hybrid search scenarios where you want to combine results from multiple retrieval methods (lexical and semantic), you must first merge them using techniques like RRF or disjunction max before applying cross-encoder reranking.</p>"},{"location":"features/search/query/rank/ce/#performance-considerations","title":"Performance Considerations","text":""},{"location":"features/search/query/rank/ce/#window-size-optimization","title":"Window Size Optimization","text":"<p>Use <code>rank_window_size</code> to balance relevance and performance:</p> <pre><code>{\n  \"query\": {\n    \"cross_encoder\": {\n      \"model\": \"ce_model\",\n      \"query\": \"search query\",\n      \"doc_template\": \"{{ title }}\",\n      \"rank_window_size\": 100,\n      \"retrieve\": {\n        \"semantic\": {\"title\": \"initial query\"}\n      }\n    }\n  }\n}\n</code></pre> <ul> <li>Small window (20-50): Faster inference, may miss relevant documents</li> <li>Medium window (50-100): Good balance for most use cases</li> <li>Large window (100+): Better recall, slower performance</li> </ul>"},{"location":"features/search/query/rank/ce/#batch-size-tuning","title":"Batch Size Tuning","text":"<p>Configure batch size based on your hardware:</p> <ul> <li>CPU: 8-32 documents per batch</li> <li>GPU: 32-128 documents per batch</li> <li>Memory-constrained: Reduce batch size if getting OOM errors</li> </ul>"},{"location":"features/search/query/rank/ce/#best-practices","title":"Best Practices","text":"<ol> <li>Template Design: Include the most relevant fields that help distinguish document relevance</li> <li>Initial Retrieval: Use efficient retrieval methods (semantic/lexical) to get good candidates</li> <li>Window Sizing: Start with 50-100 documents and adjust based on performance needs</li> <li>Model Selection: Choose models appropriate for your domain (general vs. specialized)</li> <li>Performance: Cross-encoder inference is expensive; use appropriate <code>rank_window_size</code> and <code>batch_size</code> for your hardware</li> </ol>"},{"location":"features/search/query/rank/ce/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"features/search/query/rank/ce/#with-filters","title":"With Filters","text":"<p>Cross-encoder reranking works seamlessly with search filters to first narrow down results before reranking:</p> <pre><code>{\n  \"query\": {\n    \"cross_encoder\": {\n      \"model\": \"ce_model\",\n      \"query\": \"laptop gaming\",\n      \"doc_template\": \"{{ title }} {{ specs }}\",\n      \"retrieve\": {\n        \"match\": {\"title\": \"laptop\"}\n      }\n    }\n  },\n  \"filter\": {\n    \"term\": {\"category\": \"electronics\"}\n  }\n}\n</code></pre>"},{"location":"features/search/query/rank/ce/#with-aggregations","title":"With Aggregations","text":"<p>Cross-encoder reranking works seamlessly with facets and aggregations, as aggregations are computed on the initial retrieval results before reranking. This allows you to get both relevant reranked results and accurate facet counts.</p>"},{"location":"features/search/query/rank/ce/#with-rag-retrieval-augmented-generation","title":"With RAG (Retrieval-Augmented Generation)","text":"<p>Cross-encoders are particularly useful in RAG pipelines where high-quality document ranking directly impacts the quality of generated responses.</p>"},{"location":"features/search/query/rank/rrf/","title":"RRF ranker","text":"<p>Reciprocal Rank Fusion (RRF) is a simple method to combine multiple search results with different search score numerical distributions into a single list.</p> <p>The main benefit of RRF is that it's lightweight and requires no tuning, but might provide less relevant results compared to other more computationally intensive ranking methods like Learn-to-Rank and Cross-Encoders.</p> <p>The <code>rrf</code> rank operator takes two or more child sub-queries:</p> <pre><code>{\n  \"query\": {\n    \"rrf\": {\n      \"retrieve\": [\n        {\"match\": {\"title\": \"cookie\"}},\n        {\"semantic\": {\"title\": \"cookie\"}}\n      ],\n      \"k\": 60.0,\n      \"rank_window_size\": 20\n    }\n  }\n}\n</code></pre> <p>And combines their score in the following way (taken from Elasticsearch docs):</p> <pre><code>score = 0.0\nfor q in queries:\n    if d in result(q):\n        score += 1.0 / ( k + rank( result(q), d ) )\nreturn score\n\n# where\n# k is a ranking constant\n# q is a query in the set of queries\n# d is a document in the result set of q\n# result(q) is the result set of q\n# rank( result(q), d ) is d's rank within the result(q) starting from 1\n</code></pre> <p>Fields:</p> <ul> <li><code>retrieve</code> (required, list of search queries). Two or more nested search queries to combine.</li> <li><code>k</code> (optional, float, default 60.0). The ranking constant - how strongly lower document position affects the score.</li> <li><code>rank_window_size</code> (optional, integer, default is <code>request.size</code>) This value determines the size of the individual result sets per query. A higher value will improve result relevance at the cost of performance. The final ranked result set is pruned down to the search request\u2019s size. </li> </ul> <p>The RRF ranker supports filters and aggregations, but does not support sorting.</p>"},{"location":"features/search/query/retrieve/bool/","title":"bool query","text":"<p>The <code>bool</code> query can be used to combine multiple child queries into a single search expression. The request schema is:</p> <pre><code>{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\"list of sub-queries\"],\n      \"must\": [\"list of sub-queries\"],\n      \"must_not\": [\"list of sub-queries\"]\n    }\n  }\n}\n</code></pre> <ul> <li><code>should</code> queries are the best effort ones, but at least once of them must match the documents. The more sub-queries match, the higher the final document <code>_score</code> is.</li> <li><code>must</code> queries are required, so all of them have to match for a document to be included in results.</li> <li><code>must_not</code> queries are required NOT to match. All of them should NOT match for a document to be matched.</li> </ul> <p>The <code>multi_match</code> query for <code>most_fields</code> is implemented as a <code>bool</code> query.</p> <p>Example:</p> <pre><code>{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\"match\": {\"title_english\": \"cookies\"}},\n        {\"match\": {\"title_spanish\": \"cookies\"}}\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/query/retrieve/dis_max/","title":"Disjunction-max query","text":"<p>Returns documents matching one or more wrapped queries.</p> <p>If a returned document matches multiple query clauses, the <code>dis_max</code> query assigns the document the highest relevance score from any matching clause, plus a tie breaking increment for any additional matching subqueries.</p>"},{"location":"features/search/query/retrieve/dis_max/#example-request","title":"Example request","text":"<pre><code>{\n  \"query\": {\n    \"dis_max\": {\n      \"queries\": [\n        {\"match\": {\"title\": \"cookies\"}},\n        {\"match\": {\"description\": \"cookies\"}}\n      ],\n      \"tie_breaker\": 0.3\n    }\n  }\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>queries</code>: required, a list of queries. Returned documents must match one or more of these sub-queries. If the doc matches multiple sub-queries, then the highest score is used.</li> <li><code>tie_breaker</code>: optional, float. A number between <code>0.0</code> and <code>1.0</code> used to increase scores of docs matching multiple queries at once.</li> </ul>"},{"location":"features/search/query/retrieve/dis_max/#the-tie-breaker","title":"The tie breaker","text":"<p>The <code>tie_breaker</code> parameter allows you to prioritize documents where the same keyword appears across several fields, making them score higher than documents where the keyword is present in only the most relevant single field. This distinction ensures that documents with multiple field matches for the same term are not confused with documents matching different terms across fields.</p> <p>When a document satisfies more than one condition, the <code>dis_max</code> query computes its relevance score using the following steps:</p> <ol> <li> <p>Identify the clause that produced the highest individual score.</p> </li> <li> <p>Apply the <code>tie_breaker</code> coefficient to the scores of the other matching clauses.</p> </li> <li> <p>Add the highest individual score to the adjusted scores of the other clauses.</p> </li> </ol> <p>When the <code>tie_breaker</code> is set above <code>0.0</code>, all matching clauses contribute to the final score, but the clause with the highest individual score remains the most influential.</p>"},{"location":"features/search/query/retrieve/knn/","title":"knn query","text":"<p>A <code>knn</code> search query can be used for searching over <code>text</code> fields defined as searchable with semantic search with a pre-computed embedding.</p> <p>Note: The <code>knn</code> query works with both server-side inference (using the <code>model</code> parameter) and pre-embedded documents (using the <code>dim</code> parameter). For text-based queries that require server-side embedding computation, use the <code>semantic</code> query instead.</p> <p>Unlike <code>semantic</code> query, the <code>knn</code> query does NOT run embedding inference, and expects the query embedding provided in the request:</p> <pre><code>{\n  \"query\": {\n    \"knn\": {\n      \"field\": \"title\",\n      \"query_vector\": [1,2,3,4,5],\n      \"k\": 10,\n      \"num_candidates\": 15\n    }\n  }\n}\n</code></pre> <p>Fields: * <code>field</code>: a <code>text</code> or <code>text[]</code> field name with semantic search enabled in the index mapping. * <code>query_vector</code>: a text query embedding. * <code>k</code>: an optional parameter of how many neighbor documents to fetch. By default, equals to the <code>request.size</code> field. * <code>num_candidates</code>: an optional parameter for the number of nearest neighbor candidates to consider per shard while doing knn search. Cannot exceed 10,000. Increasing num_candidates tends to improve the accuracy of the final results. Defaults to 1.5 * k if k is set, or 1.5 * size if k is not set.</p> <p>Note: For <code>text[]</code> fields, the query uses multi-vector search where the highest-scoring item in the array determines the document score. See text field types for details.</p> <p>For a case when you would like Nixiesearch to embed the query, see the <code>semantic</code> query.</p>"},{"location":"features/search/query/retrieve/match/","title":"match","text":"<p>A match query performs a lexical search over a searchable text field. The search query is analyzed before performing search.</p> <p>If you would like to search over a semantic-indexed field, consider a <code>semantic</code> and <code>knn</code> search operators instead.</p> <p>Match query can be written in two JSON formats. A full version:</p> <p><pre><code>{\n  \"query\": {\n    \"match\": {\n      \"&lt;field-name&gt;\": {\n        \"query\": \"&lt;search-query&gt;\",\n        \"operator\": \"or\"\n      }\n    }\n  }\n}\n</code></pre> Or a shorter version:</p> <pre><code>{\n  \"query\": {\n    \"match\": {\n      \"&lt;field-name&gt;\": \"&lt;search-query&gt;\"\n    }\n  }\n}\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;field-name&gt;</code>: is an existing field marked as searchable with lexical search support enabled.</li> <li><code>&lt;search-query&gt;</code>: a search query string.</li> <li><code>operator</code>: optional, possible values: <code>\"and\"</code>, <code>\"or\"</code>. Default is \"or\". For lexical search, should documents contain all or some of the terms from the search query. For semantic search this parameter is ignored.</li> </ul>"},{"location":"features/search/query/retrieve/match_all/","title":"match_all query","text":"<p>The most simple query, which matches all documents, giving them all a _score of <code>1.0</code>.</p> <pre><code>{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n</code></pre> <p>The <code>match_all</code> query has no parameters.</p>"},{"location":"features/search/query/retrieve/multi_match/","title":"multi_match","text":"<p>A <code>multi_match</code> query performs a lexical search over multiple  searchable text fields. The search query is analyzed before performing search.</p> <p>If you would like to search over a semantic-indexed field, consider a <code>semantic</code> and <code>knn</code> search operators instead.</p> <p>An operator similar to match but able to search multiple fields at once:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"&lt;field-name&gt;\", \"&lt;field-name&gt;\"],\n      \"query\": \"&lt;search-query&gt;\",\n      \"type\": \"best_fields\" //(1) \n    }\n  }\n}\n</code></pre> <ol> <li>optional</li> </ol> <p>Where:</p> <ul> <li><code>&lt;field-name&gt;</code>: is an existing field marked as searchable.</li> <li><code>&lt;search-query&gt;</code>: a search query string.</li> <li><code>type</code>: optional, possible values: <code>\"best_fields\"</code>, <code>\"most_fields\"</code>, default <code>\"best_fields\"</code>. The way field scores are combined when multiple of them are matched at once. </li> </ul>"},{"location":"features/search/query/retrieve/multi_match/#wildcard-fields","title":"Wildcard fields","text":"<p><code>multi_match</code> query also supports wildcard fields, so you can make requests like this example:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"desc_*\"],\n      \"query\": \"cookies\" \n    }\n  }\n}\n</code></pre> <p>This query will perform a <code>multi_match</code> over all fields starting with <code>desc_</code> prefix.</p>"},{"location":"features/search/query/retrieve/multi_match/#types-of-multi_match-queries","title":"Types of <code>multi_match</code> queries","text":"<p>The <code>multi_match</code> query has multiple behaviors depending on the <code>type</code> parameter:</p> <ol> <li><code>best_fields</code> (default): Finds all documents which match any field, but only the <code>_score</code> from the best matching field is used.</li> <li><code>most_fields</code>: Finds all documents which match any field, and combines the <code>_score</code> from each field.</li> </ol>"},{"location":"features/search/query/retrieve/multi_match/#best_fields","title":"best_fields","text":"<p>An example for <code>best_fields</code> query is searching over <code>title</code> and <code>description</code> fields, where a match over a <code>title</code> field might be more important than match over the <code>desription</code> field.</p> <p>The full schema of the query:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"title\", \"description\"],\n      \"query\": \"cookies\",\n      \"type\": \"best_fields\",\n      \"tie_breaker\": 0.3\n    }\n  }\n}\n</code></pre> <p>The <code>best_fields</code> query is an alias for a <code>dis_max</code> query, which generates multiple per-field <code>match</code> queries and wraps them the following way:</p> <pre><code>{\n  \"query\": {\n    \"dis_max\": {\n      \"queries\": [\n        {\"match\": {\"title\": \"cookies\"}},\n        {\"match\": {\"description\": \"cookies\"}}\n      ],\n      \"tie_breaker\": 0.3\n    }\n  }\n}\n</code></pre> <p>Normally the <code>best_fields</code> type uses the score of the single best matching field, but if tie_breaker is specified, then it calculates the score as follows:</p> <ol> <li>the score from the best matching field</li> <li>plus <code>tie_breaker * _score</code> for all other matching fields</li> </ol>"},{"location":"features/search/query/retrieve/multi_match/#most_fields","title":"most_fields","text":"<p>The <code>most_fields</code> query type is useful when searching over similar fields analyzed in a different manner, for example in different languages:</p> <pre><code>schema:\n  my-index:\n    fields:\n      title_english:\n        type: text\n        search:\n          lexical:\n            analyze: english\n      title_spanish:\n        type: text\n        search:\n          lexical:\n            analyze: spanish\n</code></pre> <p>So the following query can be used:</p> <pre><code>{\n  \"query\": {\n    \"multi_match\": {\n      \"fields\": [\"title_english\", \"title_spanish\"],\n      \"query\": \"cookies\",\n      \"type\": \"most_fields\"\n    }\n  }\n}\n</code></pre> <p>Internally the <code>most_fields</code> query is implemented as a <code>bool</code> query over multiple per-field <code>match</code> queries:</p> <pre><code>{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\"match\": {\"title_english\": \"cookies\"}},\n        {\"match\": {\"title_spanish\": \"cookies\"}}\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"features/search/query/retrieve/semantic/","title":"semantic query","text":"<p>A <code>semantic</code> query can be used to search <code>text</code> fields with semantic search enabled. Unlike the <code>knn</code> query, the <code>semantic</code> query accepts text query string and computes embeddings. </p> <p>Note: The <code>semantic</code> query only works with fields that have server-side inference configured (i.e., using the <code>model</code> parameter). For pre-embedded documents (using the <code>dim</code> parameter), use the <code>knn</code> query instead.</p> <p>So for a field <code>title</code> defined as:</p> <pre><code>inference:\n  embedding:\n    e5-small:\n      model: intfloat/e5-small-v2 # perform local ONNX inference\nschema:\n  my-index:\n    fields:\n      title:\n        type: text\n        search:\n          semantic:           # build an a-kNN HNSW index\n            model: e5-small   # use this model for embeddings\n</code></pre> <p>Such a field can be searched with the <code>semantic</code> query:</p> <pre><code>{\n  \"query\": {\n    \"semantic\": {\n      \"field\": \"title\",\n      \"query\": \"cookies\",\n      \"k\": 20,\n      \"num_candidates\": 30\n    }\n  }\n}\n</code></pre> <p>Or with a shorter form:</p> <pre><code>{\n  \"query\": {\n    \"semantic\": {\"title\": \"cookies\"}\n  }\n}\n</code></pre> <p>Where the fields are:</p> <ul> <li><code>field</code>: a <code>text</code> or <code>text[]</code> field with semantic search enabled in the index mapping.</li> <li><code>query</code>: a text query. The query is not analyzed and fed into the embedding model as-is.</li> <li><code>k</code>: an optional parameter of how many neighbor documents to fetch. By default, equals to the <code>request.size</code> field.</li> <li><code>num_candidates</code>: an optional parameter for the number of nearest neighbor candidates to consider per shard while doing knn search. Cannot exceed 10,000. Increasing num_candidates tends to improve the accuracy of the final results. Defaults to 1.5 * k if k is set, or 1.5 * size if k is not set.</li> </ul> <p>Note: For <code>text[]</code> fields, the query uses multi-vector search where the highest-scoring item in the array determines the document score. See text field types for details.</p> <p>For a case when you already have a pre-embedded query and want to search over the embedding vector directly skipping the inference, see the <code>knn</code> query.</p>"},{"location":"features/search/query/retrieve/semantic/#improving-semantic-search-relevance","title":"Improving Semantic Search Relevance","text":"<p>For improved relevance scoring beyond vector similarity, consider using cross-encoder reranking to rerank semantic search results with neural models that jointly process query-document pairs.</p>"},{"location":"help/contact/","title":"Contact us","text":"<p>If you want to reach Nixiesearch developers (for example, to get us a beer), you can use following methods:</p> <ul> <li>To babble about search: official community Slack chat</li> <li>To babble about search, but with video: you can book a call on Calendly</li> <li>To report an issue or a bug: Github Issues @ nixiesearch/nixiesearch</li> <li>Other topics: send an email to <code>roman@nixiesearch.ai</code></li> </ul> <p>Remember that this is an open-source non-commercial community project, and all these channels have a best-effort guarantee of response.</p>"},{"location":"help/support/","title":"Support","text":""},{"location":"help/support/#open-source-support","title":"Open-source support","text":"<p>If you have an issue with Nixiesearch, is stuck, or need help:</p> <ul> <li>Join our community Slack chat to discuss.</li> <li>Report bugs at GitHub issues @ nixiesearch/nixiesearch.</li> <li>For general questions, use GitHub Discussions @ nixiesearch/nixiesearch.</li> </ul>"},{"location":"help/support/#enterprise-support","title":"Enterprise support","text":"<p>If you would like to get an enterprise commercial support for Nixiesearch, please reach us directly - book a call using Calendly or send an email to <code>roman@nixiesearch.ai</code>! </p>"},{"location":"help/usage_stats/","title":"Anonymous Usage Statistics","text":"<p>The Nixiesearch open-source container image collects anonymous usage statistics to help us improve the engine. You can turn this off anytime, and if you'd like us to delete any data that's already been collected, just let us know.</p>"},{"location":"help/usage_stats/#why-do-we-even-collect-usage-data","title":"Why do we even collect usage data?","text":"<p>Our goal is to make Nixiesearch as fast, reliable, and efficient as possible. While we do a ton of testing on our side, there\u2019s no substitute for real-world data. Everyone runs Nixiesearch differently\u2014with different hardware, configurations, and workloads\u2014so your usage helps us spot edge cases, performance bottlenecks, and bugs that we might never catch on our own.</p> <p>We also use internal heuristics to fine-tune performance, and telemetry helps us understand how well those optimizations are working. In short: this data helps us make Nixiesearch better for everyone.</p>"},{"location":"help/usage_stats/#what-gets-collected","title":"What gets collected","text":"<p>Here\u2019s what we do collect:</p> <ul> <li>System Info \u2013 CPU type, RAM size, and how your Nixiesearch instance is configured.</li> <li>Performance Metrics \u2013 Timings and counters from key parts of the codebase.</li> <li>Critical Error Reports \u2013 Stack traces and details about serious crashes or bugs that haven\u2019t been reported yet.</li> </ul> <p>Here\u2019s what we don\u2019t collect:</p> <ul> <li>Your IP address and your location</li> <li>Any identifying info about you or your organization</li> <li>The actual data stored in your indexes</li> <li>Names, fields and URLs in the configuration</li> </ul>"},{"location":"help/usage_stats/#how-we-anonymize-data","title":"How We Anonymize Data","text":"<p>We take your privacy seriously. Here\u2019s how we make sure collected data stays anonymous:</p> <ul> <li>We hash all names, so things like field and index names are turned into random-looking strings.</li> <li>URLs are hashed too: nothing identifiable is stored.</li> </ul> <p>An example of usage telemetry payload:</p> <pre><code>{\n  \"config\": {\n    \"inference\": {\n      \"embedding\": {\n        \"ca1930b673f7fa40deb02c0f42401488\": {\n          \"model\": \"nixiesearch/e5-small-v2-onnx\",\n          \"pooling\": \"mean\",\n          \"prompt\": {\n            \"doc\": \"passage: \",\n            \"query\": \"query: \"\n          },\n          \"file\": null,\n          \"normalize\": true,\n          \"maxTokens\": 512,\n          \"batchSize\": 32,\n          \"cache\": {\n            \"inmem\": {\n              \"maxSize\": 32768\n            }\n          },\n          \"provider\": \"onnx\"\n        }\n      },\n      \"completion\": {}\n    },\n    \"searcher\": {},\n    \"indexer\": {},\n    \"core\": {\n      \"cache\": {\n        \"dir\": \"353eb9e18184df37da5dc0222f2a1b2f\"\n      },\n      \"host\": \"0.0.0.0\",\n      \"port\": 8080,\n      \"loglevel\": \"info\"\n    },\n    \"schema\": {\n      \"55ba44c548d3ebafd9f70e64a7f232b0\": {\n        \"name\": \"55ba44c548d3ebafd9f70e64a7f232b0\",\n        \"alias\": [],\n        \"config\": {\n          \"mapping\": {\n            \"dynamic\": false\n          },\n          \"flush\": {\n            \"interval\": \"5s\"\n          },\n          \"hnsw\": {\n            \"m\": 16,\n            \"efc\": 100,\n            \"workers\": 16\n          }\n        },\n        \"store\": {\n          \"type\": \"local\",\n          \"local\": {\n            \"type\": \"disk\",\n            \"path\": \"55ba44c548d3ebafd9f70e64a7f232b0\"\n          }\n        },\n        \"fields\": {\n          \"84cdc76cabf41bd7c961f6ab12f117d8\": {\n            \"type\": \"int\",\n            \"name\": \"84cdc76cabf41bd7c961f6ab12f117d8\",\n            \"store\": true,\n            \"sort\": false,\n            \"facet\": true,\n            \"filter\": true\n          }\n        }\n      }\n    }\n  },\n  \"system\": {\n    \"os\": \"Linux\",\n    \"arch\": \"amd64\",\n    \"jvm\": \"21\",\n    \"args\": \"-Xmx1g -verbose:gc --add-modules=jdk.incubator.vector -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false -XX:-OmitStackTraceInFastThrow -Dfile.encoding=UTF-8 -Dsun.stdout.encoding=UTF-8 -Dsun.stderr.encoding=UTF-8\"\n  },\n  \"confHash\": \"59ecb6ff79f8b2309216cef88226960d\",\n  \"macHash\": \"84abbc15cdc0f5389cc24b2a56fd267313ca81a618173c8f36e0fcef4c9b68a2\",\n  \"version\": \"0.4.1\",\n  \"mode\": \"standalone\"\n}\n</code></pre> <p>Want to see exactly what we collect? You can! Just hit the telemetry API like this:</p> <pre><code>curl -XGET http://localhost:8080/v1/system/telemetry\n</code></pre>"},{"location":"help/usage_stats/#how-to-disable-telemetry","title":"How to disable telemetry","text":"<p>You can turn off usage data collection in a few ways:</p> <ul> <li>Set the <code>NIXIESEARCH_CORE_TELEMETRY</code> environment variable to <code>false</code></li> <li>Set <code>core.telemetry: false</code> in your config file</li> </ul> <p>Any of these will stop Nixiesearch from sending telemetry. If you do disable it, we\u2019d love to hear your thoughts on why - Feel free to share feedback in our slack!</p>"},{"location":"help/usage_stats/#want-your-data-deleted","title":"Want Your Data Deleted?","text":"<p>No problem - just contact us at with the unique identifier for your Nixiesearch instance. You\u2019ll find that ID in the telemetry API response (look for the \"macHash\" field).</p> <p>You can also use that address for any other questions or concerns about the data we collect.</p>"},{"location":"reference/cache/","title":"Caching","text":""},{"location":"reference/cache/#in-memory-embedding-cache","title":"In-memory embedding cache","text":"<p>In practice many documents you ingest into index share a lot of common strings, like category names and colors. A common way to improve indexing throughput is to skip computing embeddings for common strings and instead just cache them.</p> <p>Nixiesearch has an in-memory LRU cache for common embeddings, which can be configured as follows:</p> <pre><code>schema:\n  my-index-name:\n    fields:\n      # fields here\n    cache:\n      embedding:\n        maxSize: 32768\n</code></pre> <p>The whole <code>cache</code> and <code>cache.embedding</code> sections of config file are optional.</p> <p>Where:</p> <ul> <li><code>cache.embedding.maxSize</code>: integer, optional, default=32768. Maximal number of entries in embedding LRU cache.</li> </ul> <p>A ballpark estimation of cache RAM usage:</p> <ul> <li>single embedding: <code>&lt;dimensions&gt; * 4 bytes</code>. Typical dimensions are <code>384</code> for MiniLM-L6/E5-small, and <code>768</code> for larger models.</li> <li>total usage: <code>&lt;maxSize&gt; * &lt;embedding size&gt;</code></li> </ul> <p>For example, a default <code>E5-small</code> embedding model with <code>32768</code> default cache size will take: <code>384 dims * 4 bytes * 32768 entries</code> = <code>50Mb</code> of heap RAM.</p>"},{"location":"reference/config/","title":"Config file","text":""},{"location":"reference/config/#core-config","title":"Core config","text":"<p>Main server-related settings are stored here:</p> <pre><code>core:\n  host: 0.0.0.0 # optional, default=0.0.0.0\n  port: 8080 # optional, default=8080\n  loglevel: info # optional, default info\n  telemetry: true # optional, default true\n  cache:\n    dir: ./cache # optional, default=./cache\n</code></pre>"},{"location":"reference/config/#environment-variables-overrides","title":"Environment variables overrides","text":"<p>Core config settings can be overridden with env variables:</p> <ul> <li><code>NIXIESEARCH_CORE_HOST</code>: overrides <code>core.host</code> </li> <li><code>NIXIESEARCH_CORE_PORT</code>: overrides <code>core.port</code></li> <li><code>NIXIESEARCH_CORE_LOGLEVEL</code>: overrides <code>core.loglevel</code></li> <li><code>NIXIESEARCH_CORE_TELEMETRY</code>: overrides <code>core.telemetry</code></li> <li><code>NIXIESEARCH_CORE_CACHE_DIR</code>: overrides <code>core.cache.dir</code></li> </ul> <p>Loglevel can also be set from the command-line flags. Env overrides always have higher priority than config values.</p>"},{"location":"reference/config/#telemetry-configuration","title":"Telemetry configuration","text":"<p>You can opt-out of anonymous usage telemetry collection by setting the <code>core.telemetry: false</code> option:</p> <pre><code>core:\n  telemetry: false\n</code></pre> <p>This is possible to also have a fine-grained control over telemetry parts from a config file (like error stack traces and performance metrics). But currently we only collect usage telemetry:</p> <pre><code>core:\n  telemetry:\n    usage: false\n</code></pre>"},{"location":"reference/config/#searcher-config","title":"Searcher config","text":"<p>The <code>searcher</code> section is currently reserved for future searcher-specific settings:</p> <pre><code>searcher: {}\n</code></pre> <p>This section is currently empty but part of the configuration schema for future extensibility.</p>"},{"location":"reference/config/#index-mapping","title":"Index mapping","text":"<p>You can define each index in the <code>schema</code> block of the configuration:</p> <p><pre><code>schema:\n  &lt;your-index-name&gt;:\n    alias: &lt;list of aliases&gt;\n    config:\n      &lt;index configuration&gt;\n    store:\n      &lt;store configuration&gt;\n    fields:\n      &lt;field definitions&gt;\n</code></pre> !! note     The index name is immutable, so choose it wisely. But you can always add an alias to address it using a new name.</p>"},{"location":"reference/config/#index-configuration","title":"Index configuration","text":"<p>An example of index configuration:</p> <pre><code>schema:\n  index-name:\n    config:\n      indexer:\n        ram_buffer_size: 512mb\n        flush:\n          interval: 5s # how frequently new segments are created\n      hnsw:\n        m: 16 # max number of node-node links in HNSW graph\n        efc: 100 # beam width used while building the index\n        workers: 8 # how many concurrent workers used for HNSW merge ops\n</code></pre> <p>Fields:</p> <ul> <li><code>indexer.flush.interval</code>: optional, duration, default <code>5s</code>. Index writer will periodically produce flush index segments (if there are new documents) with this interval.</li> <li><code>indexer.ram_buffer_size</code>: optional, size, default <code>512mb</code>. RAM buffer size for new segments.</li> <li><code>hnsw.m</code>: optional, int, default 16. How many links should HNSW index have? Larger value means better recall, but higher memory usage and bigger index. Common values are within 16-128 range.</li> <li><code>hnsw.efc</code>: optional, int, default 100. How many neighbors in the HNSW graph are explored during indexing. Bigger the value, better the recall, but slower the indexing speed.</li> <li><code>hnsw.workers</code>: optional, int, default = number of CPU cores. How many concurrent workers to use for index merges.</li> <li><code>indexer.merge_policy</code>: optional, merge policy config, default <code>tiered</code>. Controls how Lucene merges index segments. See Merge Policies section below for details.</li> <li><code>directory</code>: optional, enum (<code>mmap</code> or <code>nio</code>), default <code>mmap</code>. Lucene Directory implementation for index storage. Use <code>nio</code> for GraalVM native images where mmap is unavailable.</li> </ul>"},{"location":"reference/config/#merge-policies","title":"Merge Policies","text":"<p>Nixiesearch allows you to configure Lucene merge policies to optimize indexing performance for your specific use case. The merge policy determines how and when index segments are merged together.</p>"},{"location":"reference/config/#tiered-merge-policy-default","title":"Tiered Merge Policy (default)","text":"<p>Best for most use cases, provides balanced performance between indexing speed and search performance.</p> <pre><code>schema:\n  your-index:\n    config:\n      indexer:\n        merge_policy: tiered\n        # or with custom settings:\n        merge_policy:\n          tiered:\n            segments_per_tier: 10\n            max_merge_at_once: 10\n            max_merged_segment_size: 5gb\n            floor_segment_size: 16mb\n            target_search_concurrency: 1\n</code></pre> <p>Parameters: * <code>segments_per_tier</code>: optional, int, default 10. Number of segments per tier in the merge hierarchy * <code>max_merge_at_once</code>: optional, int, default 10. Maximum number of segments merged at once * <code>max_merged_segment_size</code>: optional, size, default 5gb. Maximum size of merged segments * <code>floor_segment_size</code>: optional, size, default 16mb. Minimum segment size threshold * <code>target_search_concurrency</code>: optional, int, default 1. Target concurrency level for search operations</p>"},{"location":"reference/config/#byte-size-merge-policy","title":"Byte Size Merge Policy","text":"<p>Good for maintaining consistent segment sizes, useful when you want predictable storage patterns.</p> <pre><code>schema:\n  your-index:\n    config:\n      indexer:\n        merge_policy: byte_size\n        # or with custom settings:\n        merge_policy:\n          byte_size:\n            max_merge_size: 5gb\n            min_merge_size: 16mb\n            min_merge_size_for_forced_merge: 5gb\n</code></pre> <p>Parameters: * <code>max_merge_size</code>: optional, size, default 5gb. Maximum size of segments to merge * <code>min_merge_size</code>: optional, size, default 16mb. Minimum size threshold for merging segments * <code>min_merge_size_for_forced_merge</code>: optional, size, default 5gb. Minimum size threshold for forced merges</p>"},{"location":"reference/config/#document-count-merge-policy","title":"Document Count Merge Policy","text":"<p>Good when documents are of uniform size and you want to control merging based on document count rather than byte size.</p> <pre><code>schema:\n  your-index:\n    config:\n      indexer:\n        merge_policy: doc_count\n        # or with custom settings:\n        merge_policy:\n          doc_count:\n            min_merge_docs: 10\n            max_merge_docs: 2147483647\n</code></pre> <p>Parameters: * <code>min_merge_docs</code>: optional, int, default 10. Minimum number of documents in segments before merging * <code>max_merge_docs</code>: optional, int, default 2147483647. Maximum number of documents in merged segments</p>"},{"location":"reference/config/#no-merge-policy","title":"No Merge Policy","text":"<p>Disables automatic merging entirely. Useful for read-only indexes or when you want full control over merging.</p> <pre><code>schema:\n  your-index:\n    config:\n      indexer:\n        merge_policy: none\n</code></pre> <p>This policy has no additional configuration options.</p>"},{"location":"reference/config/#store-configuration","title":"Store configuration","text":"<p>TODO</p>"},{"location":"reference/config/#fields-definitions","title":"Fields definitions","text":"<p>TODO</p>"},{"location":"reference/config/#ml-inference","title":"ML Inference","text":"<p>See ML Inference overview and RAG Search for an overview of use cases for inference models.</p>"},{"location":"reference/config/#embedding-models","title":"Embedding models","text":""},{"location":"reference/config/#onnx-models","title":"ONNX models","text":"<p>Example of a full configuration:</p> <pre><code>inference:\n  embedding:\n    your-model-name:\n      provider: onnx\n      model: nixiesearch/e5-small-v2-onnx\n      file: model.onnx\n      max_tokens: 512\n      batch_size: 32\n      pooling: mean\n      normalize: true\n      cache: false\n      prompt:\n        query: \"query: \"\n        doc: \"passage: \"\n</code></pre> <p>Fields:</p> <ul> <li><code>provider</code>: optional, string, default <code>onnx</code>. As for <code>v0.3.0</code>, only the <code>onnx</code> provider is supported.</li> <li><code>model</code>: required, string. A Huggingface handle, or an HTTP/Local/S3 URL for the model. See model URL reference for more details on how to load your model. Maximum supported embedding dimensions: 8192.</li> <li><code>prompt</code>: optional. A document and query prefixes for asymmetrical models. Nixiesearch can guess the proper prompt format for the vast majority of embedding models. See the list of supported embedding models for more details.</li> <li><code>file</code>: optional, string, default is to pick a lexicographically first file. A file name of the model - useful when HF repo contains multiple versions of the same model.</li> <li><code>max_tokens</code>: optional, int, default <code>512</code>. How many tokens from the input document to process. All tokens beyond the threshold are truncated.</li> <li><code>batch_size</code>: optional, int, default <code>32</code>. Computing embeddings is a highly parallel task, and doing it in big chunks is much more effective than one by one. For CPUs there are usually no gains of batch sizes beyong 32, but on GPUs you can go up to 1024.</li> <li><code>pooling</code>: optional, <code>cls</code>/<code>mean</code>/<code>last</code>, default auto. Which pooling method use to compute sentence embeddings. This is model specific and Nixiesearch tries to guess it automatically. Options: <code>cls</code> (first token), <code>mean</code> (average pooling), <code>last</code> (last token, used by decoder models like Qwen3-Embedding). See the list of supported embeddings models to know if it can be detected automatically. If your model is not on the list, consult the model doc on its pooling method.</li> <li><code>normalize</code>: optional, bool, default <code>true</code>. Should embeddings be L2-normalized? With normalized embeddings it becomes possible to use a faster dot-product based aKNN search.</li> <li><code>cache</code>: optional, bool or CacheSettings. Default <code>memory.max_size=32768</code>. Cache top-N LRU embeddings in RAM. See Embedding caching for more details.  </li> </ul>"},{"location":"reference/config/#openai-models","title":"OpenAI models","text":"<p>Example of a full configuration:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: openai\n      model: text-embedding-3-small\n      timeout: 2000ms\n      endpoint: \"https://api.openai.com/\"\n      dimensions: null\n      batch_size: 32\n      cache: false\n</code></pre> <p>Parameters:</p> <ul> <li>timeout: optional, duration, default 2s. External APIs might be slow sometimes.</li> <li>retry: optional, string, default \"https://api.openai.com/\". You can use alternative API or EU-specific endpoint.</li> <li>dimensions: optional, int, default empty. For matryoshka models, how many dimensions to return.</li> <li>batch_size: optional, int, default 32. Batch size for calls with many documents.</li> <li>cache: optional, bool or CacheSettings. Default <code>memory.max_size=32768</code>. Cache top-N LRU embeddings in RAM. See Embedding caching for more details.</li> </ul>"},{"location":"reference/config/#cohere-models","title":"Cohere models","text":"<p>Example of a full configuration:</p> <pre><code>inference:\n  embedding:\n    &lt;model-name&gt;:\n      provider: cohere\n      model: embed-english-v3.0\n      timeout: 2000ms\n      endpoint: \"https://api.cohere.com/\"\n      batch_size: 32\n      cache: false\n</code></pre> <p>Parameters:</p> <ul> <li>timeout: optional, duration, default 2s. External APIs might be slow sometimes.</li> <li>retry: optional, string, default \"https://api.cohere.com/\". You can use alternative API or EU-specific endpoint.</li> <li>batch_size: optional, int, default 32. Batch size for calls with many documents.</li> <li>cache: optional, bool or CacheSettings. Default <code>memory.max_size=32768</code>. Cache top-N LRU embeddings in RAM. See Embedding caching for more details.</li> </ul>"},{"location":"reference/config/#embedding-caching","title":"Embedding caching","text":"<p>Each embedding model has a <code>cache</code> section, which controls embedding caching.</p> <pre><code>inference:\n  embedding:\n    e5-small:\n      model: nixiesearch/e5-small-v2-onnx\n      cache:\n        memory:\n          max_size: 32768\n</code></pre> <p>Parameters:</p> <ul> <li>cache, optional, bool or object. Default <code>memory.max_size=32768</code>. Which cache implementation to use.</li> <li>cache.memory.max_size, optional, int, default 32768. How many string-embedding pairs to keep in the LRU cache.</li> </ul> <p>Nixiesearch currently supports only <code>memory</code> embedding cache, Redis caching is planned.</p>"},{"location":"reference/config/#reranking-models","title":"Reranking models","text":"<p>Cross-encoder reranking models can be configured in the <code>inference.ranker</code> section:</p> <pre><code>inference:\n  ranker:\n    your-model-name:\n      provider: onnx\n      model: cross-encoder/ms-marco-MiniLM-L6-v2\n      max_tokens: 512\n      batch_size: 32\n      device: cpu\n      file: model.onnx\n</code></pre> <p>Fields:</p> <ul> <li><code>provider</code>: required, string. Currently only <code>onnx</code> is supported.</li> <li><code>model</code>: required, string. A Huggingface handle, or an HTTP/Local/S3 URL for the model. See model URL reference for more details on how to load your model.</li> <li><code>max_tokens</code>: optional, int, default <code>512</code>. Maximum sequence length for query-document pairs.</li> <li><code>batch_size</code>: optional, int, default <code>32</code>. Inference batch size for processing multiple query-document pairs.</li> <li><code>device</code>: optional, string, default <code>cpu</code>. Processing device (<code>cpu</code> or <code>gpu</code>).</li> <li><code>file</code>: optional, string. A file name of the model - useful when HF repo contains multiple versions of the same model.</li> <li><code>padding_side</code>: optional, <code>left</code> or <code>right</code>. Controls tokenizer padding direction. Auto-detected for known models (e.g., <code>left</code> for Qwen3-based rerankers).</li> <li><code>prompt</code>: optional, object. Custom prompt template configuration. Auto-detected for known models like Qwen3-Reranker which use template-based prompting with special system instructions.</li> <li><code>logits_processor</code>: optional, <code>sigmoid</code> or <code>noop</code>, default <code>noop</code>. Controls how raw model outputs are transformed to scores. Auto-detected as <code>sigmoid</code> for Qwen3-based models, <code>noop</code> for traditional cross-encoders.</li> </ul> <p>Example with Qwen3-Reranker (auto-detection):</p> <pre><code>inference:\n  ranker:\n    qwen-reranker:\n      provider: onnx\n      model: zhiqing/Qwen3-Reranker-0.6B-seq-cls-ONNX\n      # The following are auto-detected for Qwen3-based models:\n      # padding_side: left\n      # logits_processor: sigmoid\n      # prompt: (model-specific template)\n      max_tokens: 512\n      batch_size: 32\n      device: cpu\n</code></pre> <p>For detailed usage examples, see Cross-Encoder Reranking documentation.</p>"},{"location":"reference/config/#llm-completion-models","title":"LLM completion models","text":"<p>Example of a full configuration:</p> <pre><code>inference:\n  completion:\n    your-model-name:\n      provider: llamacpp\n      model: Qwen/Qwen2-0.5B-Instruct-GGUF\n      file: qwen2-0_5b-instruct-q4_0.gguf\n      system: \"You are a helpful assistant, answer only in haiku.\"\n      options:\n        threads: 8\n        gpu_layers: 100\n        cont_batching: true\n        flash_attn: true\n        seed: 42\n</code></pre> <p>Fields:</p> <ul> <li><code>provider</code>: required, string. As for <code>v0.3.0</code>, only <code>llamacpp</code> is supported. Other SaaS providers like OpenAI, Cohere, mxb and Google are on the roadmap.</li> <li><code>model</code>: required, string. A Huggingface handle, or an HTTP/Local/S3 URL for the model. See model URL reference for more details on how to load your model.</li> <li><code>file</code>: optional, string. A file name for the model, if the target model has multiple. A typical case for quantized models.</li> <li><code>system</code>: optional, string, default empty. An optional system prompt to be prepended to all the user prompts.</li> <li><code>options</code>: optional, obj. A set of llama-cpp specific options. See llamacpp reference on options for more details.</li> </ul>"},{"location":"reference/languages/","title":"Language support","text":"<p>Nixiesearch language support differs for lexical and semantic search methods: * for lexical search, all Lucene analyzers are supported out of the box - see full list below * for semantic search, language support depends on the underlying embedding model used. See some examples in a Language support for semantic search below.</p> <p>Language can be set in the index mapping for text-like fields:</p> <pre><code>schema:\n  your-index-name:\n    fields:\n      title:\n        type: text\n        search: \n          lexical:\n            language: en # use an ISO-639-1 language code\n</code></pre> <p>If language is not defined, a special default language analyzer is used - no language specific transformations are done, only ICU tokenization.</p>"},{"location":"reference/languages/#language-support-for-lexicalhybrid-search","title":"Language support for lexical/hybrid search","text":"<p>Nixiesearch supports all languages from Apache Lucene library:</p> Language ISO 639-1 code Generic default English en Arabic ar Bulgarian br Bengali br Brazilian Portugese br Catalan br Simplified Chinese zh Czech cz Danish da German de Greek el Spanish es Estonian et Basque eu Persian fa Finnish fi French fr Irish ga Hindi hi Hungarian hu Armenian hy Indonesian id Italian it Lithuanian lt Latvian lv Dutch nl Norwegian no Portuguese pt Romanian ro Russian ru Serbian sr Swedish sv Thai th Turkish tr Japanese ja Polish po Korean kr Tamil ta Ukrainian ua <p>If your language is not included in the list, please file a GitHub issie: https://github.com/nixiesearch/nixiesearch/issues</p>"},{"location":"reference/languages/#language-support-for-semantic-search","title":"Language support for semantic search","text":"<p>Language support for semantic search fully depends on the embedding model used:</p> <ul> <li>sentence-transformers/all-MiniLM-L6-v2: English</li> <li>BAAI/bge-large-en-v1.5: English, a special version for Chinese</li> <li>Alibaba-NLP/gte-base-en-v1.5 English</li> <li>intfloat/multilingual-e5-large: English, Chinese and all languages from MIRACL dataset</li> </ul> <p>So if your target language is English, you can choose almost any model from the MTEB leaderboard you like. For multilingual model, the intfloat/multilingual-e5-large is recommended.</p> <p>For a full overview of supported embedding models, see the embedding inference section.</p>"},{"location":"reference/url/","title":"Supported URL formats","text":"<p>Nixiesearch configuration, index mapping and command-line options support passing URLs as locations.</p> <p>For example, offline pull-based indexing from a local file has an <code>--url</code> parameter:</p> <pre><code>docker run -i -t -v &lt;your-local-dir&gt;:/data nixiesearch/nixiesearch:latest \\\n   index file --config /data/conf.yml --index &lt;index name&gt; \\\n   --url file:///data/docs.json\n</code></pre> <p>Nixiesearch supports following URL schemas:</p> <ul> <li>Local files: <code>file:///path/to/file</code></li> <li>HTTP locations: <code>http://server.com/file.json</code></li> <li>S3-compatible locations like AWS S3, Google Cloud Store, MinIO and others: <code>s3://bucket/prefix/file.json</code></li> </ul>"},{"location":"reference/url/#local-files","title":"Local files","text":"<p>An URL is treated as a local file, if: * it starts with a <code>file://</code> schema prefix * it is a relative or absolute path like <code>/home/user/file.json</code> of just <code>file.json</code> * According to RFC 3986, Section 3.2.2 there should be either one (e.g. <code>file:/path/some.json</code>) or three slashes (e.g <code>file:///path/some.json</code>) in the prefix, but two slashes are also frequently used. Nixiesearch will handle 1, 2, and 3 slashes in the file URL.</p>"},{"location":"reference/url/#http-locations","title":"HTTP locations","text":"<p>Both HTTP and HTTPS URL schemes are supported.</p>"},{"location":"reference/url/#s3-compatible-locations","title":"S3-compatible locations","text":"<p>S3-compatible URLs have the following format:</p> <p><code>s3://bucket/prefix/file.json</code></p> <p>To pass non-URL S3 parameters like authentication and region, use ENV variables:</p> <pre><code>$ export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\n$ export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n$ export AWS_DEFAULT_REGION=us-west-2\n</code></pre>"},{"location":"reference/url/#urls-in-a-config-file","title":"URLs in a config file","text":"<p>For a configuration file URL usage, you can also unfold the S3 URL into a YAML object, which has all the internal settings exposed:</p> <pre><code>schema:\n  helloworld:\n    store:\n      distributed:\n        remote:\n          # path: s3://index-bucket/foo/bar\n          s3:\n            bucket: index-bucket\n            prefix: foo/bar\n            region: us-east-1\n            endpoint: http://localhost:8443/\n</code></pre>"},{"location":"reference/url/#urls-in-command-line-options","title":"URLs in command-line options","text":"<p>Also the nixiesearch CLI has an <code>--endpoint</code> parameter, so you can pass custom endpoint for all S3 URLs passed as cmdline parameters.</p>"},{"location":"reference/url/#decompression-support","title":"Decompression support","text":"<p>Nixiesearch also detects gz/bz2/zst compressed files (by their extension) and decompresses them on the fly. So you can also compress your source files to save space and bandwidth:</p> <pre><code>docker run -i -t -v &lt;your-local-dir&gt;:/data nixiesearch/nixiesearch:latest \\\n   index file --config /data/conf.yml --index &lt;index name&gt; \\\n   --url file:///data/docs.json.gz\n</code></pre>"},{"location":"reference/cli/","title":"Indexing","text":"<p>Nixiesearch supports two ways of running indexing:</p> <ul> <li>Push-based: a traditional REST API endpoint where you post documents to, like done in Elastic and Solr. Easy to start with, hard to scale due to backpressure and consistency issues.</li> <li>Pull-based: when Nixie pulls data from remote endpoint, maintaining the position and ingestion throughput.</li> </ul> <p>Pull-based indexing can be done over both local, HTTP and S3-hosted files, see Supported URL locations for more details.</p> <p>Nixie index CLI subcommand has the following options:</p> <pre><code>$ java -jar target/scala-3.4.2/nixiesearch.jar index --help\n\n13:49:47.757 INFO  ai.nixiesearch.main.Main$ - Staring Nixiesearch\n13:49:47.833 INFO  ai.nixiesearch.main.CliConfig - \n  -h, --help   Show help message\n\nSubcommand: api\n  -c, --config  &lt;arg&gt;     Path to a config file\n  -h, --host  &lt;arg&gt;       iface to bind to, optional, default=0.0.0.0\n  -l, --loglevel  &lt;arg&gt;   Logging level: debug/info/warn/error, default=info\n  -p, --port  &lt;arg&gt;       port to bind to, optional, default=8080\n      --help              Show help message\n\nSubcommand: file\n  -c, --config  &lt;arg&gt;     Path to a config file\n  -e, --endpoint  &lt;arg&gt;   custom S3 endpoint, optional, default=None\n  -i, --index  &lt;arg&gt;      to which index to write to\n  -l, --loglevel  &lt;arg&gt;   Logging level: debug/info/warn/error, default=info\n  -r, --recursive         recursive listing for directories, optional,\n                          default=false\n  -u, --url  &lt;arg&gt;        path to documents source\n  -h, --help              Show help message\n\nSubcommand: kafka\n  -b, --brokers  &lt;arg&gt;    Kafka brokers endpoints, comma-separated list\n  -c, --config  &lt;arg&gt;     Path to a config file\n  -g, --group_id  &lt;arg&gt;   groupId identifier of consumer. default=nixiesearch\n  -i, --index  &lt;arg&gt;      to which index to write to\n  -l, --loglevel  &lt;arg&gt;   Logging level: debug/info/warn/error, default=info\n  -o, --offset  &lt;arg&gt;     which topic offset to use for initial connection?\n                          earliest/latest/ts=&lt;unixtime&gt;/last=&lt;offset&gt;\n                          default=none (use committed offsets)\n      --options  &lt;arg&gt;    comma-separated list of kafka client custom options\n  -t, --topic  &lt;arg&gt;      Kafka topic name\n  -h, --help              Show help message\njava.lang.Exception: No command given. If unsure, try 'nixiesearch standalone'\n</code></pre>"},{"location":"reference/cli/#offline-indexing","title":"Offline indexing","text":"<p>Primary offline indexing use-case is batch and full-reindex jobs:</p> <ul> <li>when you changed index mapping by adding/altering a field and need to re-process the whole document corpus.</li> <li>when performing major non-backwards compatible Nixiesearch upgrades.</li> </ul> <p>For performance tuning during large indexing jobs, consider configuring merge policies in your index configuration.</p> <p>Nixiesearch can load a JSONL document format from a local, HTTP or S3 hosted file or directory.</p> <p>Note</p> <p>Make sure that field names in your source documents and in index mapping match! Nixiesearch will normally ignore non-mapped fields, but incompatible field formats (e.g. in JSON it's string, but in mapping it's an integer) will result in an error. </p> <p>To run an offline indexing job, use the <code>index file</code> subcommand:</p> <pre><code>docker run -i -t -v &lt;your-local-dir&gt;:/data nixiesearch/nixiesearch:latest index file\\\n  --config /data/conf.yml --index &lt;index name&gt; --url file:///data/docs.json\n</code></pre> <p>Nixiesearch will report indexing progress in logs:</p> <pre><code>$ docker run -i -t nixiesearch/nixiesearch:latest index file -v &lt;dir&gt;/data --config /data/config.yaml --index movies --url /data/movies.jsonl\n\n14:05:04.074 INFO  ai.nixiesearch.main.Main$ - Staring Nixiesearch\n14:05:04.121 INFO  a.n.main.subcommands.IndexMode$ - Starting in 'index' mode with indexer only \n14:05:04.246 INFO  ai.nixiesearch.config.Config$ - Loaded config: /home/shutty/tmp/nixie-experiments/config.yaml\n14:05:04.249 INFO  a.n.index.sync.LocalDirectory$ - initialized MMapDirectory\n14:05:04.251 INFO  a.n.index.sync.LocalDirectory$ - created on-disk directory /home/shutty/tmp/nixie-experiments/indexes/movies\nMay 24, 2024 2:05:04 PM org.apache.lucene.store.MemorySegmentIndexInputProvider &lt;init&gt;\nINFO: Using MemorySegmentIndexInput with Java 21 or later; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n14:05:04.268 INFO  a.nixiesearch.index.sync.LocalIndex$ - index dir does not contain manifest, creating...\n14:05:04.319 INFO  a.n.core.nn.model.BiEncoderCache$ - loading ONNX model hf://nixiesearch/e5-small-v2-onnx\n14:05:04.325 INFO  a.n.core.nn.model.ModelFileCache$ - using /home/shutty/.cache/nixiesearch as local cache dir\n14:05:04.416 WARN  o.h.ember.client.EmberClientBuilder - timeout (120 seconds) is &gt;= idleConnectionTime (60 seconds). It is recommended to configure timeout &lt; idleConnectionTime, or disable one of them explicitly by setting it to Duration.Inf.\n14:05:04.624 INFO  a.n.core.nn.model.HuggingFaceClient - found cached model.json card\n14:05:04.625 INFO  a.n.c.n.m.l.HuggingFaceModelLoader$ - loading model_quantized.onnx\n14:05:04.625 INFO  a.n.c.n.m.l.HuggingFaceModelLoader$ - Fetching hf://nixiesearch/e5-small-v2-onnx from HF: model=model_quantized.onnx tokenizer=tokenizer.json\n14:05:04.726 INFO  a.n.core.nn.model.HuggingFaceClient - found model_quantized.onnx in cache\n14:05:04.729 INFO  a.n.core.nn.model.HuggingFaceClient - found tokenizer.json in cache\n14:05:04.730 INFO  a.n.core.nn.model.HuggingFaceClient - found config.json in cache\n14:05:08.760 INFO  ai.djl.util.Platform - Found matching platform from: jar:file:/home/shutty/tmp/nixie-experiments/nixiesearch.jar!/native/lib/tokenizers.properties\n14:05:08.793 WARN  a.d.h.t.HuggingFaceTokenizer - maxLength is not explicitly specified, use modelMaxLength: 512\n14:05:08.919 INFO  a.n.core.nn.model.OnnxSession$ - Loaded ONNX model (size=32 MB inputs=List(input_ids, attention_mask, token_type_ids) outputs=List(last_hidden_state) dim=384)\n14:05:08.928 INFO  a.n.core.nn.model.BiEncoderCache$ - loading ONNX model hf://nixiesearch/e5-small-v2-onnx\n14:05:08.928 INFO  a.n.core.nn.model.ModelFileCache$ - using /home/shutty/.cache/nixiesearch as local cache dir\n14:05:08.929 WARN  o.h.ember.client.EmberClientBuilder - timeout (120 seconds) is &gt;= idleConnectionTime (60 seconds). It is recommended to configure timeout &lt; idleConnectionTime, or disable one of them explicitly by setting it to Duration.Inf.\n14:05:08.929 INFO  a.n.core.nn.model.HuggingFaceClient - found cached model.json card\n14:05:08.929 INFO  a.n.c.n.m.l.HuggingFaceModelLoader$ - loading model_quantized.onnx\n14:05:08.929 INFO  a.n.c.n.m.l.HuggingFaceModelLoader$ - Fetching hf://nixiesearch/e5-small-v2-onnx from HF: model=model_quantized.onnx tokenizer=tokenizer.json\n14:05:08.946 INFO  a.n.core.nn.model.HuggingFaceClient - found model_quantized.onnx in cache\n14:05:08.946 INFO  a.n.core.nn.model.HuggingFaceClient - found tokenizer.json in cache\n14:05:08.946 INFO  a.n.core.nn.model.HuggingFaceClient - found config.json in cache\n14:05:08.953 WARN  a.d.h.t.HuggingFaceTokenizer - maxLength is not explicitly specified, use modelMaxLength: 512\n14:05:09.030 INFO  a.n.core.nn.model.OnnxSession$ - Loaded ONNX model (size=32 MB inputs=List(input_ids, attention_mask, token_type_ids) outputs=List(last_hidden_state) dim=384)\n14:05:09.032 INFO  a.nixiesearch.index.sync.LocalIndex$ - index movies opened\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ - \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ - \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ - \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ - \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ - \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2554\u255d \u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ - \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n14:05:09.067 INFO  a.n.main.subcommands.IndexMode$ -                                                                                \n14:05:09.069 INFO  a.nixiesearch.util.source.URLReader$ - reading file /home/shutty/tmp/nixie-experiments/movies.jsonl\nMay 24, 2024 2:05:11 PM org.apache.lucene.internal.vectorization.VectorizationProvider lookup\nWARNING: Java vector incubator module is not readable. For optimal vector performance, pass '--add-modules jdk.incubator.vector' to enable Vector API.\n14:05:12.338 INFO  ai.nixiesearch.index.Indexer - index commit, seqnum=4\n14:05:12.340 INFO  ai.nixiesearch.index.Indexer - generated manifest for files List(_0.cfe, _0.cfs, _0.si, index.json, segments_1, write.lock)\n14:05:15.272 INFO  ai.nixiesearch.index.Indexer - index commit, seqnum=8\n14:05:15.273 INFO  ai.nixiesearch.index.Indexer - generated manifest for files List(_0.cfe, _0.cfs, _0.si, _1.cfe, _1.cfs, _1.si, index.json, segments_2, write.lock)\n14:05:16.577 INFO  ai.nixiesearch.index.Indexer - index commit, seqnum=12\n14:05:16.578 INFO  ai.nixiesearch.index.Indexer - generated manifest for files List(_0.cfe, _0.cfs, _0.si, _1.cfe, _1.cfs, _1.si, _2.cfe, _2.cfs, _2.si, index.json, segments_3, write.lock)\n14:05:16.583 INFO  a.n.core.nn.model.BiEncoderCache - closing model hf://nixiesearch/e5-small-v2-onnx\n14:05:16.585 INFO  a.n.main.subcommands.IndexMode$ - indexing done\n</code></pre> <p>After indexing, your index location will contain a set of Lucene index files:</p> <pre><code>$ ls -l indexes/movies/\ntotal 8808\n-rw-r--r-- 1 shutty shutty     549 May 24 14:05 _0.cfe\n-rw-r--r-- 1 shutty shutty 3657700 May 24 14:05 _0.cfs\n-rw-r--r-- 1 shutty shutty     321 May 24 14:05 _0.si\n-rw-r--r-- 1 shutty shutty     549 May 24 14:05 _1.cfe\n-rw-r--r-- 1 shutty shutty 3658540 May 24 14:05 _1.cfs\n-rw-r--r-- 1 shutty shutty     321 May 24 14:05 _1.si\n-rw-r--r-- 1 shutty shutty     549 May 24 14:05 _2.cfe\n-rw-r--r-- 1 shutty shutty 1664108 May 24 14:05 _2.cfs\n-rw-r--r-- 1 shutty shutty     321 May 24 14:05 _2.si\n-rw-r--r-- 1 shutty shutty    2497 May 24 14:06 index.json\n-rw-r--r-- 1 shutty shutty     318 May 24 14:05 segments_3\n-rw-r--r-- 1 shutty shutty       0 May 24 14:05 write.lock\n</code></pre> <p>You can start Nixiesearch then in a <code>Searcher</code> mode with the <code>search</code> subcommand:</p> <pre><code>docker run -i -t nixiesearch/nixiesearch:latest -v &lt;dir&gt;:/data search --config /data/conf.yaml\n</code></pre>"},{"location":"reference/cli/#online-indexing","title":"Online indexing","text":"<p>Online indexing with a REST API is mainly meant for experimentation and for small-scale document ingestion jobs.</p> <p>REST API for indexing can be used in both distributed (e.g. when you have separate deployments for Searcher and Indexer) and standalone (e.g. when Searcher and Indexer are colocated in a single node and single process) modes.</p> <p>To run Nixiesearch in a standalone mode, use the <code>standalone</code> CLI subcommand:</p> <pre><code>docker run -i -t nixiesearch/nixiesearch:latest -v &lt;dir&gt;:/data standalone --config /data/conf.yaml\n</code></pre> <p>You will see in the logs that Indexer HTTP service is listening on a port 8080:</p> <pre><code>14:11:50.616 INFO  ai.nixiesearch.index.Searcher$ - opening index movies\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ - \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2554\u255d \u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ - \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n14:11:50.759 INFO  a.n.main.subcommands.StandaloneMode$ -                                                                                \n14:11:50.787 INFO  o.h.ember.server.EmberServerBuilder - Ember-Server service bound to address: [::]:8080\n</code></pre> <p>After that you can HTTP POST the documents file to the <code>_index</code> indexing endpoint:</p> <pre><code>$ curl -XPOST -d @movies.jsonl http://localhost:8080/movies/_index\n\n{\"result\":\"created\",\"took\":8113}\n</code></pre> <p>You will also see the indexing progress in the logs:</p> <pre><code>14:12:11.026 INFO  ai.nixiesearch.api.IndexRoute - PUT /movies/_index\nMay 24, 2024 2:12:11 PM org.apache.lucene.internal.vectorization.VectorizationProvider lookup\nWARNING: Java vector incubator module is not readable. For optimal vector performance, pass '--add-modules jdk.incubator.vector' to enable Vector API.\n14:12:12.175 INFO  ai.nixiesearch.core.PrintProgress$ - processed 320 indexed docs, perf=279rps\n14:12:13.366 INFO  ai.nixiesearch.core.PrintProgress$ - processed 704 indexed docs, perf=322rps\n14:12:14.549 INFO  ai.nixiesearch.core.PrintProgress$ - processed 1088 indexed docs, perf=325rps\n14:12:15.762 INFO  ai.nixiesearch.core.PrintProgress$ - processed 1472 indexed docs, perf=317rps\n14:12:16.896 INFO  ai.nixiesearch.core.PrintProgress$ - processed 1792 indexed docs, perf=282rps\n14:12:18.105 INFO  ai.nixiesearch.core.PrintProgress$ - processed 2176 indexed docs, perf=318rps\n14:12:19.140 INFO  ai.nixiesearch.api.IndexRoute - completed indexing, took 8113ms\n14:12:19.143 INFO  ai.nixiesearch.api.API$ - HTTP/1.1 POST /movies/_index\n14:12:19.143 INFO  ai.nixiesearch.api.API$ - HTTP/1.1 200 OK\n</code></pre> <p>Finally, flush the index to disk, using the <code>_flush</code> endpoint:</p> <pre><code>$ curl -XPOST http://localhost:8080/movies/_flush\n</code></pre> <p>Nixiesearch will synchonously flush the index, and acknowledge the request with an empty (200 OK) response.</p>"},{"location":"reference/cli/search/","title":"nixiesearch search","text":"<p>The <code>search</code> command runs Nixiesearch in searcher mode, handling search queries against pre-built indexes.</p>"},{"location":"reference/cli/search/#usage","title":"Usage","text":"<pre><code>nixiesearch search --config &lt;config.yml&gt; [--api &lt;mode&gt;]\n</code></pre>"},{"location":"reference/cli/search/#options","title":"Options","text":"<ul> <li><code>--config</code>, <code>-c</code> - Path to configuration file (required)</li> <li><code>--api</code> - API mode: <code>http</code> (default) or <code>lambda</code> (optional)</li> </ul>"},{"location":"reference/cli/search/#api-modes","title":"API Modes","text":""},{"location":"reference/cli/search/#http-mode-default","title":"HTTP Mode (Default)","text":"<p>Runs as a traditional HTTP server:</p> <pre><code>nixiesearch search --config config.yml --api http\n# or simply\nnixiesearch search --config config.yml\n</code></pre> <p>The searcher listens on the configured port (default 8080) and serves the REST API.</p>"},{"location":"reference/cli/search/#lambda-mode","title":"Lambda Mode","text":"<p>Runs as an AWS Lambda function:</p> <pre><code>nixiesearch search --config config.yml --api lambda\n</code></pre> <p>In Lambda mode, Nixiesearch integrates with the AWS Lambda Runtime API and converts API Gateway V2 HTTP events to search requests. This mode requires the <code>AWS_LAMBDA_RUNTIME_API</code> environment variable (automatically set by Lambda).</p> <p>See the Lambda deployment guide for details.</p>"},{"location":"reference/cli/search/#remote-config-loading","title":"Remote Config Loading","text":"<p>Nixiesearch supports loading config files from remote locations:</p> <ul> <li>S3: <code>nixiesearch search -c s3://bucket/prefix/conf.yml</code></li> <li>HTTP/HTTPS: <code>nixiesearch search -c https://example.com/config.yml</code></li> <li>Local files: <code>nixiesearch search -c config.yml</code> or <code>nixiesearch search -c file:///path/config.yml</code></li> </ul>"},{"location":"reference/cli/standalone/","title":"Standalone mode","text":""},{"location":"reference/cli/standalone/#remote-config-file-loading","title":"Remote config file loading","text":"<p>Nixiesearch also supports loading config files from remote locations. Currently supported:</p> <ul> <li>S3-compatible block storage. Example: <code>nixiesearch standalone -c s3://bucket/prefix/conf.yml</code></li> <li>HTTP/HTTPS hosted files. Example: <code>nixiesearch standalone -c https://example.com/config.yml</code></li> <li>Local files. All <code>--config</code> option values are treated as local files if there is no URI schema prefix defined. Example: <code>nixiesearch standalone -c config.yml</code>. Optionally you can use a <code>file://</code> schema: <code>nixiesearch standalone -c file:///dir/config.yml</code>.</li> </ul>"},{"location":"tutorial/autoscaling/","title":"Auto-scaling","text":"<p>TODO</p>"},{"location":"tutorial/backup/","title":"Backup and Restore","text":""},{"location":"tutorial/backup/#overview","title":"Overview","text":"<p>Nixiesearch's S3-native architecture provides significant advantages for backup and restore operations. Unlike traditional search engines that require complex volume snapshots, nixiesearch stores all index data directly in S3, making backups as simple as copying S3 objects.</p> <p>This tutorial covers:</p> <ul> <li>Backup scenarios: Regular backups vs disaster recovery</li> <li>Restore procedures: Point-in-time and cross-environment restores</li> <li>Key concepts: S3 prefix changes and ConfigMap rollouts for zero-downtime operations</li> </ul>"},{"location":"tutorial/backup/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>A distributed nixiesearch cluster running in Kubernetes (see Distributed Deployment)</li> <li>S3 bucket with appropriate read/write permissions (see S3 Persistence)</li> <li>AWS CLI installed and configured</li> <li><code>kubectl</code> access to your Kubernetes cluster</li> </ul>"},{"location":"tutorial/backup/#backup","title":"Backup","text":""},{"location":"tutorial/backup/#manual-backup","title":"Manual Backup","text":"<p>Create a point-in-time backup by copying your current S3 prefix:</p> <pre><code># Backup current index data\naws s3 sync s3://nixiesearch-indexes/movies s3://nixiesearch-indexes/backup-movies-2024-01-15\n\n# Backup Kubernetes configuration\nkubectl get configmap nixiesearch-config -o yaml &gt; nixiesearch-config-backup.yaml\nkubectl get deployment nixiesearch-searcher -o yaml &gt; searcher-deployment-backup.yaml\nkubectl get statefulset nixiesearch-indexer -o yaml &gt; indexer-statefulset-backup.yaml\n</code></pre> <p>Note</p> <p>Underlying Lucene index consists of multiple immutable segments. Segments cannot be altered, but can be removed during the compaction process. Nixiesearch indexer only deletes actual index segments after a fixed 1h time interval to make S3 index operations atomic.</p>"},{"location":"tutorial/backup/#automated-backup","title":"Automated Backup","text":"<p>Set up automated backups using S3 lifecycle policies:</p> <pre><code>{\n  \"Rules\": [\n    {\n      \"ID\": \"NixiesearchBackup\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\"Prefix\": \"movies\"},\n      \"Transitions\": [\n        {\n          \"Days\": 30,\n          \"StorageClass\": \"STANDARD_IA\"\n        },\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>For cross-region disaster recovery, enable S3 cross-region replication:</p> <pre><code>aws s3api put-bucket-replication \\\n  --bucket nixiesearch-indexes \\\n  --replication-configuration file://replication-config.json\n</code></pre>"},{"location":"tutorial/backup/#restore","title":"Restore","text":""},{"location":"tutorial/backup/#point-in-time-restore","title":"Point-in-Time Restore","text":"<p>Restore from a specific backup by updating the S3 prefix in your configuration:</p> <ol> <li> <p>Copy backup data to new prefix: <pre><code>aws s3 sync s3://nixiesearch-indexes/backup-movies-2024-01-15 s3://nixiesearch-indexes/movies-restored\n</code></pre></p> </li> <li> <p>Update ConfigMap with new prefix: <pre><code>kubectl patch configmap nixiesearch-config --patch '\ndata:\n  config.yml: |\n    schema:\n      movies:\n        store:\n          distributed:\n            searcher:\n              memory:\n            indexer:\n              memory:\n            remote:\n              s3:\n                bucket: nixiesearch-indexes\n                prefix: movies-restored\n                region: us-east-1\n        fields:\n          # ... rest of your schema configuration\n'\n</code></pre></p> </li> <li> <p>Rolling restart to pick up new configuration: <pre><code>kubectl rollout restart deployment/nixiesearch-searcher\nkubectl rollout restart statefulset/nixiesearch-indexer\n</code></pre></p> </li> <li> <p>Verify restore: <pre><code># Check pod status\nkubectl get pods -l app=nixiesearch\n\n# Test search functionality\nkubectl port-forward svc/nixiesearch-searcher 8080:8080\ncurl \"http://localhost:8080/movies/_search?q=test\"\n</code></pre></p> </li> </ol>"},{"location":"tutorial/backup/#zero-downtime-restore","title":"Zero-Downtime Restore","text":"<p>For production environments, use a blue-green deployment approach:</p> <ol> <li> <p>Create new ConfigMap with restored prefix: <pre><code>kubectl create configmap nixiesearch-config-restored --from-file=config.yml=restored-config.yml\n</code></pre></p> </li> <li> <p>Deploy new searcher deployment: <pre><code># Update searcher deployment to use new ConfigMap\nkubectl patch deployment nixiesearch-searcher --patch '\nspec:\n  template:\n    spec:\n      volumes:\n      - name: config\n        configMap:\n          name: nixiesearch-config-restored\n'\n</code></pre></p> </li> <li> <p>Wait for new pods to be ready: <pre><code>kubectl rollout status deployment/nixiesearch-searcher\n</code></pre></p> </li> <li> <p>Switch traffic by updating the service selector if needed, or simply let Kubernetes handle the rolling update.</p> </li> <li> <p>Update indexer once searchers are healthy: <pre><code>kubectl patch statefulset nixiesearch-indexer --patch '\nspec:\n  template:\n    spec:\n      volumes:\n      - name: config\n        configMap:\n          name: nixiesearch-config-restored\n'\n</code></pre></p> </li> </ol>"},{"location":"tutorial/backup/#further-reading","title":"Further Reading","text":"<ul> <li>Distributed Deployment Overview - Architecture and concepts</li> <li>S3 Persistence Configuration - Detailed S3 setup</li> <li>Schema Migration - Handling schema changes during restore</li> <li>Version Upgrades - Combining backups with version updates</li> <li>Configuration Reference - Complete configuration options</li> <li>Auto-scaling - Scaling considerations for restored clusters</li> </ul> <p>Note</p> <p>Remember that nixiesearch's stateless architecture means no persistent volumes need backing up - all critical data lives in S3 and can be restored by simply updating configuration.</p>"},{"location":"tutorial/overview/","title":"Tutorials","text":"<p>Practical end-to-end recipes about typical search engine maintenance tasks.</p> <ul> <li> <p> Backup and Restore</p> <p>Backing up and restoring your index with no downtime.</p> <p> Backup and Restore</p> </li> <li> <p> Schema migration</p> <p>How to change document schema in backwards [in-] compatible way.</p> <p> Schema migration</p> </li> <li> <p> Version upgrades</p> <p>Major and minor version upgrades of a running cluster.</p> <p> Version upgrades</p> </li> <li> <p> Auto-scaling</p> <p>Rapid up- and down-scaling based on cluster load.</p> <p> Auto-scaling</p> </li> <li> <p> Offline indexing</p> <p>Batch offline full reindexing without interfering with your live search cluster.</p> <p> Full reindexing</p> </li> </ul>"},{"location":"tutorial/schema/","title":"Schema configuration","text":"<p>This guide covers how to configure field schemas in Nixiesearch, including the new numeric list field types.</p>"},{"location":"tutorial/schema/#basic-schema-structure","title":"Basic schema structure","text":"<p>A Nixiesearch schema defines the structure of your index through field definitions. Each field specifies its data type and capabilities:</p> <pre><code>schema:\n  movies:\n    fields:\n      title:\n        type: text\n        search: true\n        filter: true\n        store: true\n      year:\n        type: int\n        filter: true\n        facet: true\n        sort: true\n      genres:\n        type: text[]\n        filter: true\n        store: true\n</code></pre>"},{"location":"tutorial/schema/#numeric-field-types","title":"Numeric field types","text":""},{"location":"tutorial/schema/#single-value-numeric-fields","title":"Single-value numeric fields","text":"<pre><code>schema:\n  products:\n    fields:\n      price:\n        type: float\n        filter: true\n        sort: true\n        facet: true\n      quantity:\n        type: int\n        filter: true\n        sort: true\n      weight:\n        type: double\n        filter: true\n        sort: true\n      product_id:\n        type: long\n        filter: true\n        required: true\n</code></pre>"},{"location":"tutorial/schema/#numeric-list-fields","title":"Numeric list fields","text":"<p>List/array variants allow storing multiple values per field:</p> <pre><code>schema:\n  ecommerce:\n    fields:\n      name:\n        type: text\n        search: true\n\n      # Multiple ratings from different users\n      ratings:\n        type: int[]\n        filter: true     # Enable range filtering\n        store: true      # Store original values\n        required: false  # Optional field\n\n      # Historical price points  \n      price_variants:\n        type: float[]\n        filter: true\n        store: true\n\n      # Product dimensions [length, width, height]\n      dimensions:\n        type: double[]\n        store: true\n        filter: false\n\n      # Category relevance scores\n      category_scores:\n        type: long[]\n        filter: true\n        store: true\n</code></pre>"},{"location":"tutorial/schema/#field-configuration-options","title":"Field configuration options","text":"Option Description Default Available for <code>type</code> Field data type required All fields <code>store</code> Store original value <code>true</code> All fields <code>filter</code> Enable filtering <code>false</code> All fields <code>sort</code> Enable sorting <code>false</code> Single-value fields <code>facet</code> Enable faceting <code>false</code> Single-value fields <code>search</code> Enable text search <code>false</code> Text fields only <code>required</code> Field is mandatory <code>false</code> All fields"},{"location":"tutorial/schema/#complete-example","title":"Complete example","text":"<pre><code>schema:\n  product_catalog:\n    fields:\n      # Text fields\n      title:\n        type: text\n        search: true\n        filter: true\n        store: true\n        required: true\n\n      description:\n        type: text\n        search: true\n        store: true\n\n      # Single numeric fields  \n      price:\n        type: float\n        filter: true\n        sort: true\n        facet: true\n        required: true\n\n      stock:\n        type: int\n        filter: true\n        sort: true\n\n      # Numeric array fields\n      user_ratings:\n        type: int[]\n        filter: true    # Can filter by rating ranges\n        store: true\n\n      size_options:\n        type: float[]   # Available sizes: [6.5, 7.0, 7.5, 8.0]\n        filter: true\n        store: true\n\n      # Other field types\n      active:\n        type: bool\n        filter: true\n        required: true\n\n      created_date:\n        type: datetime\n        filter: true\n        sort: true\n</code></pre> <p>This schema supports documents like:</p> <pre><code>{\n  \"title\": \"Running Shoes\",\n  \"description\": \"Comfortable athletic footwear\",\n  \"price\": 89.99,\n  \"stock\": 25,\n  \"user_ratings\": [5, 4, 5, 3, 4, 5, 2],\n  \"size_options\": [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0],\n  \"active\": true,\n  \"created_date\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"tutorial/upgrade/","title":"Upgrades","text":"<p>TODO</p>"}]}